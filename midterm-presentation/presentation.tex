\documentclass[9pt]{beamer}
\usetheme{TUDOplain}
% workaround: provide commands not defiend by all bibtex styles
\providecommand{\btxandlong}{und}
\providecommand{\newblock}{}

\usepackage{pgfpages}
\setbeameroption{hide notes}

% link for how to present on mac with skim:
% https://gist.github.com/andrejbauer/ac361549ac2186be0cdb

% sourcing images
\providecommand{\source}{\\ \footnotesize \tugreen{Source:} \footnotemark}
\providecommand{\sourcefix}[1]{\\ \footnotesize \tugreen{Source:} [#1]}

\renewcommand{\caption}[1]{\\ \footnotesize{\captiongrey{#1}}}

\usepackage[english]{babel}
\usepackage[style=authortitle]{biblatex}
\addbibresource{../bibliography.bib}

% reformat footnotes very plain
\makeatletter
\renewcommand\@makefnmark{%
[\@thefnmark]}
\renewcommand\@makefntext[1]{%
  \noindent\tiny [\@thefnmark] #1}
\makeatother
% command for citing
\providecommand{\fcite}[1]{\footcite{#1}}
%

% basic utils
\usepackage[utf8]{inputenc}
\usepackage{enumerate}
\usepackage{graphicx}
\graphicspath{{../images/}}

\AtBeginSection[]{
  \begin{frame}
  \note[item]{placeholder}
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}

\usepackage{ifthen}
\usepackage{calc}
\usepackage{amsmath,amsfonts,amssymb}
\setbeamertemplate{navigation symbols}{}
%\setbeamertemplate{footline}{}
%\setbeamertemplate{footline}[frame number]{}
\setbeamertemplate{footline}{\small \vspace{-1ex} \vbox{ \insertframenumber /\inserttotalframenumber}}
%\setbeamertemplate{footline}{\fontsize{7pt}{7pt}\selectfont \vspace{-1ex} \vbox{ \insertframenumber /\inserttotalframenumber}}

\author{Matthias Jakobs}
\title{End-to-end Human Activity Recognition framework on Complex Video Datasets \\ Midterm presentation}
\date{\today}
\institute[TU Dortmund]{Pattern Recognition In Embedded Systems,\\ Department of Computer Science \\ LS XII, Technische Universit√§t Dortmund}
%
% frame command
\newenvironment{myframe}[1][]{%
\begin{frame}%
\frametitle{#1}
% start footnote numbers with 1
\setcounter{footnote}{0}


}{%
\end{frame}%
}

\begin{document}
\begin{frame}

\titlepage

\end{frame}

\section{Motivation}
\begin{myframe}[Motivation]
    \begin{columns}[T]
        \begin{column}{.45\textwidth}
            \begin{itemize}
                \item Often: Human Activity Recognition approached separately from pose estimation
                \item Pose is used for HAR, but not learned jointly.
                \begin{itemize}
                    \item Shown to be very important feature \footnotemark
                \end{itemize}
                \item Idea: Both can benefit from each other
                \item Luvizon et al. \footnotemark~ propose approach for jointly learning both
            \end{itemize}
        \end{column}
        \footnotetext[1]{\cite{jhuang_towards_2013}}
        \footnotetext[2]{\cite{luvizon_2d/3d_2018}}
        \footnotetext[3]{\cite{reining_towards_2018}}
        \begin{column}{.45\textwidth}
            \begin{figure}
                \includegraphics[width=.99\textwidth]{skeleton_har_example.png}
                \sourcefix{3}
            \end{figure}
        \end{column}
    \end{columns}
\end{myframe}

\tableofcontents

\section{Method}

\begin{myframe}[Method - Overview]
	\begin{columns}[T]
        \begin{column}{.45\textwidth}
            \begin{itemize}
                \item \textbf{Multitask Deep HAR}\footnotemark
                \begin{itemize}
                    \item Jointly train pose and action recognition
                    \item Pre-train pose estimation part, then fine-tune end-to-end
                    \item \textit{Soft-argmax}\footnotemark~makes end-to-end learning possible
                \end{itemize}
            \end{itemize}
        \end{column}
        \footnotetext[1]{\cite{luvizon_2d/3d_2018}}
        \footnotetext[2]{\cite{luvizon_human_2017}}
        \begin{column}{.45\textwidth}
            \begin{figure}
                \includegraphics[width=.99\textwidth]{endtoend-concept.png}
                \sourcefix{1}
                %\caption{Complete network pipeline.}
            \end{figure}
        \end{column}
        \end{columns}
    \end{myframe}

\begin{myframe}[Method - Softargmax]
	\begin{columns}[T]
        \begin{column}{.45\textwidth}
            \begin{itemize}
                \item According to authors: Postprocessing necessary
                \item Propose \textbf{Soft-argmax} \footnotemark ~ in previous work
                \item Computes expectations on probability maps
                \item Fully differentiable
            \end{itemize}
        \end{column}
        \footnotetext[1]{\cite{luvizon_human_2017}}
        \footnotetext[2]{\cite{luvizon_2d/3d_2018}}
        \begin{column}{.45\textwidth}
            \begin{figure}
                \includegraphics[width=.99\textwidth]{softargmax.png}
                \sourcefix{1}
                %\caption{Complete network pipeline.}
            \end{figure}
        \end{column}
	\end{columns}
\end{myframe}

\begin{myframe}[Method - Architecture]
    \begin{columns}[T]
        \begin{column}{.45\textwidth}
            \begin{itemize}
                \item \textit{Feature Extractor - Stem}
                \begin{itemize}
                    \item Based on Inception v4 \footnotemark
                    \item Implicit: Batch normalization and ReLU activation
                    \item Uses \textbf{Depthwise separate convolutional layer}\footnotemark\footnotemark
                    \begin{itemize}
                        \item Used when high number of filters needed
                        \item Needs fewer parameters
                        \item Needs fewer matrix multiplications
                    \end{itemize}
                \end{itemize}
            \end{itemize}
        \end{column}
        \begin{column}{.45\textwidth}
            \begin{figure}
                \includegraphics[width=.45\textwidth]{luvizon_stem.png}
                \sourcefix{4}
            \end{figure}
        \end{column}
	\end{columns}
    \footnotetext[1]{\cite{szegedy_inception-v4_2017}}
    \footnotetext[2]{\cite{sifre_rigid-motion_2014}}
    \footnotetext[3]{\cite{chollet_xception:_2017}}
    \footnotetext[4]{\cite{luvizon_2d/3d_2018}}
\end{myframe}

\begin{myframe}[Method - Depthwise separable convolution]
    \begin{itemize}
        \item Depthwise: $n$ filters of size $a \times a \times 1$
        \item Pointwise: $m$ filters of size $1 \times 1 \times n$
        \item Comparison:
        \begin{itemize}
            \item Regular convolution: $n \cdot m \cdot a^2$ parameters
            \item DSC: $n \cdot (m + a^2)$ parameters
        \end{itemize} 
    \end{itemize}

    \begin{figure}
        \includegraphics[width=.99\textwidth]{depthwise-visualization.png}
        \sourcefix{1}
    \end{figure}

    \footnotetext[1]{\url{https://towardsdatascience.com/a-basic-introduction-to-separable-convolutions-b99ec3102728}}
\end{myframe}

\begin{myframe}[Method - Pose estimator]
    \begin{itemize}
        \item Features $\rightarrow$ prediction block
        \item Input and output dimensions identical
        \item Intermediate results via Soft-argmax function
    \end{itemize}
    \begin{figure}
        \includegraphics[width=.80\textwidth]{multitask-part.png}
        \sourcefix{1}
        \caption{Pose estimation pipeline. Notice the intermediate results after each prediction block.}
    \end{figure}
    \footnotetext[1]{\cite{luvizon_2d/3d_2018}}
\end{myframe}

\begin{myframe}[Method - Pose Model]
	\begin{columns}[T]
        \begin{column}{.45\textwidth}
            \begin{itemize}
                \item Two paths in the network for action recognition
                \item \textit{Pose Model}
                \begin{itemize}
                    \item Arrange joint values over time in 2D matrix (\textit{pose cube})
                    \item Action heatmaps
                    \begin{itemize}
                        \item Through softmax: action probabilities
                    \end{itemize}
                \end{itemize}
            \end{itemize}
        \end{column}
        \begin{column}{.45\textwidth}
            \begin{figure}
                \includegraphics[width=.99\textwidth]{jointsovertime.png}
                \sourcefix{1}
            \end{figure}
        \end{column}
	\end{columns}
    \footnotetext[1]{\cite{luvizon_2d/3d_2018}}
\end{myframe}

\begin{myframe}[Method - Pose Model]
    \begin{figure}
        \includegraphics[width=.30\textwidth]{luvizon_actionrecognitionblock.png}
        \sourcefix{1}
    \end{figure}
    \footnotetext[1]{\cite{luvizon_2d/3d_2018}}
\end{myframe}

\begin{myframe}[Method - Visual Model]
    	\begin{columns}[T]
        \begin{column}{.45\textwidth}
            \begin{itemize}
                \item \textit{Visual Model}
                \begin{itemize}
                    \item Combination of visual features and joint probability maps
                    \item Then: Simple feature extraction on \textit{appearance cube}
                    \item Array of action precition blocks as seen before
                \end{itemize}
                \item \textit{Aggregation}
                \begin{itemize}
                    \item Combine Pose Model and Visual Model results for final result
                    \item \textit{Categorical cross-entropy}
                \end{itemize}
            \end{itemize}
        \end{column}
        \begin{column}{.45\textwidth}
            \begin{figure}
                \includegraphics[width=.99\textwidth]{appearance-features.png}
                \sourcefix{1}
            \end{figure}
            \begin{figure}
                \includegraphics[width=.50\textwidth]{luvizon_appearancebasedaction.png}
            \end{figure}
        \end{column}
	\end{columns}
    \footnotetext[1]{\cite{luvizon_2d/3d_2018}}
\end{myframe}

\begin{myframe}[Method - Extensions]
    \begin{itemize}
        \item Reimplementation in PyTorch \fcite{paszke_automatic_2017}
        \begin{itemize}
            \item Evaluate against JHMDB \fcite{jhuang_towards_2013}
        \end{itemize}
        \item Experimentation
        \begin{itemize}
            %\item Better incorporation of temporal dimension \fcite{pavllo_3d_2019}
            \item Ablation study to determine the choice of certain parameters
            \item Combined loss function of pose and action for \emph{real} end-to-end training
            \item Different representation of temporal information (next slide)
            \begin{itemize}
                \item Idea: Use for IMU time series data as presented in \footnotemark
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \footnotetext[3]{\cite{reining_towards_2018}}
\end{myframe}

\begin{myframe}[Method - Different temporal representation]
    \begin{figure}
        \includegraphics[width=.65\textwidth]{jointsovertime.png}
        \caption{Approach used by \footnotemark. Convolution over all sensors at once. \sourcefix{1}}
    \end{figure}
    \begin{figure}
        \includegraphics[width=.65\textwidth]{sensor-time.png}
        \caption{Impression of pixel coordinates of joints over time \source}
    \end{figure}
    \footnotetext[1]{\cite{luvizon_2d/3d_2018}}
    \footnotetext[2]{\url{https://avtech.com/articles/wp-content/uploads/2015/06/Intro.-Pic.png}}
\end{myframe}

\section{Datasets}
\begin{myframe}[2D Pose Datasets]
  \note[item]{placeholder}
  \begin{columns}[T]
      \begin{column}{.48\textwidth}
          \begin{itemize}
              \item \textbf{MPII Human Pose\footnotemark}
              \begin{itemize}
                  \item 40,000 annotated images
                  \begin{itemize}
                      \item 2D pose
                      \item Head size
                      \item Center of Person bounding box
                  \end{itemize}
                  \item Single and multi person
                  \item Over 401 different activities from YouTube videos
                  \item Test image predictions to be send to Max Planck institute
              \end{itemize}
          \end{itemize}
      \end{column}
      \footnotetext[1]{\cite{andriluka_2d_2014}}
      \begin{column}{.48\textwidth}
          \begin{figure}
              \includegraphics[width=0.99\textwidth]{mpii.png}
              \sourcefix{1}
          \end{figure}
      \end{column}
  \end{columns}
\end{myframe}

\begin{myframe}[Action Recognition Datasets]
  \begin{columns}[T]
      \begin{column}{.48\textwidth}
          \vspace{20px}
          \begin{itemize}
              \item \textbf{Penn Action\footnotemark}
              \begin{itemize}
                  \item 2,400 video clips of 15 actions
                  \item Limited number of actions (mainly sport)
                  \item Literature ``solved'' this dataset
              \end{itemize}
          \end{itemize}
      \end{column}
      \footnotetext[1]{\cite{zhang_actemes_2013}}
      \begin{column}{.48\textwidth}
          \begin{figure}
              \includegraphics[height=45px]{pa-01.jpg}
              \includegraphics[height=45px]{pa-02.jpg}
              \includegraphics[height=45px]{pa-03.jpg}
              \includegraphics[height=45px]{pa-04.jpg}
              \source
          \end{figure}
      \end{column}
  \end{columns}
  \footnotetext[2]{\url{https://upenn.app.box.com/v/PennAction}}
\end{myframe}

\begin{myframe}[Action Recognition Datasets]
  \begin{columns}[T]
      \begin{column}{.48\textwidth}
          \begin{itemize}
              \item \textbf{JHMDB\footnotemark}
              \begin{itemize}
                  \item Fully-annotated subset of HMDB
                  \begin{itemize}
                      \item 2D pose
                      \item Subject segmentation maps
                      \item Optical flow
                  \end{itemize}
                  \item 928 clips of 21 actions
                  \begin{itemize}
                      \item More diverse than Penn Action
                  \end{itemize}
              \end{itemize}
          \end{itemize}
      \end{column}
      \footnotetext[1]{\cite{jhuang_towards_2013}}
      \begin{column}{.48\textwidth}
          \begin{figure}
              \includegraphics[height=.55\textheight]{jhmdb.png}
              \centering
              \source
          \end{figure}
      \end{column}
  \end{columns}
  \footnotetext[2]{\url{http://jhmdb.is.tue.mpg.de/puppet_tool}}
\end{myframe}

\section{Experiments}
\begin{myframe}[Recreate experiments - Meassurements]
    % TODO: Cite all this
    \begin{itemize}
        \item 2D Pose estimation
        \begin{itemize}
            \item \textit{Probability of Correct Keypoints} with regards to head size (\textit{PCKh})
            \begin{itemize}
                \item Meassure distance between ground truth keypoint and estimation
                \item If smaller than ($\alpha \cdot \text{head bbox size}$): correctly estimated
                \item Used for highly articulated poses
                \item In literature: $\alpha \in \{0.2, 0.5\}$
            \end{itemize}    
            \item \textit{PCK} with regards to subject bounding box
            \begin{itemize}
                \item Problem with highly articulated poses
                \item Used whenever no head size given
                \item Can be computed from all datasets used
                \item $\alpha = 0.2$ used in literature
            \end{itemize}
        \end{itemize}
        \item Action recognition
        \begin{itemize}
            \item Simple accuracy meassurement in literature
            \item $F_1$ meassure not needed since all datasets are balanced w.r.t. classes
        \end{itemize}
    \end{itemize}
\end{myframe}

\begin{myframe}[Recreate experiments - 2D Pose estimation]
    \begin{itemize}
        \item Recreate results for using different parameters
        \item Evaluate significance using randomization test
        \begin{itemize}
            \item Improvement when using context heatmaps is not statistically significant (with significance level $0.05$)
            \item Improvement between $nr\_blocks = 2$ and $nr\_blocks = 4$ is significant.
            \begin{itemize}
                \item Using $8$ blocks does not significantly increase accuracy
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \begin{figure}
        \begin{table}[]
            \small
            \begin{tabular}{|l|c|c|c|c|}
            \hline
             & \textbf{nr\_blocks} & \textbf{nr\_context} & \textbf{PCKh @ 0.5 (validation)} & \textbf{PCKh @ 0.5 (test)} \\ \hline
            \textbf{Own} & 2 & 0 & 80.96 &  \\
            \textbf{} & 2 & 2 & 81.56 &  \\
            \textbf{} & 4 & 0 & 83.22 &  \\
            \textbf{} & 4 & 2 & 82.97 &  \\
            \textbf{} & 8 & 0 & 84.67 &  \\
            \textbf{} & 8 & 2 & 84.66 & \textbf{81.1}* \\ \hline
            \textbf{\footnotemark} & 8 & 2 & 89.00 & 91.2 \\ \hline
            \end{tabular}
        \end{table}
    \end{figure}
    \footnotetext[1]{\cite{luvizon_2d/3d_2018}}
    * Test results need to be send to Max Planck institute for evaluation
\end{myframe}

\begin{myframe}[Recreate experiments - 2D pose estimation]
    \begin{figure}
        \includegraphics[height=.80\textheight]{mpii_8_positive.png}
        \centering
        \caption{\tugreen{Figure:} Positive example. Ground truth pose shown in red, estimated pose shown in blue.}
    \end{figure}
\end{myframe}

\begin{myframe}[Recreate experiments - 2D pose estimation]
    \begin{figure}
        \includegraphics[height=.80\textheight]{mpii_8_negative.png}
        \centering
        \caption{\tugreen{Figure:} Negative example. Notice that the lower limbs are wrongly estimated due to the water.}
    \end{figure}    
\end{myframe}

\begin{myframe}[Recreate experiments]
    \begin{itemize}
        \item Use pretrained pose estimator
        \item First: Pose estimator is not fine-tuned, locked weights
        \item Find validation plateau and then start fine-tuning with smaller learning rate
    \end{itemize}
    \begin{figure}
        \includegraphics[height=.60\textheight]{presentation_har_pennaction.png}
        \centering
    \end{figure}    

\end{myframe}

\begin{myframe}[2D pose estimation on JHMDB]
    \begin{itemize}
        \item Used ground truth bounding boxes as in \footnotemark
        \item Additionally: \textit{PCK} w.r.t. upper body size (\textit{PCKu})
        \item Dataset more challenging because of video artifacts
        \item Unclear whether or not \footnotemark[2] use ground truth bounding box
    \end{itemize}
    \begin{figure}
        \begin{table}[]
            \small
            \begin{tabular}{|l|c|c|c|c|}
            \hline
             & \textbf{nr\_blocks} & \textbf{nr\_context} & \textbf{PCK @ 0.2 (test)} & \textbf{PCKu @ 0.2 (test)} \\ \hline
            \textbf{Own} & 2 & 0 & 86.47 & 62.33 \\
            \textbf{} & 2 & 2 & 86.73 & 62.24 \\
            \textbf{} & 4 & 0 & 86.87 &  62.71\\
            \textbf{} & 4 & 2 & 87.10 &  62.80 \\
            \textbf{} & 8 & 0 & 61.54* &  30.70* \\
            \textbf{} & 8 & 2 & 61.27* &  25.66*\\ \hline
            \textbf{\footnotemark} &  &  & 81.60 &  \\ \hline
            \end{tabular}
        \end{table}
    \end{figure}
    \footnotetext[1]{\cite{luvizon_2d/3d_2018}}
    \footnotetext[2]{\cite{song_thin-slicing_2017}}
    * overfitted on training
\end{myframe}

\begin{myframe}[HAR on JHMDB dataset]
    \begin{itemize}
        \item Same procedure as with Penn Action
        \item Overfitting observed
        \begin{itemize}
            \item Use more augmented data
            \item Also: try to decrease learning rate
            \item Then: fine-tune pose estimator after validation plateaus
        \end{itemize}
    \end{itemize}
    \begin{figure}
        \includegraphics[height=.55\textheight]{presentation_har_jhmdb_bad.png}
        \centering
    \end{figure}    
    \begin{itemize}
        \item Best result found in literature: 87.9 percent \footnotemark
    \end{itemize}
    \footnotetext[1]{\cite{choutas_potion:_2018}}
\end{myframe}

\section{Conclusion}
\begin{myframe}[Conclusion]
    \begin{itemize}
        \item Stacked architecture in pose estimator increases accuracy until $nr\_blocks = 4$
        \item Using context heatmaps does not significantly increase accuracy
        \item JHMDB seems to be harder than MPII for pose estimation
        \item What will be done next
        \begin{itemize}
            \item Fine-tuning network once the network is no longer overfitting
            \item End-to-end training on JHMDB dataset
            \item Evaluate use of different \textit{pose cube} feature extraction
        \end{itemize}
        \item Future work suggestions
        \begin{itemize}
            \item Use video pose estimator
            \item Gather bigger, fully annotated video datasets for end-to-end Human Activity Recognition
        \end{itemize}
    \end{itemize}
\end{myframe}

\begin{myframe}[Thank you]
    \centering \Large
    \emph{Thank you for your time!}
\end{myframe}

\end{document}

