
@inproceedings{wang_approach_2013,
	title = {An {Approach} to {Pose}-{Based} {Action} {Recognition}},
	doi = {10.1109/CVPR.2013.123},
	abstract = {We address action recognition in videos by modeling the spatial-temporal structures of human poses. We start by improving a state of the art method for estimating human joint locations from videos. More precisely, we obtain the K-best estimations output by the existing method and incorporate additional segmentation cues and temporal constraints to select the “best” one. Then we group the estimated joints into five body parts (e.g. the left arm) and apply data mining techniques to obtain a representation for the spatial-temporal structures of human actions. This representation captures the spatial configurations of body parts in one frame (by spatial-part-sets) as well as the body part movements(by temporal-part-sets) which are characteristic of human actions. It is interpretable, compact, and also robust to errors on joint estimations. Experimental results first show that our approach is able to localize body joints more accurately than existing methods. Next we show that it outperforms state of the art action recognizers on the UCF sport, the Keck Gesture and the MSR-Action3D datasets.},
	booktitle = {2013 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wang, Chunyu and Wang, Yizhou and Yuille, Alan L.},
	month = jun,
	year = {2013},
	keywords = {action recognition, body part movements, body parts spatial configurations, data mining, Data mining, Dictionaries, Estimation, feature learning, gesture recognition, Histograms, human actions, human joint locations estimation, human poses, Image color analysis, image motion analysis, image representation, image segmentation, Itemsets, joint estimations, Joints, K-best estimations, Keck gesture, MSR-action3D datasets, pose estimation, pose-based action recognition, segmentation cues, spatial-part-sets, spatial-temporal structures modeling, spatial-temporal structures representation, temporal constraints, temporal-part-sets, UCF sport, video signal processing, videos},
	pages = {915--922},
	file = {Wang et al. - 2013 - An Approach to Pose-Based Action Recognition.pdf:/home/matthias/Zotero/storage/ERSPKM4H/ERSPKM4H.pdf:application/pdf}
}

@inproceedings{yang_articulated_2011,
	title = {Articulated pose estimation with flexible mixtures-of-parts},
	doi = {10.1109/CVPR.2011.5995741},
	abstract = {We describe a method for human pose estimation in static images based on a novel representation of part models. Notably, we do not use articulated limb parts, but rather capture orientation with a mixture of templates for each part. We describe a general, flexible mixture model for capturing contextual co-occurrence relations between parts, augmenting standard spring models that encode spatial relations. We show that such relations can capture notions of local rigidity. When co-occurrence and spatial relations are tree-structured, our model can be efficiently optimized with dynamic programming. We present experimental results on standard benchmarks for pose estimation that indicate our approach is the state-of-the-art system for pose estimation, outperforming past work by 50\% while being orders of magnitude faster.},
	booktitle = {{CVPR} 2011},
	author = {Yang, Yi and Ramanan, Deva},
	month = jun,
	year = {2011},
	keywords = {articulated limb parts, Computational modeling, contextual co-occurrence relation, Deformable models, dynamic programming, Estimation, flexible mixture model, flexible mixture of parts, human pose estimation, Humans, image coding, image representation, Joints, pose estimation, spatial relations, Springs, standard spring models, static images, Training, tree structure, trees (mathematics)},
	pages = {1385--1392},
	file = {Yang and Ramanan - 2011 - Articulated pose estimation with flexible mixtures.pdf:/home/matthias/Zotero/storage/LHPQS7SY/Yang and Ramanan - 2011 - Articulated pose estimation with flexible mixtures.pdf:application/pdf}
}

@inproceedings{insafutdinov_deepercut:_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{DeeperCut}: {A} {Deeper}, {Stronger}, and {Faster} {Multi}-person {Pose} {Estimation} {Model}},
	isbn = {978-3-319-46466-4},
	shorttitle = {{DeeperCut}},
	abstract = {The goal of this paper is to advance the state-of-the-art of articulated pose estimation in scenes with multiple people. To that end we contribute on three fronts. We propose (1) improved body part detectors that generate effective bottom-up proposals for body parts; (2) novel image-conditioned pairwise terms that allow to assemble the proposals into a variable number of consistent body part configurations; and (3) an incremental optimization strategy that explores the search space more efficiently thus leading both to better performance and significant speed-up factors. Evaluation is done on two single-person and two multi-person pose estimation benchmarks. The proposed approach significantly outperforms best known multi-person pose estimation results while demonstrating competitive performance on the task of single person pose estimation (Models and code available at http://pose.mpi-inf.mpg.de).},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Insafutdinov, Eldar and Pishchulin, Leonid and Andres, Bjoern and Andriluka, Mykhaylo and Schiele, Bernt},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Area Under Curve, Body Part, Conv4 Bank, Integer Linear Programming, Part Detector},
	pages = {34--50},
	file = {Insafutdinov et al. - 2016 - DeeperCut A Deeper, Stronger, and Faster Multi-pe.pdf:/home/matthias/Zotero/storage/Q4K7LMFG/Q4K7LMFG.pdf:application/pdf}
}

@inproceedings{reining_towards_2018,
	title = {Towards a {Framework} for {Semi}-{Automated} {Annotation} of {Human} {Order} {Picking} {Activities} {Using} {Motion} {Capturing}},
	abstract = {Data creation for Human Activity Recognition (HAR) requires an immense human effort and contextual knowledge for manual annotation. This paper proposes a framework for semi-automated annotation of sequential data in the order picking process using a motion capturing system. Additionally, it introduces proper annotation labels by defining process steps, human activities and simple human movements in order picking scenarios. An attribute representation based on simple human movements meets the challenges set by the versatility of activities in warehousing.},
	booktitle = {2018 {Federated} {Conference} on {Computer} {Science} and {Information} {Systems} ({FedCSIS})},
	author = {Reining, Christopher and Moya Rueda, Fernando and ten Hompel, Michael and Fink, Gernot A.},
	month = sep,
	year = {2018},
	keywords = {image motion analysis, image representation, annotation labels, attribute representation, Computer architecture, contextual knowledge, data creation, human activities, human activity recognition, human movements, human order picking activities, image recognition, immense human effort, Legged locomotion, manual annotation, Manuals, motion capturing system, order picking, production engineering computing, semiautomated annotation, sequential data, Skeleton, Task analysis, Videos, warehousing, Warehousing},
	pages = {817--821},
	file = {Reining et al. - 2018 - Towards a Framework for Semi-Automated Annotation .pdf:/home/matthias/Zotero/storage/MAEVS2GC/Reining et al. - 2018 - Towards a Framework for Semi-Automated Annotation .pdf:application/pdf}
}

@article{chou_self_2017,
	title = {Self {Adversarial} {Training} for {Human} {Pose} {Estimation}},
	url = {https://arxiv.org/abs/1707.02439v2},
	abstract = {This paper presents a deep learning based approach to the problem of human
pose estimation. We employ generative adversarial networks as our learning
paradigm in which we set up two stacked hourglass networks with the same
architecture, one as the generator and the other as the discriminator. The
generator is used as a human pose estimator after the training is done. The
discriminator distinguishes ground-truth heatmaps from generated ones, and
back-propagates the adversarial loss to the generator. This process enables the
generator to learn plausible human body configurations and is shown to be
useful for improving the prediction accuracy.},
	language = {en},
	urldate = {2019-04-20},
	author = {Chou, Chia-Jung and Chien, Jui-Ting and Chen, Hwann-Tzong},
	month = jul,
	year = {2017},
	file = {Full Text PDF:/home/matthias/Zotero/storage/VSZPQT5E/VSZPQT5E.pdf:application/pdf;Snapshot:/home/matthias/Zotero/storage/33W6P3BH/Chou et al. - 2017 - Self Adversarial Training for Human Pose Estimatio.html:text/html}
}

@inproceedings{johnson_clustered_2010,
	title = {Clustered {Pose} and {Nonlinear} {Appearance} {Models} for {Human} {Pose} {Estimation}},
	booktitle = {Proceedings of the {British} {Machine} {Vision} {Conference}},
	author = {Johnson, Sam and Everingham, Mark},
	year = {2010},
	annote = {doi:10.5244/C.24.12}
}

@inproceedings{johnson_learning_2011,
	title = {Learning {Effective} {Human} {Pose} {Estimation} from {Inaccurate} {Annotation}},
	booktitle = {Proceedings of {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Johnson, Sam and Everingham, Mark},
	year = {2011}
}

@inproceedings{andriluka_2d_2014,
	title = {2D {Human} {Pose} {Estimation}: {New} {Benchmark} and {State} of the {Art} {Analysis}},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Andriluka, Mykhaylo and Pishchulin, Leonid and Gehler, Peter and Schiele, Bernt},
	month = jun,
	year = {2014}
}

@inproceedings{toshev_deeppose:_2014,
	address = {Columbus, OH, USA},
	title = {{DeepPose}: {Human} {Pose} {Estimation} via {Deep} {Neural} {Networks}},
	isbn = {978-1-4799-5118-5},
	shorttitle = {{DeepPose}},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909610},
	doi = {10.1109/CVPR.2014.214},
	abstract = {We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regressors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formulation which capitalizes on recent advances in Deep Learning. We present a detailed empirical analysis with state-ofart or better performance on four academic benchmarks of diverse real-world images.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Toshev, Alexander and Szegedy, Christian},
	month = jun,
	year = {2014},
	pages = {1653--1660},
	file = {Toshev and Szegedy - 2014 - DeepPose Human Pose Estimation via Deep Neural Ne.pdf:/home/matthias/Zotero/storage/Q36UJJG7/Toshev and Szegedy - 2014 - DeepPose Human Pose Estimation via Deep Neural Ne.pdf:application/pdf}
}

@inproceedings{gong_look_2017,
	address = {Honolulu, HI},
	title = {Look into {Person}: {Self}-{Supervised} {Structure}-{Sensitive} {Learning} and a {New} {Benchmark} for {Human} {Parsing}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {Look into {Person}},
	url = {http://ieeexplore.ieee.org/document/8100198/},
	doi = {10.1109/CVPR.2017.715},
	abstract = {Human parsing has recently attracted a lot of research interests due to its huge application potentials. However existing datasets have limited number of images and annotations, and lack the variety of human appearances and the coverage of challenging cases in unconstrained environment. In this paper, we introduce a new benchmark1 “Look into Person (LIP)” that makes a signiﬁcant advance in terms of scalability, diversity and difﬁculty, a contribution that we feel is crucial for future developments in humancentric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels, which are captured from a wider range of viewpoints, occlusions and background complexity. Given these rich annotations we perform detailed analyses of the leading human parsing approaches, gaining insights into the success and failures of these methods. Furthermore, in contrast to the existing efforts on improving the feature discriminative capability, we solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into parsing results without resorting to extra supervision (i.e., no need for speciﬁcally labeling human joints in model training). Our self-supervised learning framework can be injected into any advanced neural networks to help incorporate rich high-level knowledge regarding human joints from a global perspective and improve the parsing results. Extensive evaluations on our LIP and the public PASCAL-PersonPart dataset demonstrate the superiority of our method.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Gong, Ke and Liang, Xiaodan and Zhang, Dongyu and Shen, Xiaohui and Lin, Liang},
	month = jul,
	year = {2017},
	pages = {6757--6765},
	file = {Gong et al. - 2017 - Look into Person Self-Supervised Structure-Sensit.pdf:/home/matthias/Zotero/storage/H6S978WI/Gong et al. - 2017 - Look into Person Self-Supervised Structure-Sensit.pdf:application/pdf}
}

@inproceedings{wei_convolutional_2016,
	address = {Las Vegas, NV, USA},
	title = {Convolutional {Pose} {Machines}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780880/},
	doi = {10.1109/CVPR.2016.511},
	abstract = {Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly reﬁned estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difﬁculty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wei, Shih-En and Ramakrishna, Varun and Kanade, Takeo and Sheikh, Yaser},
	month = jun,
	year = {2016},
	pages = {4724--4732},
	file = {Wei et al. - 2016 - Convolutional Pose Machines.pdf:/home/matthias/Zotero/storage/YCBLGD9Z/Wei et al. - 2016 - Convolutional Pose Machines.pdf:application/pdf}
}

@inproceedings{newell_stacked_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Stacked {Hourglass} {Networks} for {Human} {Pose} {Estimation}},
	isbn = {978-3-319-46484-8},
	abstract = {This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a “stacked hourglass” network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Newell, Alejandro and Yang, Kaiyu and Deng, Jia},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Human pose estimation},
	pages = {483--499},
	file = {Newell et al. - 2016 - Stacked Hourglass Networks for Human Pose Estimati.pdf:/home/matthias/Zotero/storage/3UCQ7PCS/Newell et al. - 2016 - Stacked Hourglass Networks for Human Pose Estimati.pdf:application/pdf}
}

@inproceedings{martinez_simple_2017,
	address = {Venice},
	title = {A {Simple} {Yet} {Effective} {Baseline} for 3d {Human} {Pose} {Estimation}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237550/},
	doi = {10.1109/ICCV.2017.288},
	abstract = {Following the success of deep convolutional networks, state-of-the-art methods for 3d human pose estimation have focused on deep end-to-end systems that predict 3d joint locations given raw image pixels. Despite their excellent performance, it is often not easy to understand whether their remaining error stems from a limited 2d pose (visual) understanding, or from a failure to map 2d poses into 3dimensional positions.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Martinez, Julieta and Hossain, Rayat and Romero, Javier and Little, James J.},
	month = oct,
	year = {2017},
	pages = {2659--2668},
	file = {Martinez et al. - 2017 - A Simple Yet Effective Baseline for 3d Human Pose .pdf:/home/matthias/Zotero/storage/Z2TTE5AP/Martinez et al. - 2017 - A Simple Yet Effective Baseline for 3d Human Pose .pdf:application/pdf}
}

@inproceedings{guler_densepose:_2018,
	address = {Salt Lake City, UT, USA},
	title = {{DensePose}: {Dense} {Human} {Pose} {Estimation} in the {Wild}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{DensePose}},
	url = {https://ieeexplore.ieee.org/document/8578860/},
	doi = {10.1109/CVPR.2018.00762},
	abstract = {In this work we establish dense correspondences between an RGB image and a surface-based representation of the human body, a task we refer to as dense human pose estimation. We gather dense correspondences for 50K persons appearing in the COCO dataset by introducing an efﬁcient annotation pipeline. We then use our dataset to train CNN-based systems that deliver dense correspondence ‘in the wild’, namely in the presence of background, occlusions and scale variations. We improve our training set’s effectiveness by training an inpainting network that can ﬁll in missing ground truth values and report improvements with respect to the best results that would be achievable in the past. We experiment with fully-convolutional networks and region-based models and observe a superiority of the latter. We further improve accuracy through cascading, obtaining a system that delivers highly-accurate results at multiple frames per second on a single gpu. Supplementary materials, data, code, and videos are provided on the project page http://densepose.org.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Guler, Riza Alp and Neverova, Natalia and Kokkinos, Iasonas},
	month = jun,
	year = {2018},
	pages = {7297--7306},
	file = {Guler et al. - 2018 - DensePose Dense Human Pose Estimation in the Wild.pdf:/home/matthias/Zotero/storage/H4N36ZJ9/Guler et al. - 2018 - DensePose Dense Human Pose Estimation in the Wild.pdf:application/pdf}
}

@inproceedings{khalid_multi-modal_2018,
	title = {Multi-{Modal} {Three}-{Stream} {Network} for {Action} {Recognition}},
	doi = {10.1109/ICPR.2018.8546131},
	abstract = {Human action recognition in video is an active yet challenging research topic due to high variation and complexity of data. In this paper, a novel video based action recognition framework utilizing complementary cues is proposed to handle this complex problem. Inspired by the successful two stream networks for action classification, additional pose features are studied and fused to enhance understanding of human action in a more abstract and semantic way. Towards practices, not only ground truth poses but also noisy estimated poses are incorporated in the framework with our proposed pre-processing module. The whole framework and each cue are evaluated on varied benchmarking datasets as JHMDB, sub-JHMDB and Penn Action. Our results outperform state-of-the-art performance on these datasets and show the strength of complementary cues.},
	booktitle = {2018 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Khalid, M. U. and Yu, J.},
	month = aug,
	year = {2018},
	keywords = {action classification, active yet challenging research topic, complementary cues, complex problem, feature extraction, Feature extraction, ground truth poses, human action recognition, image classification, image motion analysis, image recognition, image sequences, Interpolation, multimodal three-stream network, noisy estimated poses, object detection, Pattern recognition, Penn Action, pose estimation, successful two stream networks, Tensile stress, Three-dimensional displays, Training, Two dimensional displays, video based action recognition framework, video signal processing},
	pages = {3210--3215},
	file = {IEEE Xplore Abstract Record:/home/matthias/Zotero/storage/4L4TMCIJ/8546131.html:text/html;IEEE Xplore Full Text PDF:/home/matthias/Zotero/storage/TJG5VBLH/Khalid and Yu - 2018 - Multi-Modal Three-Stream Network for Action Recogn.pdf:application/pdf}
}

@incollection{simonyan_two-stream_2014,
	title = {Two-{Stream} {Convolutional} {Networks} for {Action} {Recognition} in {Videos}},
	url = {http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf},
	urldate = {2019-04-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Simonyan, Karen and Zisserman, Andrew},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {568--576},
	file = {NIPS Full Text PDF:/home/matthias/Zotero/storage/YQZ9UN6X/Simonyan and Zisserman - 2014 - Two-Stream Convolutional Networks for Action Recog.pdf:application/pdf;NIPS Snapshot:/home/matthias/Zotero/storage/BVM68IDS/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.html:text/html}
}

@inproceedings{cao_realtime_2017,
	address = {Honolulu, HI},
	title = {Realtime {Multi}-person 2D {Pose} {Estimation} {Using} {Part} {Affinity} {Fields}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099626/},
	doi = {10.1109/CVPR.2017.143},
	abstract = {We present an approach to efﬁciently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Afﬁnity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed ﬁrst in the inaugural COCO 2016 keypoints challenge, and signiﬁcantly exceeds the previous state-of-the-art result on the MPII MultiPerson benchmark, both in performance and efﬁciency.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
	month = jul,
	year = {2017},
	pages = {1302--1310},
	file = {Cao et al. - 2017 - Realtime Multi-person 2D Pose Estimation Using Par.pdf:/home/matthias/Zotero/storage/XP9UTUIT/Cao et al. - 2017 - Realtime Multi-person 2D Pose Estimation Using Par.pdf:application/pdf}
}

@incollection{leibe_keep_2016,
	address = {Cham},
	title = {Keep {It} {SMPL}: {Automatic} {Estimation} of 3D {Human} {Pose} and {Shape} from a {Single} {Image}},
	volume = {9909},
	isbn = {978-3-319-46453-4 978-3-319-46454-1},
	shorttitle = {Keep {It} {SMPL}},
	url = {http://link.springer.com/10.1007/978-3-319-46454-1_34},
	abstract = {We describe the ﬁrst method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we ﬁrst use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then ﬁt (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly ﬁt it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Bogo, Federica and Kanazawa, Angjoo and Lassner, Christoph and Gehler, Peter and Romero, Javier and Black, Michael J.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	doi = {10.1007/978-3-319-46454-1_34},
	pages = {561--578},
	file = {Bogo et al. - 2016 - Keep It SMPL Automatic Estimation of 3D Human Pos.pdf:/home/matthias/Zotero/storage/G7NY9DH2/Bogo et al. - 2016 - Keep It SMPL Automatic Estimation of 3D Human Pos.pdf:application/pdf}
}

@inproceedings{andriluka_posetrack:_2018,
	address = {Salt Lake City, UT, USA},
	title = {{PoseTrack}: {A} {Benchmark} for {Human} {Pose} {Estimation} and {Tracking}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{PoseTrack}},
	url = {https://ieeexplore.ieee.org/document/8578640/},
	doi = {10.1109/CVPR.2018.00542},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Andriluka, Mykhaylo and Iqbal, Umar and Insafutdinov, Eldar and Pishchulin, Leonid and Milan, Anton and Gall, Juergen and Schiele, Bernt},
	month = jun,
	year = {2018},
	pages = {5167--5176},
	file = {Andriluka et al. - 2018 - PoseTrack A Benchmark for Human Pose Estimation a.pdf:/home/matthias/Zotero/storage/B9MSZF5G/Andriluka et al. - 2018 - PoseTrack A Benchmark for Human Pose Estimation a.pdf:application/pdf}
}

@article{dantone_body_2014,
	title = {Body {Parts} {Dependent} {Joint} {Regressors} for {Human} {Pose} {Estimation} in {Still} {Images}},
	volume = {36},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/6802375/},
	doi = {10.1109/TPAMI.2014.2318702},
	abstract = {In this work, we address the problem of estimating 2d human pose from still images. Articulated body pose estimation is challenging due to the large variation in body poses and appearances of the different body parts. Recent methods that rely on the pictorial structure framework have shown to be very successful in solving this task. They model the body part appearances using discriminatively trained, independent part templates and the spatial relations of the body parts using a tree model. Within such a framework, we address the problem of obtaining better part templates which are able to handle a very high variation in appearance. To this end, we introduce parts dependent body joint regressors which are random forests that operate over two layers. While the ﬁrst layer acts as an independent body part classiﬁer, the second layer takes the estimated class distributions of the ﬁrst one into account and is thereby able to predict joint locations by modeling the interdependence and co-occurrence of the parts. This helps to overcome typical ambiguities of tree structures, such as self-similarities of legs and arms. In addition, we introduce a novel dataset termed F ashionP ose that contains over 7, 000 images with a challenging variation of body part appearances due to a large variation of dressing styles. In the experiments, we demonstrate that the proposed parts dependent joint regressors outperform independent classiﬁers or regressors. The method also performs better or similar to the state-of-the-art in terms of accuracy, while running with a couple of frames per second.},
	language = {en},
	number = {11},
	urldate = {2019-04-25},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Dantone, Matthias and Gall, Juergen and Leistner, Christian and Van Gool, Luc},
	month = nov,
	year = {2014},
	pages = {2131--2143},
	file = {Dantone et al. - 2014 - Body Parts Dependent Joint Regressors for Human Po.pdf:/home/matthias/Zotero/storage/MA3YZMKS/Dantone et al. - 2014 - Body Parts Dependent Joint Regressors for Human Po.pdf:application/pdf}
}

@article{sigal_humaneva:_2010,
	title = {{HumanEva}: {Synchronized} {Video} and {Motion} {Capture} {Dataset} and {Baseline} {Algorithm} for {Evaluation} of {Articulated} {Human} {Motion}},
	volume = {87},
	issn = {0920-5691, 1573-1405},
	shorttitle = {{HumanEva}},
	url = {http://link.springer.com/10.1007/s11263-009-0273-6},
	doi = {10.1007/s11263-009-0273-6},
	language = {en},
	number = {1-2},
	urldate = {2019-04-25},
	journal = {International Journal of Computer Vision},
	author = {Sigal, Leonid and Balan, Alexandru O. and Black, Michael J.},
	month = mar,
	year = {2010},
	pages = {4--27},
	file = {Sigal et al. - 2010 - HumanEva Synchronized Video and Motion Capture Da.pdf:/home/matthias/Zotero/storage/4YD9UFS2/Sigal et al. - 2010 - HumanEva Synchronized Video and Motion Capture Da.pdf:application/pdf}
}

@inproceedings{jhuang_towards_2013,
	title = {Towards understanding action recognition},
	abstract = {Although action recognition in videos is widely studied, current methods often fail on real-world datasets. Many recent approaches improve accuracy and robustness to cope with challenging video sequences, but it is often unclear what affects the results most. This paper attempts to provide insights based on a systematic performance evaluation using thoroughly-annotated data of human actions. We annotate human Joints for the HMDB dataset (J-HMDB). This annotation can be used to derive ground truth optical ﬂow and segmentation. We evaluate current methods using this dataset and systematically replace the output of various algorithms with ground truth. This enables us to discover what is important – for example, should we work on improving ﬂow algorithms, estimating human bounding boxes, or enabling pose estimation? In summary, we ﬁnd that highlevel pose features greatly outperform low/mid level features; in particular, pose over time is critical. While current pose estimation algorithms are far from perfect, features extracted from estimated pose on a subset of J-HMDB, in which the full body is visible, outperform low/mid-level features. We also ﬁnd that the accuracy of the action recognition framework can be greatly increased by reﬁning the underlying low/mid level features; this suggests it is important to improve optical ﬂow and human detection algorithms. Our analysis and J-HMDB dataset should facilitate a deeper understanding of action recognition algorithms.},
	language = {en},
	booktitle = {Proceedings of the {IEEE} international conference on computer vision},
	author = {Jhuang, Hueihan and Gall, Juergen and Zufﬁ, Silvia and Schmid, Cordelia and Black, Michael J},
	year = {2013},
	pages = {3192--3199},
	file = {Jhuang et al. - Towards understanding action recognition.pdf:/home/matthias/Zotero/storage/7J7C65KD/Jhuang et al. - Towards understanding action recognition.pdf:application/pdf}
}

@inproceedings{zhang_actemes_2013,
	title = {From {Actemes} to {Action}: {A} {Strongly}-{Supervised} {Representation} for {Detailed} {Action} {Understanding}},
	shorttitle = {From {Actemes} to {Action}},
	doi = {10.1109/ICCV.2013.280},
	abstract = {This paper presents a novel approach for analyzing human actions in non-scripted, unconstrained video settings based on volumetric, x-y-t, patch classifiers, termed actemes. Unlike previous action-related work, the discovery of patch classifiers is posed as a strongly-supervised process. Specifically, key point labels (e.g., position) across space time are used in a data-driven training process to discover patches that are highly clustered in the space time key point configuration space. To support this process, a new human action dataset consisting of challenging consumer videos is introduced, where notably the action label, the 2D position of a set of key points and their visibilities are provided for each video frame. On a novel input video, each acteme is used in a sliding volume scheme to yield a set of sparse, non-overlapping detections. These detections provide the intermediate substrate for segmenting out the action. For action classification, the proposed representation shows significant improvement over state-of-the-art low-level features, while providing spatiotemporal localization as additional output, which sheds further light into detailed action understanding.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Zhang, W. and Zhu, M. and Derpanis, K. G.},
	month = dec,
	year = {2013},
	keywords = {actemes, action classification, action detection, action label, action understanding, action-related work, Cameras, consumer videos, Context, data-driven training process, gesture recognition, human action dataset, human actions analyzing, image classification, image representation, patch classifiers discovery, Penn Action, Semantics, sliding volume scheme, space time, spacetime key-point configuration space, sparse nonoverlapping detections, spatiotemporal localization, Spatiotemporal phenomena, supervised representation, Training, Trajectory, unconstrained video settings, video signal processing, Visualization},
	pages = {2248--2255},
	file = {IEEE Xplore Abstract Record:/home/matthias/Zotero/storage/JH7MIMKB/6751390.html:text/html;IEEE Xplore Full Text PDF:/home/matthias/Zotero/storage/6B6GJATY/Zhang et al. - 2013 - From Actemes to Action A Strongly-Supervised Repr.pdf:application/pdf}
}

@incollection{tompson_joint_2014,
	title = {Joint {Training} of a {Convolutional} {Network} and a {Graphical} {Model} for {Human} {Pose} {Estimation}},
	url = {http://papers.nips.cc/paper/5573-joint-training-of-a-convolutional-network-and-a-graphical-model-for-human-pose-estimation.pdf},
	urldate = {2019-04-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Tompson, Jonathan J and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {1799--1807},
	file = {NIPS Full Text PDF:/home/matthias/Zotero/storage/WT2YDKQR/Tompson et al. - 2014 - Joint Training of a Convolutional Network and a Gr.pdf:application/pdf;NIPS Snapshot:/home/matthias/Zotero/storage/DHH68QVG/5573-joint-training-of-a-convolutional-network-and-a-graphical-model-for-human-pose-estimation.html:text/html}
}

@inproceedings{carreira_human_2016,
	address = {Las Vegas, NV, USA},
	title = {Human {Pose} {Estimation} with {Iterative} {Error} {Feedback}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780881/},
	doi = {10.1109/CVPR.2016.512},
	abstract = {Hierarchical feature extractors such as Convolutional Networks (ConvNets) have achieved impressive performance on a variety of classiﬁcation tasks using purely feedforward processing. Feedforward architectures can learn rich representations of the input space but do not explicitly model dependencies in the output spaces, that are quite structured for tasks such as articulated human pose estimation or object segmentation. Here we propose a framework that expands the expressive power of hierarchical feature extractors to encompass both input and output spaces, by introducing top-down feedback. Instead of directly predicting the outputs in one go, we use a self-correcting model that progressively changes an initial solution by feeding back error predictions, in a process we call Iterative Error Feedback (IEF). IEF shows excellent performance on the task of articulated pose estimation in the challenging MPII and LSP benchmarks, matching the state-of-the-art without requiring ground truth scale annotation.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Carreira, Joao and Agrawal, Pulkit and Fragkiadaki, Katerina and Malik, Jitendra},
	month = jun,
	year = {2016},
	pages = {4733--4742},
	file = {Carreira et al. - 2016 - Human Pose Estimation with Iterative Error Feedbac.pdf:/home/matthias/Zotero/storage/MDWP3UJ5/Carreira et al. - 2016 - Human Pose Estimation with Iterative Error Feedbac.pdf:application/pdf}
}

@inproceedings{hu_bottom-up_2016,
	address = {Las Vegas, NV, USA},
	title = {Bottom-{Up} and {Top}-{Down} {Reasoning} with {Hierarchical} {Rectified} {Gaussians}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780973/},
	doi = {10.1109/CVPR.2016.604},
	abstract = {Convolutional neural nets (CNNs) have demonstrated remarkable performance in recent history. Such approaches tend to work in a “unidirectional” bottom-up feed-forward fashion. However, practical experience and biological evidence tells us that feedback plays a crucial role, particularly for detailed spatial understanding tasks. This work explores “bidirectional” architectures that also reason with top-down feedback: neural units are inﬂuenced by both lower and higher-level units.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Hu, Peiyun and Ramanan, Deva},
	month = jun,
	year = {2016},
	pages = {5600--5609},
	file = {Hu and Ramanan - 2016 - Bottom-Up and Top-Down Reasoning with Hierarchical.pdf:/home/matthias/Zotero/storage/W52WHAQF/Hu and Ramanan - 2016 - Bottom-Up and Top-Down Reasoning with Hierarchical.pdf:application/pdf}
}

@article{bulat_human_2016,
	title = {Human pose estimation via {Convolutional} {Part} {Heatmap} {Regression}},
	url = {http://arxiv.org/abs/1609.01743},
	doi = {10.1007/978-3-319-46478-7_44},
	abstract = {This paper is on human pose estimation using Convolutional Neural Networks. Our main contribution is a CNN cascaded architecture specifically designed for learning part relationships and spatial context, and robustly inferring pose even for the case of severe part occlusions. To this end, we propose a detection-followed-by-regression CNN cascade. The first part of our cascade outputs part detection heatmaps and the second part performs regression on these heatmaps. The benefits of the proposed architecture are multi-fold: It guides the network where to focus in the image and effectively encodes part constraints and context. More importantly, it can effectively cope with occlusions because part detection heatmaps for occluded parts provide low confidence scores which subsequently guide the regression part of our network to rely on contextual information in order to predict the location of these parts. Additionally, we show that the proposed cascade is flexible enough to readily allow the integration of various CNN architectures for both detection and regression, including recent ones based on residual learning. Finally, we illustrate that our cascade achieves top performance on the MPII and LSP data sets. Code can be downloaded from http://www.cs.nott.ac.uk/{\textasciitilde}psxab5/},
	urldate = {2019-04-25},
	journal = {arXiv:1609.01743 [cs]},
	author = {Bulat, Adrian and Tzimiropoulos, Georgios},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.01743},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: accepted to ECCV 2016},
	file = {arXiv\:1609.01743 PDF:/home/matthias/Zotero/storage/WIPV68ZS/Bulat and Tzimiropoulos - 2016 - Human pose estimation via Convolutional Part Heatm.pdf:application/pdf;arXiv.org Snapshot:/home/matthias/Zotero/storage/GWTEGHCA/1609.html:text/html}
}

@inproceedings{rafi_efficient_2016,
	address = {York, UK},
	title = {An {Efficient} {Convolutional} {Network} for {Human} {Pose} {Estimation}},
	isbn = {978-1-901725-59-9},
	url = {http://www.bmva.org/bmvc/2016/papers/paper109/index.html},
	doi = {10.5244/C.30.109},
	abstract = {In recent years, human pose estimation has greatly beneﬁted from deep learning and huge gains in performance have been achieved. The trend to maximise the accuracy on benchmarks, however, resulted in computationally expensive deep network architectures that require expensive hardware and pre-training on large datasets. This makes it difﬁcult to compare different methods and to reproduce existing results. In this paper, we therefore propose an efﬁcient deep network architecture that can be efﬁciently trained on mid-range GPUs without the need of any pre-training. Despite the low computational requirements of our network, it is on par with much more complex models on popular benchmarks for human pose estimation.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2016},
	publisher = {British Machine Vision Association},
	author = {Rafi, Umer and Leibe, Bastian and Gall, Juergen and Kostrikov, Ilya},
	year = {2016},
	pages = {109.1--109.11},
	file = {Rafi et al. - 2016 - An Efficient Convolutional Network for Human Pose .pdf:/home/matthias/Zotero/storage/VLD48VTN/Rafi et al. - 2016 - An Efficient Convolutional Network for Human Pose .pdf:application/pdf}
}

@inproceedings{pfister_flowing_2015,
	address = {Santiago, Chile},
	title = {Flowing {ConvNets} for {Human} {Pose} {Estimation} in {Videos}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410579/},
	doi = {10.1109/ICCV.2015.222},
	abstract = {The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to beneﬁt from temporal context by combining information across the multiple frames using optical ﬂow.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Pfister, Tomas and Charles, James and Zisserman, Andrew},
	month = dec,
	year = {2015},
	pages = {1913--1921},
	file = {Pfister et al. - 2015 - Flowing ConvNets for Human Pose Estimation in Vide.pdf:/home/matthias/Zotero/storage/S2PBL27W/Pfister et al. - 2015 - Flowing ConvNets for Human Pose Estimation in Vide.pdf:application/pdf}
}

@inproceedings{charles_personalizing_2016,
	title = {Personalizing {Human} {Video} {Pose} {Estimation}},
	abstract = {We propose a personalized ConvNet pose estimator that automatically adapts itself to the uniqueness of a person’s appearance to improve pose estimation in long videos.},
	language = {en},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Charles, James and Pfister, Tomas and Magee, Derek and Hogg, David and Zisserman, Andrew},
	year = {2016},
	pages = {3063--3072},
	file = {Charles et al. - Personalizing Human Video Pose Estimation.pdf:/home/matthias/Zotero/storage/ZL7IF5ZC/Charles et al. - Personalizing Human Video Pose Estimation.pdf:application/pdf}
}

@article{iqbal_pose_2016,
	title = {Pose for {Action} - {Action} for {Pose}},
	url = {http://arxiv.org/abs/1603.04037},
	abstract = {In this work we propose to utilize information about human actions to improve pose estimation in monocular videos. To this end, we present a pictorial structure model that exploits high-level information about activities to incorporate higher-order part dependencies by modeling action specific appearance models and pose priors. However, instead of using an additional expensive action recognition framework, the action priors are efficiently estimated by our pose estimation framework. This is achieved by starting with a uniform action prior and updating the action prior during pose estimation. We also show that learning the right amount of appearance sharing among action classes improves the pose estimation. We demonstrate the effectiveness of the proposed method on two challenging datasets for pose estimation and action recognition with over 80,000 test images.},
	urldate = {2019-04-25},
	journal = {arXiv:1603.04037 [cs]},
	author = {Iqbal, Umar and Garbade, Martin and Gall, Juergen},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.04037},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to FG-2017},
	file = {arXiv\:1603.04037 PDF:/home/matthias/Zotero/storage/YFP952VJ/Iqbal et al. - 2016 - Pose for Action - Action for Pose.pdf:application/pdf;arXiv.org Snapshot:/home/matthias/Zotero/storage/977VHYE5/1603.html:text/html}
}

@inproceedings{gkioxari_chained_2016,
	address = {Cham},
	title = {Chained {Predictions} {Using} {Convolutional} {Neural} {Networks}},
	isbn = {978-3-319-46493-0},
	abstract = {In this work, we present an adaptation of the sequence-to-sequence model for structured vision tasks. In this model, the output variables for a given input are predicted sequentially using neural networks. The prediction for each output variable depends not only on the input but also on the previously predicted output variables. The model is applied to spatial localization tasks and uses convolutional neural networks (CNNs) for processing input images and a multi-scale deconvolutional architecture for making spatial predictions at each step. We explore the impact of weight sharing with a recurrent connection matrix between consecutive predictions, and compare it to a formulation where these weights are not tied. Untied weights are particularly suited for problems with a fixed sized structure, where different classes of output are predicted at different steps. We show that chain models achieve top performing results on human pose estimation from images and videos.},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Gkioxari, Georgia and Toshev, Alexander and Jaitly, Navdeep},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	pages = {728--743},
	file = {Gkioxari et al. - 2016 - Chained Predictions Using Convolutional Neural Net.pdf:/home/matthias/Zotero/storage/F2CKZYDX/Gkioxari et al. - 2016 - Chained Predictions Using Convolutional Neural Net.pdf:application/pdf}
}