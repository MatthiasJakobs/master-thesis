\documentclass[twoside]{article}

\usepackage{lipsum} % Package to generate dummy text throughout this template

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage[utf8]{inputenc}
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage{multicol} % Used for the two-column layout of the document
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{float} % Required for tables and figures in the multi-column environment - they need to be placed in specific locations with the [H] (e.g. \begin{table}[H])
\usepackage{hyperref} % For hyperlinks in the PDF

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text
\usepackage{paralist} % Used for the compactitem environment which makes bullet points with less space between them

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\Roman{subsection}} % Roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{cite}
\usepackage{paralist, tabularx}
\usepackage{graphicx}
\graphicspath{{../images/}}


\usepackage{float}


%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{\vspace{-15mm}\fontsize{24pt}{10pt}\selectfont\textbf{Towards a state-of-the-art end-to-end Human Activity Recognition framework}} % Article title

\author{
\large
\textsc{Matthias Jakobs}\\[2mm] % Your name
\normalsize \href{mailto:matthias.jakobs@tu-dortmund.de}{matthias.jakobs@tu-dortmund.de} % Your email address
\vspace{-5mm}
}
\date{}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Insert title

\thispagestyle{fancy} % All pages have headers and footers

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

%\begin{abstract}
\vspace{20px}

%\end{abstract}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\begin{multicols}{2} % Two-column layout throughout the main article text

\section{Motivation}
\label{sec:motivation}


% With such a system, a large variety of training data in different, \textit{unconstrained} environments can be captured, hopefully improving the quality of the HAR process.

% One advantage of using motion capturing systems to capture human poses is that they do not suffer from occlusion because of the variety of camera angles.
% In monocular video, however, occlusion is a serious concern.
% For example, the action of holding a phone to ones ear might occlude the hand joint, depending on the camera angle.
% Current state-of-the-art methods do not address this problem directly and thus evaluating these models on the occlusion problem and further experimentation in that regard will be the main proposal of this master thesis.

% In addition to the direct contribution to \cite{reining_towards_2018}, pose information can be incorporated into HAR for more accurate results \cite{khalid_multi-modal_2018}.
% While the authors focused on 2D pose data 3D pose data could improve the accuracy even more.


% 1. HAR basics
% 2. HAR im warenhaus. benutzt mocap. langsam, aufw√§ndig, teuer
% 3. optimal: nur aus video. Ein ansatz ist paper
% 4. promising results. but: evaluation with regards to 2d action recognition on small dataset. better and bigger datasets available

Human Activity Recognition (HAR) is a task where a signal capturing different human activities (like video) is segmented and classified.
The granularity of the activities to be classified depends on the domain in which HAR is used.

Recently,\cite{reining_towards_2018} proposed a framework for HAR in the context of order picking in a warehouse.
The authors used a motion capturing system in a controlled environment to create a skeleton of the person performing the action.
They propose a pipeline for HAR using \textit{attributes} like \textit{arms reaching forward} and \textit{upper body moving back} as features, extracted from skeleton data.

A motion capturing system requires strict laboratory conditions.
In recent years, approaches for 3D pose estimation were developed which can be used to generate skeleton data from monocular video and replace motion capturing systems (see \textbf{section \ref{sec:recent}}).
These approaches are not only more cost efficient but also more practical since the training data can be more realistic and diverse (more "in-the-wild") as opposed to laboratory data.

Pose estimation and action recognition were mostly viewed as separate problems.
\cite{luvizon_2d/3d_2018} proposed an end-to-end learning approach for jointly estimating pose and recognizing action, showing promising results on many standard benchmarks (see \textbf{section \ref{sec:experiment}.\ref{sec:dataset}} for an explanation).
However, their evaluation benchmark for action recognition from 2D video, Penn Action \cite{zhang_actemes_2013}, is a small dataset with just 15 different action labels.
For a better understanding of the benefits of jointly learning pose and action recognition evaluation on a more complex dataset is necessary.

Thus, this master thesis proposes to \textbf{a)} replicate the work of \cite{luvizon_2d/3d_2018} and evaluating it on more recent and complex benchmarks and \textbf{b)} extend their work by incorporating recent additions to pose estimators, presented in the next section.

%------------------------------------------------
\section{Recent Work}
\label{sec:recent}
3D pose estimation from video developed in recent years, building on the work of 2D pose estimation in still images and video.
The most common approach is to first generate the 2D pose using a dedicated estimator and then \textit{lift} it into the 3D space \cite{pavllo_3d_2019}\cite{martinez_simple_2017}.
This two-step approach outperforms traditional end-to-end approaches \cite{pavllo_3d_2019}.
State-of-the-art 2D pose estimators like \cite{newell_stacked_2016}\cite{wei_convolutional_2016} are often used for generating 2D pose data.
In \cite{wei_convolutional_2016} the authors propose a \textit{pose machine} based on fully convolutional neural networks which refines an initial, local estimation of joint positions through multiple stages.
Each stage increases the receptive field, resolving the ambiguity of local features.
In \cite{newell_stacked_2016} the authors utilize a sequence of \textit{hourglass} modules.
Such a module consists of a symmetric architecture where the input gets scaled down through convolution and pooling layers and then scaled back up by nearest neighbor upsampling.
Through skip connections features from previous layers are preserved for use in later layers.

A state-of-the-art model for lifting 2D poses into 3D space was recently proposed by \cite{pavllo_3d_2019}.
One of the key additions is the use of temporal information between single frames.
While some work on video has been done prior (\cite{lin_recurrent_2017}\cite{hossain_exploiting_2018}) the authors approach to temporal information processing by using temporal convolutions achieves state-of-the-art performance on the common benchmarks.
Also, the authors use different 2D pose estimators based on \cite{he_mask_2017} and \cite{chen_cascaded_2018}, suggesting that improvements in 2D pose estimators are still important and necessary to develop better 3D pose estimators.

As mentioned in \textbf{section \ref{sec:motivation}} \cite{luvizon_2d/3d_2018} use an end-to-end learning approach for jointly learning poses and recognize actions, outperforming previous 3D pose estimators like \cite{martinez_simple_2017}.
Traditionally, skeletal joints are detected as heatmaps.
Each heatmap $H_i$ contains the likelihood of each $(x,y)$ coordinate corresponding to the joint position $i$.
The authors in \cite{luvizon_2d/3d_2018}, however, use a \textit{soft-argmax}\cite{luvizon_human_2017} to turn the heatmaps into $(x,y)$ coordinates instead of using traditional softmax as a post-processing step.
Thus, their estimator is fully differentiable and can be incorporated into an end-to-end training pipeline.

%------------------------------------------------
\section{Method}

The primary goal of this master thesis is to reproduce the promising results presented in \cite{luvizon_2d/3d_2018}.
After implementing their pipeline in PyTorch \cite{paszke_automatic_2017} the model will be evaluated against HMDB \cite{kuehne_hmdb:_2011} and Kinetics \cite{kay_kinetics_2017} benchmarks.
Also, since the authors published their paper, better approaches for 2D and 3D pose estimation have emerged \cite{pavllo_3d_2019}, outperforming their model on the Human3.6M benchmark (see \textbf{section \ref{sec:experiment}.\ref{sec:dataset}}).
Thus, the secondary goal of this thesis will be to incorporate these novel ideas into the end-to-end model.
By incorporating these ideas this thesis proposes state-of-the-art performance on the previously mentioned action recognition and pose estimation benchmarks.

%------------------------------------------------
\section{Experiments}
\label{sec:experiment}
\subsection{Dataset}
\label{sec:dataset}

In evaluating 2D pose estimators, three datasets are most commonly used.
MPII Human Pose \cite{andriluka_2d_2014} is often used for single person and multi person 2D pose estimation.
Using Youtube videos as source the authors gathered approximately 40,000 annotated images of nearly 500 different actions performed by humans.
These actions include (but are not limited to) activities like skiing, playing music, carrying boxes etc.

In the sport domain, Leeds Sport Pose (LSP) \cite{johnson_clustered_2010} and LSP-Extended \cite{johnson_learning_2011} are often used.
While they focus on a narrow domain many 2D pose estimators were evaluated using these medium sized datasets.

For 3D pose benchmarks, Human3.6M \cite{ionescu_human3.6m:_2014} is widely used.
11 actors perform 17 different scenarios (like eating, talking on phone etc.) in a controlled lab environment wearing motion capturing suits.
The dataset provides 2D and 3D Pose at the same time and is thus widely used for evaluating the lifting task from 2D to 3D described in \textbf{section \ref{sec:recent}}.

In addition to Human3.6M HumanEva I \cite{sigal_humaneva:_2010} is used.
While smaller than Human3.6M, HumanEva I is often used to evaluate specific actions like walking and jogging.

With regards to action recognition, \cite{luvizon_2d/3d_2018} focus on evaluating 2D action recognition on the Penn Action dataset \cite{zhang_actemes_2013}.
This dataset features approximately 2300 video clips of 15 different actions.
The authors gathered the video clips from Youtube, which means that they are diverse with regards to setting and background.

Another common action recognition benchmark is HMDB \cite{kuehne_hmdb:_2011}.
It consists of 51 different actions with at least 101 video clips per action.
The video files were gathered from Youtube as well as from movies.

A more recent benchmark focuses on evaluating even more actions. Kinetics \cite{kay_kinetics_2017} features 400 different actions with at least 400 video clips per action. Again, these clips were collected from Youtube.


\subsection{Metric}

Most 3D pose estimation models use the mean per-joint error (in millimeters) as their main metric.
Given the predicted 3D joint locations $x_0^{*}, \dots, x_n^{*}$ for $n$ joints it computes the mean difference given the ground-truth $\hat{x_i}$ using

\newcommand\norm[1]{\left\Vert#1\right\Vert}

\[
    e = \frac{1}{n} \sum_{i=1}^{n} \norm{\hat{x_i} - x_i^{*}}_2
\]

As this metric does not take different scales and orientation into account, the root locations of the vectors are aligned to make the results translation invariant and better comparable.

When evaluating action recognition, accuracy is used.
Each video clip evaluated consists of a person performing a single action.
Thus, the overall accuracy over the test set is reported.


%------------------------------------------------
%   REFERENCE LIST
%------------------------------------------------
\bibliographystyle{acm}
\bibliography{../bibliography}
%----------------------------------------------------------------------------------------

\end{multicols}

\end{document}
