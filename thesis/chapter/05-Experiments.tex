\chapter{Experiments}
\label{sec:chapter5}
In the previous chapter, the method for Human Activity Recognition by \cite{luvizon_2d/3d_2018} was introduced.
In addition, this thesis proposed experiments to gain a better insight into the authors method.
This chapter first introduces the datasets used in the experiments in Section \ref{sec:exp-datasets}, specifcally the \textit{MPII Human Pose} dataset for $2D$ pose estimation, and the \textit{Penn Action} and \textit{JHMDB} datasets for Human Activity Recognition.
Afterwards, the metrics used in the authors work, as well as the experiments done in this thesis, are discussed in Section \ref{sec:exp-metrics}.
Lastly, this chapter discusses the experiment setups, as well as the results, in Section \ref{sec:exp-results}.
First, some experiments from \cite{luvizon_2d/3d_2018} are recreated, focusing on the $2D$ pose estimation and $2D$ Human Activity Recognition experiments.
Second, further experiementation towards understanding the capabilities of the model are presented, including a qualitative and quantitative evaluation of the Soft-argmax function, the accuracy of the authors model achieved on the complex JHMDB dataset, as well as an approach for training the model in an end-to-end approach, without using a pretrained pose estimator.
% TODO: if time series model: introduce too

\section{Datasets}
\label{sec:exp-datasets}

\subsection{MPII Human Pose}
\label{sec:exp-mpii}

In \cite{andriluka_2d_2014}, the authors present a dataset for 2D human pose estimation on still images.
It contains $25.000$ images, where $40.000$ persons were annotated.
The annotations include $16$ joint annotations in addition to an indicator of whether or not the joint is visible or not.
The $16$ joints are \textit{left / right ankle}, \textit{left / right knee}, \textit{left / right hip}, \textit{left / right elbow}, \textit{left / right shoulder}, \textit{left / right wrist}, \textit{pelvis}, \textit{thorax}, \textit{upper neck} and \textit{top of the head}.
See \fref{fig:mpii_example_images} for example images of the dataset.
In addition, the body center coordinates are given in addition to a scale indicating the size of the person bounding box w.r.t. $200$ pixels.
Also, a bounding box of the head is given, which is used to compute the \textit{PCKh} metric (see \sref{sec:exp-pckh}).
The images were extracted from YouTube videos and do not contain artifacts commonly found in videos like compression or blur.
Additionally, each image is assigned an activity performed in the video it was extracted from, totalling $401$ total activities.
However, these annotations were not used for Human Action Recognition since they are too fine-grained and the number of samples per activity is too low for training a deep neural network.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.7\textwidth]{mpii_example_images.png}
    \caption{Four example images from the MPII dataset \cite{andriluka_2d_2014}. }
    \label{fig:mpii_example_images}
\end{figure}

We follow the approaches from \cite{luvizon_2d/3d_2018} for preprocessing, which the authors present in their supplemental material.
First, a person bounding box is estimated using the center body annotation from the dataset.
The authors multiply the scale given by the annotation $s_{orig}$ by $1.25$, resulting in $s_{new}$.
They do not motivate the reason for using this specific value, but it most likely was used to enlarge the bounding box to contain more context around the person.
Next, they compute the width and height using $s_{new} \cdot 200$, which results in a square bounding box.
In addition, the authors also alter the center position $(c_x,  c_y)$ given by the annotation by computing $(c_{x}^{new}, c_y^{new}) = (c_x, c_y + s_{new} \cdot 12)$.
Again, the authors do not provide a reasoning for moving the center position further towards the neck of the person in this way.
Once the bounding box is computed, the image is cropped to the size of the bounding box around the newly computed center coordinate and rescalled to a size of $256 \times 256$ .
In the case where a joint annotation falls outside of the now cropped image, the authors set the visibility of the joint to $0$ and set the $(x,y)$ coordinates of the joint to $(-1e9, -1e9)$.

Additionally, the authors introduce parameters used for augmentation.
These values are sampled from their respective sets whenever augmentation is performed.
Specifically, they introduce $s_{aug} \in \{0.7, 1, 1.3\}$ which gets multiplied with $s_{new}$ computed earlier.
Also, $r_{aug} \in \{-40, -35, \dots, 35, 40\}$ is introduced to rotate the image $r_{aug}$ degrees around its center, possibly introducing black borders around the image.
Moreover, when augmenting, the image is horizontally flipped with a chance of $50$ percent.
See \fref{fig:mpii_example_augmentation} for multiple examples of different augmented images, including augmented pose. 

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.99\textwidth]{mpii_dataset_augmented_examples.png}
    \caption{\textbf{From left to right}: \textbf{1.} Original image from the MPII dataset. \textbf{2.} Original image with the ground truth pose superimposed. \textbf{3.} Image after estimating the bounding box, cropping and rescaling. \textbf{4.} Augmented image using $s_{aug} = 1.3$, $r_{aug} = -15$ degrees and flipping the image vertically.}
    \label{fig:mpii_example_augmentation}
\end{figure}

When using horizontal flipping, a problem occured where the learned pose resembled a generic skeleton.
See \fref{fig:ghost_skeleton} for an example image.
Because of the horizontal flipping of the image, the pose needed to be flipped as well.
Furthermore, however, the labels of some joints needed to be changed as well.
As an example, the right arm (from the perspective of the subject in the image) becomes the left arm when the image is flipped.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.80\textwidth]{ghost_skeleton.png}
    \caption{Example of a phenomenon which occured when the image and pose were flipped durring augmentation, but the annotations were not properly changed. As an example, the left arm becomes the right arm after flipping. The ground truth pose is shown in red, while the learned pose is shown in blue. }
    \label{fig:ghost_skeleton}
\end{figure}

The dataset does not contain annotations for the test data, other than the scale and center coordinates.
To evaluate the test images, the joints need to be evaluated and the results need to be send to the authors for comparison to the ground truth pose.
This ensures that the test data annotations are not mistakenly or maliciously used in training.
For all datasets used in the experiments, $10$ percent of the training datapoints were withheld from training and used as validation data.

\subsection{Penn Action}
\label{sec:exp-penn}

Another dataset used in \cite{luvizon_2d/3d_2018} is the Penn Action dataset \cite{zhang_actemes_2013}.
It contains $2326$ video clips of $15$ different actions performed.
The actions performed are mostly sports related and include \textit{baseball swing}, \textit{clean and jerk}, \textit{jumping jacks}, \textit{pushup}, \textit{strum guitar}, \textit{bench press}, \textit{golf swing}, \textit{baseball pitch}, \textit{situp}, \textit{tennis forehand}, \textit{bowling}, \textit{jump rope}, \textit{pullup}, \textit{squat} and \textit{tennis serve}.
In addition, the authors provide annotations for $13$ body joints,
including \textit{left and right shoulders, elbows, wrists, hips and knees} as well as one annotation of the \textit{head} of the person.
Some example images can be seen in \fref{fig:pennaction_example_images}. 

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.7\textwidth]{pennaction_example_images.png}
    \caption{Four example images from the Penn Action dataset \cite{zhang_actemes_2013}. }
    \label{fig:pennaction_example_images}
\end{figure}

It was decided by \cite{luvizon_2d/3d_2018} that the number of joints should be identical to the ones presented in \cite{andriluka_2d_2014} \sref{sec:exp-mpii} because the network assumes a fixed number of joints.
This then means that the network architecture does not need to be changed, no matter which dataset is used.
Specifically, the \textit{pose cube} dimensionality does not need to be changed.
To achieve this, the \textit{head} annotation of the Penn Action dataset is mapped to the \textit{upper neck} joint of the MPII dataset and the missing joints were interpreted as not visible.

It was decided to introduce additional augmentation since the dataset is smaller in comparison to MPII and because it was observed that the HAR models were overfitting on the training data.
Thus, two additional augmentation methods were used.
First, with a probability of $0.5$, salt and pepper augmentation was applyied to the training images.
In this augmentation method, random noise in the form of white and black pixels gets applied to the image.
Second, using an augmentation strategy called Dropout, black rectangles are placed randomly inside the image, covering chunks of the original image.
The last method was also introduced on half of the test images.
The network then needs to make decisions on partially occluded images, making it more robust, because it forces the network to utilize alternative paths on some images, effectively making the model more general.

For some experiments, we compute a ground truth bounding box of the person by calculating the minimum and maximum $x$ and $y$ coordinates of the pose and defining these as the corners of the bounding box.
Additionally, to include more context, we decided to increase the bounding box by $30$ pixels in both dimensions.

Additionally, the authors in \cite{luvizon_2d/3d_2018} decide to process the video clips in chunks of $16$ frames.
This is again important since the network architecture can be kept the same because \textit{pose cube} dimensions do not change that way.
Thus, we decided to precompute $16$ frames subclips, which we call \textit{fragments}, and save them separately to increase the training speed, since the preprocessing and extraction of the subclips does not need to be done at training time. 

\subsection{JHMDB}
\label{sec:exp-jhmdb}

Similar to \cite{zhang_actemes_2013} \sref{sec:exp-penn}, the JHMDB dataset \cite{jhuang_towards_2013} contains annotations for pose and action in video clips.
The dataset was created by taking a subset of the HMDB action recogntion dataset \cite{kuehne_hmdb:_2011}, which was then annotated using the \textit{puppet tool} \cite{zuffi_pictorial_2012}.
This tool allows to not only annotate the pose of the person but also automatically computes a binary segmentation map of the person, further referred to as the \textit{puppet mask}.
See \fref{fig:puppet_tool_example} for a visualization of the annotation process and the puppet tool.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.4\textwidth]{puppet_tool_example.png}
    \caption{Example of an image, annotated using the puppet flow. The dots indicate the joint positions, while the transparent human figure prior automatically adjusts, yielding the human segmentation map. Notice that, by using the figure, even joints that are not visible (like the ankles) still get anatomically plausible annotations. Image taken from \cite{max_planck_institute_for_intelligent_systems_jhmdb_nodate}.}
    \label{fig:puppet_tool_example}
\end{figure}

The clips in the HMDB dataset were taken from YouTube.
The annotated subset contains the following actions:
\textit{brush hair}, \textit{catch}, \textit{clap}, \textit{climb stair}, \textit{golf}, \textit{jump}, \textit{kick ball}, \textit{pick}, \textit{pour}, \textit{pullup}, \textit{push}, \textit{run}, \textit{shoot ball}, \textit{shoot bow}, \textit{shoot gun}, \textit{sit}, \textit{stand}, \textit{swing baseball}, \textit{throw}, \textit{walk} as well as \textit{wave}.
Some example images of the dataset can be seen in \fref{fig:jhmdb_example_images}.
Notice that the actions are more diverse, in comparison to the Penn Action dataset, specifically because it also contains non-sport activities.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.7\textwidth]{jhmdb_example_images.png}
    \caption{Four example images from the JHMDB dataset \cite{jhuang_towards_2013}. }
    \label{fig:jhmdb_example_images}
\end{figure}

The puppet tool defines $15$ different joints.
They are identical to the ones annotated in the Penn Action dataset, but additionally contain \textit{neck} and \textit{belly}.
The same procedure was used to map the joints to the format provided by the MPII dataset.
Also, the approach for computing ground truth bounding boxes, as well as preprocessing and augmenting the \textit{fragments} is identical to the one presented earlier in \sref{sec:exp-penn}.

\section{Evaluation Metrics}
\label{sec:exp-metrics}
\subsection{PCK}
\label{sec:exp-pck}

The \textit{Probability of Correct Keypoints (PCK)} metric \cite{ferrari_progressive_2008} is often used in the literature to evaluate estimated pose when a human bounding box is given.
See \sref{sec:pck_related_work} for a historical motivation of this metric.
To determine if a predicted keypoint location $k_{est} = (x_{est}, y_{est})$ is estimated close enough to the ground truth keypoint $k_{gt} = (x_{gt}, y_{gt})$, first the absolute distance $d = \lVert k_{est} - k_{gt} \rVert$ is computed.
Afterwards, the maximum $l_{max} = max(bbox_{height}, bbox_{width})$ of the bounding box side lengths is computed and multiplied by a hyperparameter $\alpha$, resulting in $l_{comp} = l_{max} \cdot \alpha$.
Then, a keypoint is determined to be estimated correctly if $d < l_{comp}$.
Typical values for $\alpha$ found in the literature are $0.2$ and $0.1$.
This metric is used primarily for evaluating pose on the JHMDB dataset since it does not provide head bounding box annotations, which are necessary for using the PCKh metric.

\subsection{PCKh}
\label{sec:exp-pckh}
One downside of using the PCK metric based on person bounding box side lengths is that, for highly articulated poses, the bounding box is not always an accurate representation of the body size.
Thus, the threshold $l_{comp}$ highly depends on the pose performed.

In \cite{andriluka_2d_2014}, the authors thus propose to use the head bounding box diameter, assuming that this bounding box does not change for different poses.
See \sref{sec:pose-machine-evaluation} for a detailed historic motivation of the metric.
In most cases, $\alpha$ is set to $0.5$ and $0.2$.

\subsection{Single- and Multi-Clip Accuracy}
To evaluate the accuracy of the HAR pipeline, the authors in \cite{luvizon_2d/3d_2018} use two different approaches.
First, they take the video clip to evaluate and extract a $16$ frame fragment from the middle of the clip.
Then, they estimate the action performed.
Since the output of the network is a Softmax activation, they use the \textit{argmax} function to determine the action with the highest score.
This is referred to further as Single-Clip accuracy.

Additionally, the authors extract multiple $16$ frame fragments from the clip by starting at frame $0$ and then incrementing the starting position by $8$ for as long as there are at least $16$ frames left in the clip.
Then, for each fragment, they predict the action identical to the Single-Clip accuracy.
If the majority of the fragments estimated the same action, this is used to determine the Multi-Clip accuracy. 

\section{Experimental Results}
% TODO: Think about if the network was actually overfitting everytime you write it. Hard to say without train accuracy. On the other hand, who will know...

\label{sec:exp-results}
\subsection{Accuracy of Soft-argmax function}
For evaluating the accuracy of the Soft-argmax function, we performed two experiments.
For both experiments, an estimated coordinate $(x_{est},y_{est})$ was considered to be correct in comparison to the ground truth coordinate $(x_{gt}, y_{gt}$ if both $\lvert x_{est} - x_{gt} \rvert \leq d$ and $\lvert y_{est} - y_{gt} \rvert \leq d$, allowing for a $d$ pixel discrepancy between prediction and ground truth.
The reason for using a threshold of $d$ pixels was that the output of the Soft-argmax function are fractions of width and height with $(x_{frac}, y_{frac}) \in [0,1]$.
To compute the image coordinate, a multiplication with the width and height of the input image as well as a rounding step is necessary, possibly introducing rounding errors.

First, synthetic images of size $255 \times 255$ pixels were created, since this is the size of the input images to the network after preprocessing.
At each $x,y$ position, a two-dimensional gaussian with mean $(x,y)$ and covariance $c$ was placed.
Afterwards, the expectations were computed using the Soft-argmax function and compared to the ground truth mean value.
We chose to set $d=2$.
Performing this for each pixel coordinate and different covariances $c$, it was observed that the Soft-argmax function accurately regressed the true expectation for small covariances.
As the covariance increases, the accuracy decreases, especially around the borders.
See \fref{fig:softargmax_variance_test} for a visualization, where violett pixels indicate a wrong prediction and yellow pixels indicate a correct prediction.
As can be seen, the Soft-argmax function gets less accurate the higher the covariance is, as well as the closer the mean is to the border of the image.
This could lead to wrong estimations for training images where the joint coordinates are close to the border of the image.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.7\textwidth]{softargmax_variance_test.png}
    \caption{Evaluation of the accuracy of the Soft-argmax function using synthetic data. Yellow pixels $i,j$ indicate where the Soft-argmax function correctly regressed the peak of the gaussian with mean value $i,j$, while violett indicates wrong predictions. Notice that the accuracy decreases when approaching the border of the image and when the covariance is increasing. }
    \label{fig:softargmax_variance_test}
\end{figure}

Second, to evaluate the impact of this behaviour on actual data, we analyzed the accuracy quantitatively.
Synthetic joint heatmaps were generated by placing gaussians at the position of the ground truth pose coordinates of a subset of the MPII dataset and the distance between the computed coordinates and ground truth coordinate was computed with different covariance values $c$.
We evaluated different values for $d$ and found that the the Soft-argmax function is generally accurate within a $2$ pixel window for reasonable covariance values.
The experiment was conducted on $1000$ random images from the MPII dataset.
See  for the mean accuracies achieved for covariances $c \in \{1, 2, 5, 10, 20, 50 \}$ and distances $d \in \{1, 2, 3, 4\}$.

As can be seen in \tref{tab:softargmax_numeric_eval}, the Soft-argmax function is accurate in practice for a $2$ pixer threshold.
Notice that the accuracies are significantly lower for a $1$ pixel threshold.
This contrast between $d=1$ and $d=2$ is most likely due to the rounding errors discussed earlier.

\begin{table}[]
    \centering
    \scalebox{0.90}{%
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    threshold & \textbf{$c=1$} & \textbf{$c=2$} & \textbf{$c=5$} & \textbf{$c=10$} & \textbf{$c=20$} & \textbf{$c=50$} \\ \hline
    $1$ & 33.829 & 33.809 & 33.768 & 33.707 & 33.584 & 33.190 \\ \hline 
    $2$ & 99.952 & 99.931 & 99.843 & 99.741 & 99.469 & 98.598 \\ \hline 
    $3$ & 99.959 & 99.945 & 99.891 & 99.809 & 99.639 & 99.054 \\ \hline 
    $4$ & 99.959 & 99.952 & 99.911 & 99.829 & 99.700 & 99.197 \\ \hline 
    \end{tabular}}
    \caption{Mean average accuracy (in percent) of Soft-argmax when detecting ground truth coordinates from synthetic joint heatmaps. Threshold referrs to the amound of pixels the estimate is allowed to deviate from the ground truth annotation. $c$ referres to the covariance used for creating the synthetic heatmaps. The large discrepancy between a threshold of $1$ and a threshold of $2$ is most likely due to rounding errors.}
    \label{tab:softargmax_numeric_eval}
\end{table}

In summary, while the Soft-argmax function appears to be less accurate around the borders of the image, this does not effect the accuracy significantly when estimating actual pose coordinates, according to our experiments.

\subsection{Replication of Original Work}
\label{sec:exp-replication}
% TODO

First, in order to recreate the work of \cite{luvizon_2d/3d_2018}, a pose estimator using the MPII dataset was trained.
In addition to the authors work, the number of prediction blocks as well as the number of context heatmaps was varied to observe the effect on the overall accuracy of the model.
Second, the 2D HAR experiments performed by the authors on the Penn Action datasets were recreated.
If not otherwise mentioned, the experiment settings mentioned are taken from \cite{luvizon_2d/3d_2018} as well as the supplemental material provided by the authors.

\subsubsection{Pose estimation}
In their work, the authors use $8$ prediction blocks in their pose estimator for evaluating the accuracy of the model on the MPII dataset.
In this thesis, additional experiments using $2$ and $4$ prediction blocks were performed.
In addition, we chose to evaluate all models without context heatmaps as well as with $2$ context heatmaps, since this what the authors chose for their experiment.

The training data of the MPII dataset was split into a training and validation set for evaluating the models accuracy on unseen data in intervals of $1000$ iterations.
An iteration is defined as the processing of one batch.
The batch size varies, depending on the experiment, since the models are of different size and thus allow for bigger batch sizes when using smaller models.
The batch size was set to $45, 30$ and $20$ for $2, 4$ and $8$ prediction blocks, respectively.
As an optimizer, the authors use the RMSProp algorithm.
A initial learning rate of $1^{-3}$ was used in accordance to the work of the authors.

\begin{equation}
    \label{eq:elasticnetloss}
    L_p = \frac{1}{N_j} \sum_{i=1}^{N_j} \lVert(p_{est} - p)\rVert_1 + \lVert p_{est} - p \rVert^2_2.
\end{equation}

The authors define a loss function named \textit{Elastic Net Loss}.
For each joint in the pose, the $L_1$ and $L_2$ norms between the estimated and ground truth joint are computed and added.
Then, these sums are again summed and normalized by the number of joints in the pose.
See \eref{eq:elasticnetloss} for the formula, where $p_{est}$ referrs to the estimated joint position by the pose estimator and $p$ referrs to the ground truth pose.
In addition to computing the Elastic net loss, the authors predict the visibility of each joint.
The annotations of MPII provide ground truth labels $v \in \{0,1\}$ of whether a joint is occluded or not.
Binary cross-entropy is used to compute a loss $L_v$ between the predicted visibility vector $v_{est}$ and the ground truth $v$.
Finally, both losses get combined in the following way:

\begin{equation}
    L = L_p + 0.01 \cdot L_v.
\end{equation}

For all subsequent pose estimation experiments, $L$ is used to compute the loss of the pose estimator.

As discussed before, for evaluating a model on the test dataset, the estimations need to be send to the \textit{Max Planck Institute for Intelligent Systems} for evaluation.
Because they limit the amount of submissions to $4$, it was not possible to evaluate each trained model on the test data, which is why it was decided to report the validation accuracy for comparison and only submit estimations from the best performing model.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.7\textwidth]{mpii_vals.png}
    \caption{Validation accuracies of all evaluated pose estimation configurations on the MPII dataset. Notice that the addition of context heatmaps does not significantly improve upon the accuracy for $nr\_blocks > 2$. }
    \label{fig:mpii_vals}
\end{figure}

As can be seen in \fref{fig:mpii_vals} as well as \tref{tab:mpii_results}, the higher the number of prediction blocks, the more accurate the model performs on the validation data.
In addition, using context heatmaps appears to increase the accuracy as well, especially so when using $2$ prediction blocks.
To test whether or not the increase in accuracy is statistically significant, randomization tests with a significance level of $0.05$ were performed.
For the randomization test, $3000$ permutations were used.
As can be seen in \tref{tab:mpii_results}, the only statistically significant increases in accuracy where observed when comparing $2$ and $4$ prediction block models without context heatmaps, as well as between $4$ and $8$ prediction block models when using $2$ context heatmaps.
This suggests that, while keeping the number of prediction blocks constant, the use of context heatmaps does not influence the accuracy of the model in a significant way.
% TODO: Talk more about results after rerunning experiments of figure something out

\begin{table}[]
    \small
    \centering
    \begin{tabular}{|l|l|c|c|}
    \hline
        \textbf{nr\_blocks} & \textbf{nr\_context}  & \textbf{PCKh @ 0.5 (validation)} & \textbf{p-value} \\ \hline
        \begin{tabular}{@{}c@{}} 2 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}}80.96 \\ 81.56  \end{tabular} & 0.54  \\ \hline
    
        \begin{tabular}{@{}c@{}} 4 \\ 4 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}}83.22 \\ 82.97  \end{tabular} & 0.78  \\ \hline

        \begin{tabular}{@{}c@{}} 8 \\ 8 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}}84.67 \\ 84.66  \end{tabular} & 0.97  \\ \hline

        \begin{tabular}{@{}c@{}} 2 \\ 4 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 0 \end{tabular} & \begin{tabular}{@{}c@{}} 80.96 \\ 83.22  \end{tabular} & \textbf{0.02}  \\ \hline

        \begin{tabular}{@{}c@{}} 2 \\ 4 \end{tabular} & \begin{tabular}{@{}c@{}} 2 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}} 81.56 \\ 82.97  \end{tabular} & 0.16  \\ \hline

        \begin{tabular}{@{}c@{}} 4 \\ 8 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 0 \end{tabular} & \begin{tabular}{@{}c@{}} 83.22 \\ 84.67  \end{tabular} & 0.14  \\ \hline

        \begin{tabular}{@{}c@{}} 4 \\ 8 \end{tabular} & \begin{tabular}{@{}c@{}} 2 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}} 82.97 \\ 84.66  \end{tabular} & \textbf{0.081}  \\ \hline

    \end{tabular}
    \caption{Different model configurations and their corresponding PCKh validation score. The $p$-values were computed using a randomization test. $p$-values below the significance level of $0.05$ are shown in bold, indicating that the change in accuracy is significant.} % TODO
    \label{tab:mpii_results}
\end{table}

Next, the best performing model ($8$ prediction blocks, $2$ context heatmaps) was used to estimate the pose on the test dataset and the results were submitted to the \textit{Max Planck Institute for Intelligent Systems}.
The results can be seen in \tref{tab:mpii_test}, in direct comparison to the values reported by the authors.
As can be seen, our model performs significantly worse in comparison to the reported values.
One possible reason is the way the test poses were estimated.
The authors report that they averaged multiple estimations to compute a final estimation.
More specifically, they first computed the pose for the regular test image.
Afterwards, they flipped the image, computed the pose again, and flipped the pose again.
Moreover, they report that they additionally displaced the bounding box used for cropping the image around the subject, after which they estimate the pose again.
Because the authors do not mention how this was done in detail, and since the number of submissions allowed was limited, it was decided to not use displacement and merely averaging the estimated pose with and without mirroring the image.
% TODO: Do small displacement once experiments are done and then say: I dont know exactly how they did it, which might explain why its worse

% TODO: authors report 89 percent accuracy on val and 91.2 on test. Mention that, because it supports the theory that the way the test data was estimated is the reason for discrepancy

\begin{table}[]
    \centering
    \scalebox{0.90}{%
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|}
        \hline
        &Head & Shoulder & Elbow & Wrist & Hip & Knee  & Ankle & Total  \\ \hline
        out recreation& 95.7  & 90.5  & 81.4  & 74.6  & 82.5  & 73.0 & 66.2 & 81.4 \\
        \cite{luvizon_2d/3d_2018}& 98.1  & 96.6  & 92.0  & 87.5  & 90.6  & 88.0 & 82.7 & 91.2  \\ \hline
    \end{tabular}}
    \caption{Our recreation in direct comparison to the original work by \cite{luvizon_2d/3d_2018}. Values represent PCKh using $\alpha = 0.5$.  } %TODO: New values and / or explain differences
    \label{tab:mpii_test}
\end{table}

% TODO: zwischenfazit

\subsubsection{HAR using Penn Action dataset}
In order to recreate the Human Activity Recognition results from \cite{luvizon_2d/3d_2018} on the Penn Action dataset, we first trained a pose estimator, which is later incorporated into the HAR model.
The authors used a hybrid dataset for training, consisting of $75$ percent MPII and $25$ percent Penn Action training data.
They do not, however, motivate this decision.
The pose estimator contains $4$ prediction blocks, uses $2$ context heatmaps and was trained using a batch size of $30$ and a learning rate of $1e-3$.
The loss, as well as validation accuracy scores can be seen in \fref{fig:pose_mixed_results}.
Since the authors did not provide validation accuracy results or test results on this experiment, it was decided to evaluate the trained pose estimator on the test set of the Penn Action dataset, to gain better insight into the performance of the network.
With a PCK accuracy value of $0.84$ and a PCKu value of $0.67$, the network performs significantly worse on the Penn Action test dataset, in comparison to the results of the mixed validation set.
This suggests that mixing the dataset in such a way might not lead to an optimal solution w.r.t. the intended target dataset.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.7\textwidth]{pose_mixed_results.png}
    \caption{Loss and validation accuracies, using PCK and PCKu, both with $\alpha = 0.2$.}
    \label{fig:pose_mixed_results}
\end{figure}

Afterwards, the pretrained pose estimator was inserted into the HAR model.
The model was trained using Penn Action data only.
Also, the weights of the pose estimator were initially frozen.
This was done in order to find the point in training where the validation accuracies plateaus, at which point the authors unfreeze the weights and lower the learning rate.
As an optimizer, Stochastic Gradient Descend was used, with the addition of a momentum value $\gamma = 0.98$ and the use of a method called \textit{Nesterov Accelerated Gradient (NAG)} \cite{nesterov_method_1983}.
Momentum is used to accelarate the gradient descent process by incorporating a fraction $\gamma$ of the previous step size to compute the next step size to take.
Consider a regular update to the weights $w$ of a neuron using gradient descent, as explained in \sref{sec:gradient_descent}, given by the following formula:

\begin{equation}
    w_{t+1} = w_t - \eta \nabla w_t,
\end{equation}

where $\eta$ refers to the learning rate and $\nabla w_t$ contains the gradients computed using the loss function.
When using momentum, a update term $\nu_t$ is computed in the following way:

\begin{equation}
    \nu_t = \gamma \nu_{t-1} + \eta \nabla w_t.
\end{equation}

Afterwards, the new weights are computed by subtracting $\nu_t$ from $w_t$:

\begin{equation}
    w_{t+1} = w_t - \nu_t.
\end{equation}

The intuition behind momentum is that the descend can be accelerated by incorporating previous descend operations.
If the descent in a certain direction was used multiple times before, then the assumption is that the descend in a similar direction further descreases the loss.
This assumption, however, can lead to the case where the minimum of the loss function surpassed.
To minimize the probability of this happening while still accelerating the descend towards the local minimum, Nesterov Accelerated Gradient adds an additional step to the momentum process.
It calculates a \textit{look-ahead} weight $w_{la} = w_{t} \gamma \nu_{t-1}$ and evaluates the gradient using this new weight.
If the gradient at $w_{la}$ points to a different direction than $\gamma \nu_{t-1}$ it will correct the surpassing of the minimum to an extend by reducing the overall magnitude of the step taken.
The following formula describes the update process of weights $w_t$ using the NAG:

\begin{equation}
    w_{t+1} = w_t - \gamma \nu_{t-1} - \eta \nabla w_{la}.
\end{equation}

In the experiment conducted in this thesis, the validation did not increase after the TODO iteration
A learning rate of $2^{-5}$ was used, as suggested by the authors in their work.

- Started finetuning at iteration X and reduce learning rate by X % TODO
- Talk about results
% TODO: single and multi clip evaluation
% TODO: statistical significance testing between my results and authors
% TODO: zwischenfazit

\subsection{Pose estimation on JHMDB dataset}
- same ablation study as before
    - evaluate different number of blocks and context heatmaps
- parameters
    - learning rate 1e-3, as discussed in MPII

Similar to the experiment performed in \sref{sec:exp-replication}, this thesis evaluated different pose estimator configurations using the JHMDB dataset.
Specifically, pose estimators with $2, 4$ and $8$ prediction blocks are trained with and without context heatmaps to assess the performance of the network on a more challenging video dataset.
In terms of architecture, there are no changes to the experiments on the MPII dataset.
The pose estimators are trained using an augmentation ratio of $1:6$, which means that for each original frame there are $6$ randomly augmented frames in the dataset.
The test accuracies, computed on the JHMDB test split, are shown for each configuration in \tref{tab:jhmdb_results}.

\begin{table}[]
    \small
    \centering
    \begin{tabular}{|l|l|c|c|c|}
    \hline
        \textbf{nr\_blocks} & \textbf{nr\_context}  & \textbf{PCK @ 0.2} & \textbf{PCKu @ 0.2} & \textbf{p-value} \\ \hline
        \begin{tabular}{@{}c@{}} 2 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}}80.96 \\ 81.56  \end{tabular} & 0.54  \\ \hline
    
        \begin{tabular}{@{}c@{}} 4 \\ 4 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}}83.22 \\ 82.97  \end{tabular} & 0.78  \\ \hline

        \begin{tabular}{@{}c@{}} 8 \\ 8 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}}84.67 \\ 84.66  \end{tabular} & 0.97  \\ \hline

        \begin{tabular}{@{}c@{}} 2 \\ 4 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 0 \end{tabular} & \begin{tabular}{@{}c@{}} 80.96 \\ 83.22  \end{tabular} & \textbf{0.02}  \\ \hline

        \begin{tabular}{@{}c@{}} 2 \\ 4 \end{tabular} & \begin{tabular}{@{}c@{}} 2 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}} 81.56 \\ 82.97  \end{tabular} & 0.16  \\ \hline

        \begin{tabular}{@{}c@{}} 4 \\ 8 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 0 \end{tabular} & \begin{tabular}{@{}c@{}} 83.22 \\ 84.67  \end{tabular} & 0.14  \\ \hline

        \begin{tabular}{@{}c@{}} 4 \\ 8 \end{tabular} & \begin{tabular}{@{}c@{}} 2 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}} 82.97 \\ 84.66  \end{tabular} & \textbf{0.081}  \\ \hline

    \end{tabular}
    \caption{Different model configurations for estimating pose on the JHMDB dataset, with their corresponding PCK and PCKu test accuracies. The $p$-values were computed using a randomization test. $p$-values below the significance level of $0.05$ are shown in bold, indicating that the change in accuracy is significant.}
    \label{tab:jhmdb_results}
\end{table}


% TODO: talk about a general conclusion like \textit{the higher the number of blocks the better} when comparing MPII and JHMDB results

% TODO: report the number of iterations used
% TODO: general conclusion: is this dataset harder? why?

\subsection{HAR on JHMDB Dataset}
- use the pretrained pose estimator from before, 4 blocks, 2 context heatmaps
- parameters
- Same as in pennaciton: first find the validation plateau point
- afterwards: new experiment with finetune
% TODO: Compare to PennAction
% TODO: We used gt bb because the model would not be able to achieve scores high enough

trained for 12k iterations with lr of 2e-5. Noticed that overfitting. Increased ratio of augmented to non-augmented to 1 to 10 from 1 to 6. Also, decreased learning rate to 1e-6. Trained until validation plateau at iteration 15k.
% TODO: Do graphic with all different experiments combined (validation). Until 12k: ares/no_finetune_with_td_aug6. Refine: kronos/no_finetune_with_td_refined_smaller_lr. Finetuning: kronos/with_finetune_with_td_refine start at 15k, batch size 2 because otherwise model does not fit because more gradients need to be stored in gpu memory.

\subsection{Effect of Combining Loss Functions}
Next, this thesis proposes a method for training the network in an end-to-end approach.
This means that no part of the network is pretrained.
To achieve this, the loss of the pose estimator and the loss of the action recognition pipeline are combined.
More specifically, during the training process, the losses of all intermediate results from the pose estimator are computed using the elastic net loss.
Afterwards, the loss of all intermediate results from the action recognition are computed using categorical cross-entropy.
It was decided to weigh both losses equally, i.e., sum them without weighting either the pose estimation or action recognition loss more.

It was observed that the network started overfitting during training.
This is most likely due to the small size of the dataset, compared to the size of the untrained network.
It was decided to introduce $L_2$ regularization on the weights using a parameter of $\lambda = 0.9$ to make the network less suceptable to overfitting.
% TODO: change this if parameters change

- explain how e2e training was done (using pose loss etc.)
- show results
- explain why it didnt work (or why it did)

\begin{itemize}
    \item Smaller model
    \begin{itemize}
        \item 2 prediction blocks in pose and action
        \item lowerd amount of filters in pose and visual model (create graphic?)
        \item lowerd amount of filters in stem
        \item idea: bigger batch size than 1 
    \end{itemize}
\end{itemize}

% \subsection{Effect of Using Different Representations of Pose}
% \label{sec:different_pose_representation_experiment}

