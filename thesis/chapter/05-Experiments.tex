\chapter{Experiments}
\label{sec:chapter5}
In the previous chapter, the method for Human Activity Recognition by \cite{luvizon_2d/3d_2018} was introduced.
In addition, this thesis proposed experiments to gain a better insight into the authors method.
This chapter first introduces the datasets used in the experiments in Section \ref{sec:exp-datasets}, specifcally the \textit{MPII Human Pose} dataset for $2D$ pose estimation, and the \textit{Penn Action} and \textit{JHMDB} datasets for Human Activity Recognition.
Afterwards, the metrics used in the authors work, as well as the experiments done in this thesis, are discussed in Section \ref{sec:exp-metrics}.
Lastly, this chapter discusses the experiment setups, as well as the results, in Section \ref{sec:exp-results}.
First, some experiments from \cite{luvizon_2d/3d_2018} are recreated, focusing on the $2D$ pose estimation and $2D$ Human Activity Recognition experiments.
Second, further experiementation towards understanding the capabilities of the model are presented, including a qualitative and quantitative evaluation of the Soft-argmax function, the accuracy of the authors model achieved on the complex JHMDB dataset, as well as an approach for training the model in an end-to-end approach, without using a pretrained pose estimator.

\section{Datasets}
\label{sec:exp-datasets}

\subsection{MPII Human Pose}
\label{sec:exp-mpii}

In \cite{andriluka_2d_2014}, the authors present a dataset for 2D human pose estimation on still images.
It contains $25.000$ images, where $40.000$ persons were annotated.
The annotations include $16$ joint annotations in addition to an indicator of whether or not the joint is visible or not.
The $16$ joints are \textit{left / right ankle}, \textit{left / right knee}, \textit{left / right hip}, \textit{left / right elbow}, \textit{left / right shoulder}, \textit{left / right wrist}, \textit{pelvis}, \textit{thorax}, \textit{upper neck} and \textit{top of the head}.
See \fref{fig:mpii_example_images} for example images of the dataset.
In addition, the body center coordinates are given in addition to a scale indicating the size of the person bounding box w.r.t. $200$ pixels.
Also, a bounding box of the head is given, which is used to compute the \textit{PCKh} metric (see \sref{sec:exp-pckh}).
The images are extracted from YouTube videos and do not contain artifacts commonly found in videos like compression or blur.
Additionally, each image is assigned an activity performed in the video it was extracted from, totalling $401$ total activities.
However, these annotations were not used for Human Action Recognition since they are too fine-grained and the number of samples per activity is too low for training a deep neural network.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.7\textwidth]{mpii_example_images.png}
    \caption{Four example images from the MPII dataset \cite{andriluka_2d_2014}. }
    \label{fig:mpii_example_images}
\end{figure}

We follow the approaches from \cite{luvizon_2d/3d_2018} for preprocessing, which the authors present in their supplemental material.
First, a person bounding box is estimated using the center body annotation from the dataset.
The authors multiply the scale given by the annotation $s_{orig}$ by $1.25$, resulting in $s_{new}$.
They do not motivate the reason for using this specific value, but it most likely was used to enlarge the bounding box to contain more context around the person.
Next, they compute the width and height using $s_{new} \cdot 200$, which results in a square bounding box.
In addition, the authors also alter the center position $(c_x,  c_y)$ given by the annotation by computing $(c_{x}^{new}, c_y^{new}) = (c_x, c_y + s_{new} \cdot 12)$.
Again, the authors do not provide a reasoning for moving the center position further towards the neck of the person in this way.
Once the bounding box is computed, the image is cropped to the size of the bounding box around the newly computed center coordinate and rescalled to a size of $256 \times 256$ .
In the case where a joint annotation falls outside of the now cropped image, the authors set the visibility of the joint to $0$ and set the $(x,y)$ coordinates of the joint to $(-1e9, -1e9)$.

Additionally, the authors introduce parameters used for augmentation.
These values are sampled from their respective sets whenever augmentation is performed.
Specifically, they introduce $s_{aug} \in \{0.7, 1, 1.3\}$ which gets multiplied with $s_{new}$ computed earlier.
Also, $r_{aug} \in \{-40, -35, \dots, 35, 40\}$ is introduced to rotate the image $r_{aug}$ degrees around its center, possibly introducing black borders around the image.
Moreover, when augmenting, the image is horizontally flipped with a chance of $50$ percent.
See \fref{fig:mpii_example_augmentation} for multiple examples of different augmented images, including augmented pose.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.99\textwidth]{mpii_dataset_augmented_examples.png}
    \caption{\textbf{From left to right}: \textbf{1.} Original image from the MPII dataset. \textbf{2.} Original image with the ground truth pose superimposed. \textbf{3.} Image after estimating the bounding box, cropping and rescaling. \textbf{4.} Augmented image using $s_{aug} = 1.3$, $r_{aug} = -15$ degrees and flipping the image vertically.}
    \label{fig:mpii_example_augmentation}
\end{figure}

When using horizontal flipping, a problem occured where the learned pose resembled a generic skeleton.
See \fref{fig:ghost_skeleton} for an example image.
Because of the horizontal flipping of the image, the pose needed to be flipped as well.
Furthermore, however, the labels of some joints needed to be changed as well.
As an example, the right arm (from the perspective of the subject in the image) becomes the left arm when the image is flipped.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.80\textwidth]{ghost_skeleton.png}
    \caption{Example of a phenomenon which occured when the image and pose were flipped durring augmentation, but the annotations were not properly changed. As an example, the left arm becomes the right arm after flipping. The ground truth pose is shown in red, while the learned pose is shown in blue. }
    \label{fig:ghost_skeleton}
\end{figure}

The dataset does not contain annotations for the test data, other than the scale and center coordinates.
To evaluate the test images, the joints need to be evaluated and the results need to be send to the authors of the dataset at the \textit{Max Planck Institute for Intelligent Systems} for comparison to the ground truth pose.
This ensures that the test data annotations are not mistakenly or maliciously used in training.
For all datasets used in the experiments, $10$ percent of the training datapoints are withheld from training and used as validation data.

\subsection{Penn Action}
\label{sec:exp-penn}

Another dataset used in \cite{luvizon_2d/3d_2018} is the Penn Action dataset \cite{zhang_actemes_2013}.
It contains $2326$ video clips of $15$ different actions performed.
The actions performed are mostly sports related and include \textit{baseball swing}, \textit{clean and jerk}, \textit{jumping jacks}, \textit{pushup}, \textit{strum guitar}, \textit{bench press}, \textit{golf swing}, \textit{baseball pitch}, \textit{situp}, \textit{tennis forehand}, \textit{bowling}, \textit{jump rope}, \textit{pullup}, \textit{squat} and \textit{tennis serve}.
In addition, the authors provide annotations for $13$ body joints,
including \textit{left and right shoulders, elbows, wrists, hips and knees} as well as one annotation of the \textit{head} of the person.
Some example images can be seen in \fref{fig:pennaction_example_images}. 

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.7\textwidth]{pennaction_example_images.png}
    \caption{Four example images from the Penn Action dataset \cite{zhang_actemes_2013}. }
    \label{fig:pennaction_example_images}
\end{figure}

It was decided by \cite{luvizon_2d/3d_2018} that the number of joints should be identical to the ones presented in \cite{andriluka_2d_2014} \sref{sec:exp-mpii} because the network assumes a fixed number of joints.
This then means that the network architecture does not need to be changed, no matter which dataset is used.
Specifically, the \textit{pose cube} dimensionality does not need to be changed.
To achieve this, the \textit{head} annotation of the Penn Action dataset is mapped to the \textit{upper neck} joint of the MPII dataset and the missing joints were interpreted as not visible.

Additional augmentation is introduced since the dataset is smaller in comparison to MPII and because it was observed that the HAR models were overfitting on the training data.
Thus, two additional augmentation methods are used.
First, with a probability of $0.5$, salt and pepper augmentation is applyied to the training images.
In this augmentation method, random noise in the form of white and black pixels gets applied to the image.
Second, using an augmentation strategy called Dropout, black rectangles are placed randomly inside the image, covering chunks of the original image.
The network then needs to make decisions on partially occluded images, making it more robust, because it forces the network to utilize alternative paths on some images, effectively making the model more general.
Dropout is applied with a probability of $0.5$ as well.

For some experiments, we compute a ground truth bounding box of the person by calculating the minimum and maximum $x$ and $y$ coordinates of the pose and defining these as the corners of the bounding box.
Additionally, to include more context, the bounding box is increased by $30$ pixels in both dimensions to add more context around the subject.

Additionally, the authors in \cite{luvizon_2d/3d_2018} decide to process the video clips in chunks of $16$ frames.
This is again important since the network architecture can be kept the same because \textit{pose cube} dimensions do not change that way.
Thus, it was decided to precompute $16$ frames subclips, which this thesis refers to as \textit{fragments}, and save them separately to increase the training speed, since the preprocessing and extraction of the subclips does not need to be done at training time. 

\subsection{JHMDB}
\label{sec:exp-jhmdb}

Similar to \cite{zhang_actemes_2013} \sref{sec:exp-penn}, the JHMDB dataset \cite{jhuang_towards_2013} contains annotations for pose and action in video clips.
The dataset was created by taking a subset of the HMDB action recogntion dataset \cite{kuehne_hmdb:_2011}, which was then annotated using the \textit{puppet tool} \cite{zuffi_pictorial_2012}.
This tool allows to not only annotate the pose of the person but also automatically computes a binary segmentation map of the person, further referred to as the \textit{puppet mask}.
See \fref{fig:puppet_tool_example} for a visualization of the annotation process and the puppet tool.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.4\textwidth]{puppet_tool_example.png}
    \caption{Example of an image, annotated using the puppet flow. The dots indicate the joint positions, while the transparent human figure prior automatically adjusts, yielding the human segmentation map. Notice that, by using the figure, even joints that are not visible (like the ankles) still get anatomically plausible annotations. Image taken from \cite{max_planck_institute_for_intelligent_systems_jhmdb_nodate}.}
    \label{fig:puppet_tool_example}
\end{figure}

The clips in the HMDB dataset are taken from YouTube.
The annotated subset contains the following actions:
\textit{brush hair}, \textit{catch}, \textit{clap}, \textit{climb stair}, \textit{golf}, \textit{jump}, \textit{kick ball}, \textit{pick}, \textit{pour}, \textit{pullup}, \textit{push}, \textit{run}, \textit{shoot ball}, \textit{shoot bow}, \textit{shoot gun}, \textit{sit}, \textit{stand}, \textit{swing baseball}, \textit{throw}, \textit{walk} as well as \textit{wave}.
Some example images of the dataset can be seen in \fref{fig:jhmdb_example_images}.
Notice that the actions are more diverse, in comparison to the Penn Action dataset, specifically because it also contains non-sport activities.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.7\textwidth]{jhmdb_example_images.png}
    \caption{Four example images from the JHMDB dataset \cite{jhuang_towards_2013}. }
    \label{fig:jhmdb_example_images}
\end{figure}

The puppet tool defines $15$ different joints.
They are identical to the ones annotated in the Penn Action dataset, but additionally contain \textit{neck} and \textit{belly}.
The same procedure is used to map the joints to the format provided by the MPII dataset.
Also, the approach for computing ground truth bounding boxes, as well as preprocessing and augmenting the \textit{fragments} is identical to the one presented earlier in \sref{sec:exp-penn}.

\section{Evaluation Metrics}
\label{sec:exp-metrics}
\subsection{PCK}
\label{sec:exp-pck}

The \textit{Probability of Correct Keypoints (PCK)} metric \cite{ferrari_progressive_2008} is often used in the literature to evaluate estimated pose when a human bounding box is given.
See \sref{sec:pck_related_work} for a historical motivation of this metric.
To determine if a predicted keypoint location $k_{est} = (x_{est}, y_{est})$ is estimated close enough to the ground truth keypoint $k_{gt} = (x_{gt}, y_{gt})$, first the absolute distance $d = \lVert k_{est} - k_{gt} \rVert$ is computed.
Afterwards, the maximum $l_{max} = max(bbox_{height}, bbox_{width})$ of the bounding box side lengths is computed and multiplied by a hyperparameter $\alpha$, resulting in $l_{comp} = l_{max} \cdot \alpha$.
Then, a keypoint is determined to be estimated correctly if $d < l_{comp}$.
Typical values for $\alpha$ found in the literature are $0.2$ and $0.1$.
This metric is used primarily for evaluating pose on the JHMDB dataset since it does not provide head bounding box annotations, which are necessary for using the PCKh metric.

\subsection{PCKh}
\label{sec:exp-pckh}
One downside of using the PCK metric based on person bounding box side lengths is that, for highly articulated poses, the bounding box is not always an accurate representation of the body size.
Thus, the threshold $l_{comp}$ highly depends on the pose performed.

In \cite{andriluka_2d_2014}, the authors thus propose to use the head bounding box diameter, assuming that this bounding box does not change for different poses.
See \sref{sec:pose-machine-evaluation} for a detailed historic motivation of the metric.
In most cases, $\alpha$ is set to $0.5$ and $0.2$.

\subsection{Single- and Multi-Clip Accuracy}
To evaluate the accuracy of the HAR pipeline, the authors in \cite{luvizon_2d/3d_2018} use two different approaches.
First, they take the video clip to evaluate and extract a $16$ frame fragment from the middle of the clip.
Second, they estimate the action performed.
Since the output of the network is a Softmax activation, they use the \textit{argmax} function to determine the action with the highest score.
This is referred to further as Single-Clip accuracy.

Additionally, the authors extract multiple $16$ frame fragments from the clip by starting at frame $0$ and then incrementing the starting position by $8$ for as long as there are at least $16$ frames left in the clip.
Then, for each fragment, they predict the action identical to the Single-Clip accuracy.
If the majority of the fragments estimated the same action, this is used to determine the Multi-Clip accuracy. 

\section{Experimental Results}
% TODO: Think about if the network was actually overfitting everytime you write it. Hard to say without train accuracy. On the other hand, who will know...

\label{sec:exp-results}
\subsection{Accuracy of Soft-argmax function}
For evaluating the accuracy of the Soft-argmax function, two experiments are performed.
For both experiments, an estimated coordinate $(x_{est},y_{est})$ is considered to be correct in comparison to the ground truth coordinate $(x_{gt}, y_{gt}$ if both $\lvert x_{est} - x_{gt} \rvert \leq d$ and $\lvert y_{est} - y_{gt} \rvert \leq d$, allowing for a $d$ pixel discrepancy between prediction and ground truth.
The reason for using a threshold of $d$ pixels is that the output of the Soft-argmax function are fractions of width and height with $(x_{frac}, y_{frac}) \in [0,1]$.
To compute the image coordinate, a multiplication with the width and height of the input image as well as a rounding step is necessary, possibly introducing rounding errors.
Thus, $d$ is set to $d=2$.

First, synthetic images of size $255 \times 255$ pixels are created, since this is the size of the input images to the network after preprocessing.
At each $x,y$ position, a two-dimensional gaussian with mean $(x,y)$ and covariance $c$ is placed.
Afterwards, the expectations are computed using the Soft-argmax function and compared to the ground truth mean value.
Performing this for each pixel coordinate and different covariances $c$, it was observed that the Soft-argmax function accurately regresses the true expectation for small covariances.
As the covariance increases, the accuracy decreases, especially around the borders.
See \fref{fig:softargmax_variance_test} for a visualization, where violett pixels indicate a wrong prediction and yellow pixels indicate a correct prediction.
As can be seen, the Soft-argmax function gets less accurate the higher the covariance is, as well as the closer the mean is to the border of the image.
This could lead to wrong estimations for training images where the joint coordinates are close to the border of the image.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.7\textwidth]{softargmax_variance_test.png}
    \caption{Evaluation of the accuracy of the Soft-argmax function using synthetic data. Yellow pixels $i,j$ indicate where the Soft-argmax function correctly regressed the peak of the gaussian with mean value $i,j$, while violett indicates wrong predictions. Notice that the accuracy decreases when approaching the border of the image and when the covariance is increasing. }
    \label{fig:softargmax_variance_test}
\end{figure}

Second, to evaluate the impact of this behaviour on actual data, we analyze the accuracy quantitatively.
Synthetic joint heatmaps are generated by placing gaussians at the position of the ground truth pose coordinates of a subset of the MPII dataset and the distance between the computed coordinates and ground truth coordinate is computed with different covariance values $c$.
We evaluate different values for $d$ and find that the the Soft-argmax function is generally accurate within a $2$ pixel window for reasonable covariance values.
The experiment is conducted on $1000$ random images from the MPII dataset.
See  for the mean accuracies achieved for covariances $c \in \{1, 2, 5, 10, 20, 50 \}$ and distances $d \in \{1, 2, 3, 4\}$.

As can be seen in \tref{tab:softargmax_numeric_eval}, the Soft-argmax function is accurate in practice for a $2$ pixel threshold.
Notice that the accuracies are significantly lower for a $1$ pixel threshold.
This contrast between $d=1$ and $d=2$ is most likely due to the rounding errors discussed earlier.

\begin{table}[]
    \centering
    \scalebox{0.90}{%
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \hline
    threshold & \textbf{$c=1$} & \textbf{$c=2$} & \textbf{$c=5$} & \textbf{$c=10$} & \textbf{$c=20$} & \textbf{$c=50$} \\ \hline
    $1$ & 33.829 & 33.809 & 33.768 & 33.707 & 33.584 & 33.190 \\ \hline 
    $2$ & 99.952 & 99.931 & 99.843 & 99.741 & 99.469 & 98.598 \\ \hline 
    $3$ & 99.959 & 99.945 & 99.891 & 99.809 & 99.639 & 99.054 \\ \hline 
    $4$ & 99.959 & 99.952 & 99.911 & 99.829 & 99.700 & 99.197 \\ \hline 
    \end{tabular}}
    \caption{Mean average accuracy (in percent) of Soft-argmax when detecting ground truth coordinates from synthetic joint heatmaps. Threshold referrs to the amound of pixels the estimate is allowed to deviate from the ground truth annotation. $c$ referres to the covariance used for creating the synthetic heatmaps. The large discrepancy between a threshold of $1$ and a threshold of $2$ is most likely due to rounding errors.}
    \label{tab:softargmax_numeric_eval}
\end{table}

In summary, while the Soft-argmax function appears to be less accurate around the borders of the image, this does not effect the accuracy significantly when estimating actual pose coordinates, according to the quantitative experiment.

\subsection{Replication of Original Work}
\label{sec:exp-replication}
% TODO

First, in order to recreate the work of \cite{luvizon_2d/3d_2018}, a pose estimator using the MPII dataset is trained.
In addition to the authors work, the number of prediction blocks as well as the number of context heatmaps is varied to observe the effect on the overall accuracy of the model.
Second, the 2D HAR experiments performed by the authors on the Penn Action datasets are recreated.
If not otherwise mentioned, the experiment settings mentioned are taken from \cite{luvizon_2d/3d_2018} as well as the supplemental material provided by the authors.
From now on, for each experiment, an augmentation ratio is given.
The ratio of $1:a$ expresses that for each training image there are $a$ randomly augmented images in the dataset.

\subsubsection{Pose estimation}
In their work, the authors use $8$ prediction blocks in their pose estimator for evaluating the accuracy of the model on the MPII dataset.
In this thesis, additional experiments using $2$ and $4$ prediction blocks are performed.
In addition, we chose to evaluate all models without context heatmaps as well as with $2$ context heatmaps, since this what the authors chose for their experiment.

The training data of the MPII dataset is split into a training and validation set for evaluating the models accuracy on unseen data in intervals of $1000$ iterations.
An iteration is defined as the processing of one batch.
The batch size varies, depending on the experiment, since the models are of different size and thus allow for bigger batch sizes when using smaller models.
The batch size is set to $45, 30$ and $20$ for $2, 4$ and $8$ prediction blocks, respectively.
As an optimizer, the authors use the RMSProp algorithm.
A initial learning rate of $10^{-3}$ is used in accordance to the work of the authors.
The augmentation ratio is set to $1:3$, because more augmentation did not lead to higher accuracy in the performed experiments.

\begin{equation}
    \label{eq:elasticnetloss}
    L_p = \frac{1}{N_j} \sum_{i=1}^{N_j} \lVert(p_{est} - p)\rVert_1 + \lVert p_{est} - p \rVert^2_2.
\end{equation}

The authors define a loss function named \textit{Elastic Net Loss}.
For each joint in the pose, the $L_1$ and $L_2$ norms between the estimated and ground truth joint are computed and added.
Then, these sums are again summed and normalized by the number of joints in the pose.
See \eref{eq:elasticnetloss} for the formula, where $p_{est}$ referrs to the estimated joint position by the pose estimator and $p$ referrs to the ground truth pose.
In addition to computing the Elastic net loss, the authors predict the visibility of each joint.
The annotations of MPII provide ground truth labels $v \in \{0,1\}$ of whether a joint is occluded or not.
Binary cross-entropy is used to compute a loss $L_v$ between the predicted visibility vector $v_{est}$ and the ground truth $v$.
Finally, both losses are combined in the following way:

\begin{equation}
    L = L_p + 0.01 \cdot L_v.
\end{equation}

For all subsequent pose estimation experiments, $L$ is used to compute the loss of the pose estimator.

As discussed before, for evaluating a model on the test dataset, the estimations need to be send to the \textit{Max Planck Institute for Intelligent Systems} for evaluation.
Because they limit the amount of submissions to $4$, it is not possible to evaluate each trained model on the test data, which is why it was decided to report the validation accuracy for comparison and only submit estimations from the best performing model.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.7\textwidth]{mpii_vals.png}
    \caption{Validation accuracies of all evaluated pose estimation configurations on the MPII dataset. Notice that the addition of context heatmaps does not significantly improve upon the accuracy for $nr\_blocks > 2$. }
    \label{fig:mpii_vals}
\end{figure}

As can be seen in \fref{fig:mpii_vals} as well as \tref{tab:mpii_results}, the higher the number of prediction blocks, the more accurate the model performs on the validation data.
In addition, using context heatmaps appears to increase the accuracy as well, especially so when using $2$ prediction blocks.
To test whether or not the increase in accuracy is statistically significant, randomization tests with a significance level of $0.05$ are performed.
For the randomization test, $3000$ permutations are used.
As can be seen in \tref{tab:mpii_results}, the only statistically significant increases in accuracy are observed when comparing $2$ and $4$ prediction block models without context heatmaps, as well as between $4$ and $8$ prediction block models when using $2$ context heatmaps.
This suggests that, while keeping the number of prediction blocks constant, the use of context heatmaps does not influence the accuracy of the model in a significant way.
% TODO: Talk more about results after rerunning experiments of figure something out

\begin{table}[]
    \small
    \centering
    \begin{tabular}{|l|l|c|c|}
    \hline
        \textbf{nr\_blocks} & \textbf{nr\_context}  & \textbf{PCKh @ 0.5 (validation)} & \textbf{p-value} \\ \hline
        \begin{tabular}{@{}c@{}} 2 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}}80.96 \\ 81.56  \end{tabular} & 0.54  \\ \hline
    
        \begin{tabular}{@{}c@{}} 4 \\ 4 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}}83.22 \\ 82.97  \end{tabular} & 0.78  \\ \hline

        \begin{tabular}{@{}c@{}} 8 \\ 8 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}}84.67 \\ 84.66  \end{tabular} & 0.97  \\ \hline

        \begin{tabular}{@{}c@{}} 2 \\ 4 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 0 \end{tabular} & \begin{tabular}{@{}c@{}} 80.96 \\ 83.22  \end{tabular} & \textbf{0.02}  \\ \hline

        \begin{tabular}{@{}c@{}} 2 \\ 4 \end{tabular} & \begin{tabular}{@{}c@{}} 2 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}} 81.56 \\ 82.97  \end{tabular} & 0.16  \\ \hline

        \begin{tabular}{@{}c@{}} 4 \\ 8 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 0 \end{tabular} & \begin{tabular}{@{}c@{}} 83.22 \\ 84.67  \end{tabular} & 0.14  \\ \hline

        \begin{tabular}{@{}c@{}} 4 \\ 8 \end{tabular} & \begin{tabular}{@{}c@{}} 2 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}} 82.97 \\ 84.66  \end{tabular} & \textbf{0.081}  \\ \hline

    \end{tabular}
    \caption{Different model configurations and their corresponding PCKh validation score. The $p$-values were computed using a randomization test. $p$-values below the significance level of $0.05$ are shown in bold, indicating that the change in accuracy is significant.} % TODO
    \label{tab:mpii_results}
\end{table}

Next, the best performing model ($8$ prediction blocks, $2$ context heatmaps) is used to estimate the pose on the test dataset and the results were submitted to the \textit{Max Planck Institute for Intelligent Systems}.
The results can be seen in \tref{tab:mpii_test}, in direct comparison to the values reported by the authors.
As can be seen, our model performs significantly worse in comparison to the reported values.
One possible reason is the way the test poses are estimated.
The authors report that they averaged multiple estimations to compute a final estimation.
More specifically, they first computed the pose for the regular test image.
Afterwards, they flipped the image, computed the pose again, and flipped the pose again.
Moreover, they report that they additionally displaced the bounding box used for cropping the image around the subject, after which they estimate the pose again.
Because the authors do not mention how this was done in detail, and since the number of submissions allowed was limited, it was decided to not use displacement and merely averaging the estimated pose with and without mirroring the image.
% TODO: Do small displacement once experiments are done and then say: I dont know exactly how they did it, which might explain why its worse

% TODO: Show some images

% TODO: authors report 89 percent accuracy on val and 91.2 on test. Mention that, because it supports the theory that the way the test data was estimated is the reason for discrepancy

\begin{table}[]
    \centering
    \scalebox{0.90}{%
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|}
        \hline
        &Head & Shoulder & Elbow & Wrist & Hip & Knee  & Ankle & Total  \\ \hline
        out recreation& 95.7  & 90.5  & 81.4  & 74.6  & 82.5  & 73.0 & 66.2 & 81.4 \\
        \cite{luvizon_2d/3d_2018}& 98.1  & 96.6  & 92.0  & 87.5  & 90.6  & 88.0 & 82.7 & 91.2  \\ \hline
    \end{tabular}}
    \caption{Our recreation in direct comparison to the original work by \cite{luvizon_2d/3d_2018}. Values represent PCKh using $\alpha = 0.5$.  } %TODO: New values and / or explain differences
    \label{tab:mpii_test}
\end{table}

% TODO: zwischenfazit

\subsubsection{HAR using Penn Action dataset}
In order to recreate the Human Activity Recognition results from \cite{luvizon_2d/3d_2018} on the Penn Action dataset, we first train a pose estimator, which is later incorporated into the HAR model.
The authors used a hybrid dataset for training, consisting of $75$ percent MPII and $25$ percent Penn Action training data.
They do not, however, motivate this decision.
The pose estimator contains $4$ prediction blocks, uses $2$ context heatmaps and was trained using a batch size of $30$ and a learning rate of $10^{-3}$.
The loss, as well as validation accuracy scores achieved on the Penn Action dataset can be seen in \fref{fig:pose_mixed_results}.
% TODO: test on test dataset aswell
% TODO: Discuss results 

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.7\textwidth]{pose_mixed_results.png}
    \caption{Loss and validation accuracies after training the pose estimator using a mixture dataset of MPII and Penn Action.}
    \label{fig:pose_mixed_results}
\end{figure}

Afterwards, the pretrained pose estimator is inserted into the HAR model.
The model is trained using Penn Action data only.
Also, the weights of the pose estimator are initially frozen.
This is done in order to find the point in training where the validation accuracies plateaus, at which point the authors unfreeze the weights and lower the learning rate.
As an optimizer, Stochastic Gradient Descend is used, with the addition of a momentum value $\gamma = 0.98$ and the use of a method called \textit{Nesterov Accelerated Gradient (NAG)} \cite{nesterov_method_1983}.
Momentum is used to accelarate the gradient descent process by incorporating a fraction $\gamma$ of the previous step size to compute the next step size to take.
Consider a regular update to the weights $w$ of a neuron using gradient descent, as explained in \sref{sec:gradient_descent}, given by the following formula:

\begin{equation}
    w_{t+1} = w_t - \eta \nabla w_t,
\end{equation}

where $\eta$ refers to the learning rate and $\nabla w_t$ contains the gradients computed using the loss function.
When using momentum, a update term $\nu_t$ is computed in the following way:

\begin{equation}
    \nu_t = \gamma \nu_{t-1} + \eta \nabla w_t.
\end{equation}

Afterwards, the new weights are computed by subtracting $\nu_t$ from $w_t$:

\begin{equation}
    w_{t+1} = w_t - \nu_t.
\end{equation}

The intuition behind momentum is that the descend can be accelerated by incorporating previous descend operations.
If the descent in a certain direction was used multiple times before, then the assumption is that the descend in a similar direction further descreases the loss.
This assumption, however, can lead to the case where the minimum of the loss function surpassed.
To minimize the probability of this happening while still accelerating the descend towards the local minimum, Nesterov Accelerated Gradient adds an additional step to the momentum process.
It calculates a \textit{look-ahead} weight $w_{la} = w_{t} \gamma \nu_{t-1}$ and evaluates the gradient using this new weight.
If the gradient at $w_{la}$ points to a different direction than $\gamma \nu_{t-1}$ it will correct the surpassing of the minimum to an extend by reducing the overall magnitude of the step taken.
The following formula describes the update process of weights $w_t$ using the NAG:

\begin{equation}
    w_{t+1} = w_t - \gamma \nu_{t-1} - \eta \nabla w_{la}.
\end{equation}

In the experiment conducted in this thesis, the validation does not increase after the TODO iteration
A learning rate of $2^{-5}$ is used, as suggested by the authors in their work.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.8\textwidth]{har_pennaction_results.png}
    \caption{TODO}
    \label{fig:har_pennaction_results}
\end{figure}

- Started finetuning at iteration X and reduce learning rate by X % TODO
- Talk about results
% TODO: on test: confusion matrix for action and per-joint accuracy before and after finetuning. discuss changes
% TODO: show good / bad example images
% TODO: single and multi clip evaluation
% TODO: statistical significance testing between my results and authors
% TODO: zwischenfazit

\subsection{Pose estimation on JHMDB dataset}
\label{sec:exp-pose-jhmdb}
Similar to the experiment performed in \sref{sec:exp-replication}, this thesis evaluates different pose estimator configurations using the JHMDB dataset.
Specifically, pose estimators with $2, 4$ and $8$ prediction blocks are trained with and without context heatmaps to assess the performance of the network on a more challenging video dataset.
In terms of architecture, there are no changes to the experiments on the MPII dataset.
The pose estimators are trained using an augmentation ratio of $1:6$, which means that for each original frame there are $6$ randomly augmented frames in the dataset.
The test accuracies, computed on the JHMDB test split, are shown for each configuration in \tref{tab:jhmdb_results}.

% TODO: Explain why 8 prediction block results are so bad

\begin{table}[]
    \small
    \centering
    \begin{tabular}{|l|c|c|c|c|c|c|}
    \hline
        \textbf{nr\_blocks} & 2 & 2 & 4 & 4 & 8 & 8 \\ \hline
        \textbf{nr\_context} & 0 & 2 & 0 & 2 & 0 & 2 \\ \hline
        \textbf{PCK @ 0.2} & \textbf{96.23} & 94.99 & 96.21 & 95.55 & 76.01 & 75.80 \\ \hline
        \textbf{PCK @ 0.1} & 90.84 & 89.42 & \textbf{90.98} & 90.33 & 53.30 & 55.46 \\ \hline
        \textbf{PCKu @ 0.2} & 87.34 & 86.57 & \textbf{87.60} & 87.14 & 46.01 & 47.86 \\ \hline
    \end{tabular}
    \caption{Test accuracies of the different pose estimator configurations, computed on the JHMDB test set. PCK is computed for two threshold values, $\alpha = 0.2$ as well as $\alpha = 0.1$. Additionally, PCKu is computed using $\alpha = 0.2$. Maximum values per metric are shown in bold.}
    \label{tab:jhmdb_results}
\end{table}


\begin{table}[]
    \small
    \centering
    \begin{tabular}{|l|l|c|c|}
    \hline
        \textbf{nr\_blocks} & \textbf{nr\_context}  & \textbf{PCK @ 0.1} & \textbf{p-value} \\ \hline
        \begin{tabular}{@{}c@{}} 2 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}} 90.84 \\ 89.42  \end{tabular} & \textbf{0.0016}  \\ \hline
    
        \begin{tabular}{@{}c@{}} 4 \\ 4 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}} 90.98 \\ 90.33  \end{tabular} & 0.127  \\ \hline

        \begin{tabular}{@{}c@{}} 2 \\ 4 \end{tabular} & \begin{tabular}{@{}c@{}} 0 \\ 0 \end{tabular} & \begin{tabular}{@{}c@{}} 90.84 \\ 90.98  \end{tabular} & 0.748  \\ \hline

        \begin{tabular}{@{}c@{}} 2 \\ 4 \end{tabular} & \begin{tabular}{@{}c@{}} 2 \\ 2 \end{tabular} & \begin{tabular}{@{}c@{}} 89.41 \\ 90.33  \end{tabular} & \textbf{0.037}  \\ \hline

    \end{tabular}
    \caption{Different model configurations for estimating pose on the JHMDB dataset, with their corresponding PCK accuracy values, estimated on the test set using $\alpha = 0.1$. The configurations using $8$ prediction blocks are ommited because the results are significantly worse in comparion to the other configurations. The $p$-values were computed using a randomization test. $p$-values below the significance level of $0.05$ are shown in bold, indicating that the change in accuracy is significant.}
    \label{tab:jhmdb_results_confidence}
\end{table}

% TODO: Per-joint accuracy
% TODO: talk about a general conclusion like \textit{the higher the number of blocks the better} when comparing MPII and JHMDB results

% TODO: report the number of iterations used
% TODO: general conclusion: is this dataset harder? why?

\subsection{HAR on JHMDB Dataset}
To assess the performance of the action recognition model on the JHMDB dataset, the pretrained pose estimator with $4$ blocks and $2$ context maps from TODO is used, in order to compare the results to the performance on the Penn Action dataset experiment TODO.
The batch size is set to $12$ since the GPU used for experimentation did not allow for a bigger batch size.
Keep in mind that, since JHMDB is a video dataset, each item in the batch contains $16$ frames (see \sref{sec:exp-jhmdb}).
Effectively, this leads to a batch size of $16 \cdot batch\_size$ for the pose estimator since the pose for each frame is computed independently, before aggregating the estimated poses into the pose cube (see \sref{sec:pose_based_action_recognition}).
Similar to before, the point where the validation data accuracy does not increase further is found by training the model with a pose estimator whos weights are initially fixed.
As an initial learning rate, $2^{-5}$ is chosen, since this is the value used in the Penn Action HAR experiment.
Additionally, an augmentation ratio $1:6$ is used.

% TODO: reference the following image

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.8\textwidth]{har_jhmdb_combined.png}
    \caption{HAR JHMDB. TODO}
    \label{fig:har_jhmdb_combined}
\end{figure}

%bal

After approximately $15.000$ iterations, the validation accuracy plateaus.
At iteration $12.000$, the augmentation ratio is increased to $1:10$, and the learning rate is reduced to $10^{-6}$ since the model started overfitting.
Afterwards, the weights of the pose estimator are unfrozen and the finetuning process is started.
A smaller batch size of $2$ is used since the additional storage of computed gradients of the model inside the GPU virtual memory does not allow for a higher batch size.

% TODO: Validation without finetune: 82.81, 89.11 (single / multi)
% TODO: show validation and test accuracies
% TODO: on test: confusion matrix for action and per-joint accuracy before and after finetuning. discuss changes
% TODO: show good / bad example images
% TODO: Compare to PennAction
% TODO: We used gt bb because the model would not be able to achieve scores high enough

% TODO: Do graphic with all different experiments combined (validation). Until 12k: ares/no_finetune_with_td_aug6. Refine: kronos/no_finetune_with_td_refined_smaller_lr. Finetuning: kronos/with_finetune_with_td_refine start at 15k, batch size 2 because otherwise model does not fit because more gradients need to be stored in gpu memory.

\subsection{Effect of Combining Loss Functions}
Next, this thesis proposes a method for training the network in an end-to-end approach.
This means that no part of the network is pretrained.
To achieve this, the loss of the pose estimator and the loss of the action recognition pipeline are combined.
First, during the training process, the losses of all intermediate results from the pose estimator are computed using the elastic net loss.
Second, the loss of all intermediate results from the action recognition are computed using categorical cross-entropy.
It was decided to weigh both losses equally, i.e., sum them without weighting either the pose estimation or action recognition loss more in order to not make any assumptions about which part is more important.

The network needs a high amount of iterations for training until the validation accuracy converges.
This is to be expected, since no part of the network is pretrained and thus all parts have to be trained from random initialization.
See \fref{fig:e2e_big} for a graphical representation of the training process.
Training accuracy of the action recognition part of the network quickly increases and is above the training accuracy of the pose estimator in the beginning.  
This is to be expected since, at the beginning, the pose information is not reliable.
Thus, the network focuses on utilizing visual features for classification.
After approximately $25.000$ iterations, the pose training accuracy surpasses the action train accuracy and continues to converge towards $1$.

HAR validation is "jittery". this indicates uncertainty on unseen data. 


\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.8\textwidth]{e2e_big.png}
    \caption{Loss and validation accuracies of the pipeline trained using an end-to-end approach, without pretraining individual parts of the model. Validation accuracy given as percentage of correctly classified validation datapoints. TODO}
    \label{fig:e2e_big}
\end{figure}

\def \e2eFinalIteration {$228.000$ } % TODO

\begin{table}[]
    \small
    \centering
    \begin{tabular}{|c|c|c|c|c|}
    \hline
        \textbf{Single clip accuracy} & \textbf{Multi clip accuracy}  & \textbf{PCK @ 0.2} & \textbf{PCK @ 0.1} & \textbf{PCKu @ 0.2}\\ \hline
        0.81 & 0.84 & 0.83 & 0.51 & 0.36 \\ \hline
    \end{tabular}
    \caption{Test results on the JHMDB test set after \e2eFinalIteration iterations. TODO}
    \label{tab:e2e-quantitative-results}
\end{table}

% TODO: Compare action to pretrained
\tref{tab:e2e-quantitative-results}.
When comparing the pose accuracy of the end-to-end model to the pose estimators trained in \sref{sec:exp-pose-jhmdb}, it is apparent that the end-to-end model is significantly worse in all three metrics.
Further experimentation is necessary to assess whether or not the model hyperparameters like learning rate and batch size can be optimized in order for the end-to-end model to achieve comparable results to the pretrained model.
However, the findings are promising, because the network is , in general, able to achieve a high classification accuracy by being trained in an end-to-end approach.
This suggests that, while the pose estimator might be weaker in comparison to a standalone pose estimator, it is enough to make accurate action classifications.

\begin{figure}[htb!]
    \centering
    \includegraphics[width=0.999\textwidth]{cm-e2e.png}
    \caption{TODO}
    \label{fig:cm-e2e}
\end{figure}

A confusion matrix of the action classifcation on the test dataset is computed to gain a better understanding of the types of missclassification of the network (see \fref{fig:cm-e2e}).
From the confusion matrix, it becomes clear that the network is highly uncertain between the classes \textit{sit} and \textit{stand}.
Only $17\%$ of the test datapoints classified as \textit{stand} were actually from the \textit{stand} class.
In addition, in $39\%$ of the cases where the network predicted \textit{stand}, the ground truth class was \textit{sit}.

% TODO: Intraclass variance: half of each "sit" clip are people standing. see images for examples. Either classes in dataset need to be relabeled or network architecture needs to incorporate more temporal information because 16 frame chunks are not enough. future work!
% TODO: PUT THIS IN CONCLUSION CHAPTER

% One reason for such a large amount of confusion might be that the image context around the subject does not give a clear indication of the action, especially when considering the \textit{stand} class.
% Consider the high accuracy of the model on classes such as \textit{pullup} and \textit{golf}, where each image contains the object used in the interaction, in these cases a pullup bar and a golf club.
% This might also explain why the network is able to correctly estimate the \textit{sit} class in $83\%$ of test data, since a chair or other object the subject is sitting on might lead to useful image features.
% In addition, pose information of the upper body are very similar in both cases, which leads to another possible point of confusion.

% TODO: said stand, was wave
$22\%$ of test images are wrongly classified as \textit{stand}, when the actual class is \textit{wave}.
Would probably be better with more accurate pose at the hands.
Also intraclass variance

% TODO: show good / bad example images

% TODO: Comparison to JHMDB HAR with pretrained


\begin{table}[]
    \small
    \centering
    \scalebox{0.90}{%
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
        \textbf{Ankle (r)} & \textbf{Knee (r)}  & \textbf{Hip (r)} & \textbf{Hip (l)} & \textbf{Knee (l)} & \textbf{Ankle (l)} & \textbf{Pelvis} & \textbf{Upper neck} \\ \hline
        0.55 & 0.52 & 0.63 & 0.64 & 0.48 & 0.50 & 0.56 & 0.72 \\ \hline \hline
        \textbf{Head top} & \textbf{Wrist (r)} & \textbf{Elbow (r)} & \textbf{Shoulder (r)} & \textbf{Shoulder (l)} & \textbf{Elbow (l)} & \textbf{Wrist (l)} & \textbf{Total} \\ \hline
        0.75 & 0.31  & 0.37 & 0.47 & 0.54 & 0.45 & 0.47 & 0.51 \\ \hline \hline
        \textbf{Arms (l)} & \textbf{Arms (r)} & \textbf{Arms (both)} & \textbf{Legs (l)} & \textbf{Legs (r)} & \textbf{Legs (both)} & \textbf{Upper body} & \textbf{Lower body}  \\ \hline
        0.49 & 0.39 & 0.44 & 0.54 & 0.57 & 0.55 & 0.51 & 0.56 \\ \hline \hline
    \end{tabular}}
    \caption{Per joint accuracy, computed on the JHMDB test set using PCK @ 0.1 meassure. In addition, aggregated accuracy values are given in the third row for different sets of joints. \textit{Arms} for both left and right are computed by taking the average of the shoulder, elbow and wrist accuracies. \textit{Legs} are computed the same way, using knee, ankle and hip accuracy values. For \textit{Upper body}, upper neck and head top are added to both \textit{Arms} aggregations. \textit{Lower body} is computed by aggregating both \textit{legs} accuracies as well as pelvis.}
    \label{tab:e2e-perjoint}
\end{table}

In \tref{tab:e2e-perjoint}, the accuracy of the poses are given for each joint, as well as for some aggregations like \textit{upper body}.
On average, the network is more precise on the joints associated with the legs in comparison to the arm joints.
Also, joints associated with the right side of the subjects are detected with a higher accuracy than the joints associated with the left side.
% TODO: Why?

%TODO: explain why it didnt work (or why it did)
% TODO: Idea: show heatmaps per actions or joint accuracies per action. This would be better for explanation
