
@inproceedings{wang_approach_2013,
	title = {An {Approach} to {Pose}-{Based} {Action} {Recognition}},
	doi = {10.1109/CVPR.2013.123},
	abstract = {We address action recognition in videos by modeling the spatial-temporal structures of human poses. We start by improving a state of the art method for estimating human joint locations from videos. More precisely, we obtain the K-best estimations output by the existing method and incorporate additional segmentation cues and temporal constraints to select the “best” one. Then we group the estimated joints into five body parts (e.g. the left arm) and apply data mining techniques to obtain a representation for the spatial-temporal structures of human actions. This representation captures the spatial configurations of body parts in one frame (by spatial-part-sets) as well as the body part movements(by temporal-part-sets) which are characteristic of human actions. It is interpretable, compact, and also robust to errors on joint estimations. Experimental results first show that our approach is able to localize body joints more accurately than existing methods. Next we show that it outperforms state of the art action recognizers on the UCF sport, the Keck Gesture and the MSR-Action3D datasets.},
	booktitle = {2013 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Wang, Chunyu and Wang, Yizhou and Yuille, Alan L.},
	month = jun,
	year = {2013},
	keywords = {action recognition, body part movements, body parts spatial configurations, data mining, Data mining, Dictionaries, Estimation, feature learning, gesture recognition, Histograms, human actions, human joint locations estimation, human poses, Image color analysis, image motion analysis, image representation, image segmentation, Itemsets, joint estimations, Joints, K-best estimations, Keck gesture, MSR-action3D datasets, pose estimation, pose-based action recognition, segmentation cues, spatial-part-sets, spatial-temporal structures modeling, spatial-temporal structures representation, temporal constraints, temporal-part-sets, UCF sport, video signal processing, videos},
	pages = {915--922},
	file = {Wang et al. - 2013 - An Approach to Pose-Based Action Recognition.pdf:/home/matthias/Zotero/storage/ERSPKM4H/ERSPKM4H.pdf:application/pdf}
}

@inproceedings{yang_articulated_2011,
	title = {Articulated pose estimation with flexible mixtures-of-parts},
	doi = {10.1109/CVPR.2011.5995741},
	abstract = {We describe a method for human pose estimation in static images based on a novel representation of part models. Notably, we do not use articulated limb parts, but rather capture orientation with a mixture of templates for each part. We describe a general, flexible mixture model for capturing contextual co-occurrence relations between parts, augmenting standard spring models that encode spatial relations. We show that such relations can capture notions of local rigidity. When co-occurrence and spatial relations are tree-structured, our model can be efficiently optimized with dynamic programming. We present experimental results on standard benchmarks for pose estimation that indicate our approach is the state-of-the-art system for pose estimation, outperforming past work by 50\% while being orders of magnitude faster.},
	booktitle = {{CVPR} 2011},
	author = {Yang, Yi and Ramanan, Deva},
	month = jun,
	year = {2011},
	keywords = {Estimation, image representation, Joints, pose estimation, articulated limb parts, Computational modeling, contextual co-occurrence relation, Deformable models, dynamic programming, flexible mixture model, flexible mixture of parts, human pose estimation, Humans, image coding, spatial relations, Springs, standard spring models, static images, Training, tree structure, trees (mathematics)},
	pages = {1385--1392},
	file = {Yang and Ramanan - 2011 - Articulated pose estimation with flexible mixtures.pdf:/home/matthias/Zotero/storage/LHPQS7SY/Yang and Ramanan - 2011 - Articulated pose estimation with flexible mixtures.pdf:application/pdf}
}

@inproceedings{insafutdinov_deepercut:_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{DeeperCut}: {A} {Deeper}, {Stronger}, and {Faster} {Multi}-person {Pose} {Estimation} {Model}},
	isbn = {978-3-319-46466-4},
	shorttitle = {{DeeperCut}},
	abstract = {The goal of this paper is to advance the state-of-the-art of articulated pose estimation in scenes with multiple people. To that end we contribute on three fronts. We propose (1) improved body part detectors that generate effective bottom-up proposals for body parts; (2) novel image-conditioned pairwise terms that allow to assemble the proposals into a variable number of consistent body part configurations; and (3) an incremental optimization strategy that explores the search space more efficiently thus leading both to better performance and significant speed-up factors. Evaluation is done on two single-person and two multi-person pose estimation benchmarks. The proposed approach significantly outperforms best known multi-person pose estimation results while demonstrating competitive performance on the task of single person pose estimation (Models and code available at http://pose.mpi-inf.mpg.de).},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Insafutdinov, Eldar and Pishchulin, Leonid and Andres, Bjoern and Andriluka, Mykhaylo and Schiele, Bernt},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Area Under Curve, Body Part, Conv4 Bank, Integer Linear Programming, Part Detector},
	pages = {34--50},
	file = {Insafutdinov et al. - 2016 - DeeperCut A Deeper, Stronger, and Faster Multi-pe.pdf:/home/matthias/Zotero/storage/Q4K7LMFG/Q4K7LMFG.pdf:application/pdf}
}

@inproceedings{reining_towards_2018,
	title = {Towards a {Framework} for {Semi}-{Automated} {Annotation} of {Human} {Order} {Picking} {Activities} {Using} {Motion} {Capturing}},
	abstract = {Data creation for Human Activity Recognition (HAR) requires an immense human effort and contextual knowledge for manual annotation. This paper proposes a framework for semi-automated annotation of sequential data in the order picking process using a motion capturing system. Additionally, it introduces proper annotation labels by defining process steps, human activities and simple human movements in order picking scenarios. An attribute representation based on simple human movements meets the challenges set by the versatility of activities in warehousing.},
	booktitle = {2018 {Federated} {Conference} on {Computer} {Science} and {Information} {Systems} ({FedCSIS})},
	author = {Reining, Christopher and Moya Rueda, Fernando and ten Hompel, Michael and Fink, Gernot A.},
	month = sep,
	year = {2018},
	keywords = {image motion analysis, image representation, annotation labels, attribute representation, Computer architecture, contextual knowledge, data creation, human activities, human activity recognition, human movements, human order picking activities, image recognition, immense human effort, Legged locomotion, manual annotation, Manuals, motion capturing system, order picking, production engineering computing, semiautomated annotation, sequential data, Skeleton, Task analysis, Videos, warehousing, Warehousing},
	pages = {817--821},
	file = {Reining et al. - 2018 - Towards a Framework for Semi-Automated Annotation .pdf:/home/matthias/Zotero/storage/MAEVS2GC/Reining et al. - 2018 - Towards a Framework for Semi-Automated Annotation .pdf:application/pdf}
}

@article{chou_self_2017,
	title = {Self {Adversarial} {Training} for {Human} {Pose} {Estimation}},
	url = {https://arxiv.org/abs/1707.02439v2},
	abstract = {This paper presents a deep learning based approach to the problem of human
pose estimation. We employ generative adversarial networks as our learning
paradigm in which we set up two stacked hourglass networks with the same
architecture, one as the generator and the other as the discriminator. The
generator is used as a human pose estimator after the training is done. The
discriminator distinguishes ground-truth heatmaps from generated ones, and
back-propagates the adversarial loss to the generator. This process enables the
generator to learn plausible human body configurations and is shown to be
useful for improving the prediction accuracy.},
	language = {en},
	urldate = {2019-04-20},
	author = {Chou, Chia-Jung and Chien, Jui-Ting and Chen, Hwann-Tzong},
	month = jul,
	year = {2017},
	file = {Full Text PDF:/home/matthias/Zotero/storage/VSZPQT5E/VSZPQT5E.pdf:application/pdf;Snapshot:/home/matthias/Zotero/storage/33W6P3BH/Chou et al. - 2017 - Self Adversarial Training for Human Pose Estimatio.html:text/html}
}

@inproceedings{johnson_clustered_2010,
	title = {Clustered {Pose} and {Nonlinear} {Appearance} {Models} for {Human} {Pose} {Estimation}},
	booktitle = {Proceedings of the {British} {Machine} {Vision} {Conference}},
	author = {Johnson, Sam and Everingham, Mark},
	year = {2010},
	annote = {doi:10.5244/C.24.12}
}

@inproceedings{johnson_learning_2011,
	title = {Learning {Effective} {Human} {Pose} {Estimation} from {Inaccurate} {Annotation}},
	booktitle = {Proceedings of {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Johnson, Sam and Everingham, Mark},
	year = {2011}
}

@inproceedings{andriluka_2d_2014,
	title = {2D {Human} {Pose} {Estimation}: {New} {Benchmark} and {State} of the {Art} {Analysis}},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Andriluka, Mykhaylo and Pishchulin, Leonid and Gehler, Peter and Schiele, Bernt},
	year = {2014},
	file = {Andriluka et al. - 2014 - 2D Human Pose Estimation New Benchmark and State .pdf:/home/matthias/Zotero/storage/5H69WV9S/Andriluka et al. - 2014 - 2D Human Pose Estimation New Benchmark and State .pdf:application/pdf}
}

@inproceedings{toshev_deeppose:_2014,
	address = {Columbus, OH, USA},
	title = {{DeepPose}: {Human} {Pose} {Estimation} via {Deep} {Neural} {Networks}},
	isbn = {978-1-4799-5118-5},
	shorttitle = {{DeepPose}},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909610},
	doi = {10.1109/CVPR.2014.214},
	abstract = {We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regressors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formulation which capitalizes on recent advances in Deep Learning. We present a detailed empirical analysis with state-ofart or better performance on four academic benchmarks of diverse real-world images.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Toshev, Alexander and Szegedy, Christian},
	month = jun,
	year = {2014},
	pages = {1653--1660},
	file = {Toshev and Szegedy - 2014 - DeepPose Human Pose Estimation via Deep Neural Ne.pdf:/home/matthias/Zotero/storage/Q36UJJG7/Toshev and Szegedy - 2014 - DeepPose Human Pose Estimation via Deep Neural Ne.pdf:application/pdf}
}

@inproceedings{gong_look_2017,
	address = {Honolulu, HI},
	title = {Look into {Person}: {Self}-{Supervised} {Structure}-{Sensitive} {Learning} and a {New} {Benchmark} for {Human} {Parsing}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {Look into {Person}},
	url = {http://ieeexplore.ieee.org/document/8100198/},
	doi = {10.1109/CVPR.2017.715},
	abstract = {Human parsing has recently attracted a lot of research interests due to its huge application potentials. However existing datasets have limited number of images and annotations, and lack the variety of human appearances and the coverage of challenging cases in unconstrained environment. In this paper, we introduce a new benchmark1 “Look into Person (LIP)” that makes a signiﬁcant advance in terms of scalability, diversity and difﬁculty, a contribution that we feel is crucial for future developments in humancentric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels, which are captured from a wider range of viewpoints, occlusions and background complexity. Given these rich annotations we perform detailed analyses of the leading human parsing approaches, gaining insights into the success and failures of these methods. Furthermore, in contrast to the existing efforts on improving the feature discriminative capability, we solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into parsing results without resorting to extra supervision (i.e., no need for speciﬁcally labeling human joints in model training). Our self-supervised learning framework can be injected into any advanced neural networks to help incorporate rich high-level knowledge regarding human joints from a global perspective and improve the parsing results. Extensive evaluations on our LIP and the public PASCAL-PersonPart dataset demonstrate the superiority of our method.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Gong, Ke and Liang, Xiaodan and Zhang, Dongyu and Shen, Xiaohui and Lin, Liang},
	month = jul,
	year = {2017},
	pages = {6757--6765},
	file = {Gong et al. - 2017 - Look into Person Self-Supervised Structure-Sensit.pdf:/home/matthias/Zotero/storage/H6S978WI/Gong et al. - 2017 - Look into Person Self-Supervised Structure-Sensit.pdf:application/pdf}
}

@inproceedings{wei_convolutional_2016,
	address = {Las Vegas, NV, USA},
	title = {Convolutional {Pose} {Machines}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780880/},
	doi = {10.1109/CVPR.2016.511},
	abstract = {Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly reﬁned estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difﬁculty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wei, Shih-En and Ramakrishna, Varun and Kanade, Takeo and Sheikh, Yaser},
	month = jun,
	year = {2016},
	pages = {4724--4732},
	file = {Wei et al. - 2016 - Convolutional Pose Machines.pdf:/home/matthias/Zotero/storage/YCBLGD9Z/Wei et al. - 2016 - Convolutional Pose Machines.pdf:application/pdf}
}

@inproceedings{newell_stacked_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Stacked {Hourglass} {Networks} for {Human} {Pose} {Estimation}},
	isbn = {978-3-319-46484-8},
	abstract = {This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a “stacked hourglass” network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Newell, Alejandro and Yang, Kaiyu and Deng, Jia},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Human pose estimation, occlusion},
	pages = {483--499},
	file = {Newell et al. - 2016 - Stacked Hourglass Networks for Human Pose Estimati.pdf:/home/matthias/Zotero/storage/3UCQ7PCS/Newell et al. - 2016 - Stacked Hourglass Networks for Human Pose Estimati.pdf:application/pdf}
}

@inproceedings{martinez_simple_2017,
	address = {Venice},
	title = {A {Simple} {Yet} {Effective} {Baseline} for 3d {Human} {Pose} {Estimation}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237550/},
	doi = {10.1109/ICCV.2017.288},
	abstract = {Following the success of deep convolutional networks, state-of-the-art methods for 3d human pose estimation have focused on deep end-to-end systems that predict 3d joint locations given raw image pixels. Despite their excellent performance, it is often not easy to understand whether their remaining error stems from a limited 2d pose (visual) understanding, or from a failure to map 2d poses into 3dimensional positions.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Martinez, Julieta and Hossain, Rayat and Romero, Javier and Little, James J.},
	month = oct,
	year = {2017},
	pages = {2659--2668},
	file = {Martinez et al. - 2017 - A Simple Yet Effective Baseline for 3d Human Pose .pdf:/home/matthias/Zotero/storage/Z2TTE5AP/Martinez et al. - 2017 - A Simple Yet Effective Baseline for 3d Human Pose .pdf:application/pdf}
}

@inproceedings{guler_densepose:_2018,
	address = {Salt Lake City, UT, USA},
	title = {{DensePose}: {Dense} {Human} {Pose} {Estimation} in the {Wild}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{DensePose}},
	url = {https://ieeexplore.ieee.org/document/8578860/},
	doi = {10.1109/CVPR.2018.00762},
	abstract = {In this work we establish dense correspondences between an RGB image and a surface-based representation of the human body, a task we refer to as dense human pose estimation. We gather dense correspondences for 50K persons appearing in the COCO dataset by introducing an efﬁcient annotation pipeline. We then use our dataset to train CNN-based systems that deliver dense correspondence ‘in the wild’, namely in the presence of background, occlusions and scale variations. We improve our training set’s effectiveness by training an inpainting network that can ﬁll in missing ground truth values and report improvements with respect to the best results that would be achievable in the past. We experiment with fully-convolutional networks and region-based models and observe a superiority of the latter. We further improve accuracy through cascading, obtaining a system that delivers highly-accurate results at multiple frames per second on a single gpu. Supplementary materials, data, code, and videos are provided on the project page http://densepose.org.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Guler, Riza Alp and Neverova, Natalia and Kokkinos, Iasonas},
	month = jun,
	year = {2018},
	pages = {7297--7306},
	file = {Guler et al. - 2018 - DensePose Dense Human Pose Estimation in the Wild.pdf:/home/matthias/Zotero/storage/H4N36ZJ9/Guler et al. - 2018 - DensePose Dense Human Pose Estimation in the Wild.pdf:application/pdf}
}

@inproceedings{khalid_multi-modal_2018,
	title = {Multi-{Modal} {Three}-{Stream} {Network} for {Action} {Recognition}},
	doi = {10.1109/ICPR.2018.8546131},
	abstract = {Human action recognition in video is an active yet challenging research topic due to high variation and complexity of data. In this paper, a novel video based action recognition framework utilizing complementary cues is proposed to handle this complex problem. Inspired by the successful two stream networks for action classification, additional pose features are studied and fused to enhance understanding of human action in a more abstract and semantic way. Towards practices, not only ground truth poses but also noisy estimated poses are incorporated in the framework with our proposed pre-processing module. The whole framework and each cue are evaluated on varied benchmarking datasets as JHMDB, sub-JHMDB and Penn Action. Our results outperform state-of-the-art performance on these datasets and show the strength of complementary cues.},
	booktitle = {2018 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Khalid, M. U. and Yu, J.},
	month = aug,
	year = {2018},
	keywords = {image motion analysis, pose estimation, video signal processing, Training, image recognition, action classification, active yet challenging research topic, complementary cues, complex problem, feature extraction, Feature extraction, ground truth poses, human action recognition, image classification, image sequences, Interpolation, multimodal three-stream network, noisy estimated poses, object detection, Pattern recognition, Penn Action, successful two stream networks, Tensile stress, Three-dimensional displays, Two dimensional displays, video based action recognition framework},
	pages = {3210--3215},
	file = {IEEE Xplore Abstract Record:/home/matthias/Zotero/storage/4L4TMCIJ/8546131.html:text/html;IEEE Xplore Full Text PDF:/home/matthias/Zotero/storage/TJG5VBLH/Khalid and Yu - 2018 - Multi-Modal Three-Stream Network for Action Recogn.pdf:application/pdf}
}

@incollection{simonyan_two-stream_2014,
	title = {Two-{Stream} {Convolutional} {Networks} for {Action} {Recognition} in {Videos}},
	url = {http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf},
	urldate = {2019-04-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Simonyan, Karen and Zisserman, Andrew},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {568--576},
	file = {NIPS Full Text PDF:/home/matthias/Zotero/storage/YQZ9UN6X/Simonyan and Zisserman - 2014 - Two-Stream Convolutional Networks for Action Recog.pdf:application/pdf;NIPS Snapshot:/home/matthias/Zotero/storage/BVM68IDS/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.html:text/html}
}

@inproceedings{cao_realtime_2017,
	address = {Honolulu, HI},
	title = {Realtime {Multi}-person 2D {Pose} {Estimation} {Using} {Part} {Affinity} {Fields}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099626/},
	doi = {10.1109/CVPR.2017.143},
	abstract = {We present an approach to efﬁciently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Afﬁnity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed ﬁrst in the inaugural COCO 2016 keypoints challenge, and signiﬁcantly exceeds the previous state-of-the-art result on the MPII MultiPerson benchmark, both in performance and efﬁciency.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
	month = jul,
	year = {2017},
	pages = {1302--1310},
	file = {Cao et al. - 2017 - Realtime Multi-person 2D Pose Estimation Using Par.pdf:/home/matthias/Zotero/storage/XP9UTUIT/Cao et al. - 2017 - Realtime Multi-person 2D Pose Estimation Using Par.pdf:application/pdf}
}

@incollection{leibe_keep_2016,
	address = {Cham},
	title = {Keep {It} {SMPL}: {Automatic} {Estimation} of 3D {Human} {Pose} and {Shape} from a {Single} {Image}},
	volume = {9909},
	isbn = {978-3-319-46453-4 978-3-319-46454-1},
	shorttitle = {Keep {It} {SMPL}},
	url = {http://link.springer.com/10.1007/978-3-319-46454-1_34},
	abstract = {We describe the ﬁrst method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we ﬁrst use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then ﬁt (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly ﬁt it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Bogo, Federica and Kanazawa, Angjoo and Lassner, Christoph and Gehler, Peter and Romero, Javier and Black, Michael J.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	doi = {10.1007/978-3-319-46454-1_34},
	pages = {561--578},
	file = {Bogo et al. - 2016 - Keep It SMPL Automatic Estimation of 3D Human Pos.pdf:/home/matthias/Zotero/storage/G7NY9DH2/Bogo et al. - 2016 - Keep It SMPL Automatic Estimation of 3D Human Pos.pdf:application/pdf}
}

@inproceedings{andriluka_posetrack:_2018,
	address = {Salt Lake City, UT, USA},
	title = {{PoseTrack}: {A} {Benchmark} for {Human} {Pose} {Estimation} and {Tracking}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{PoseTrack}},
	url = {https://ieeexplore.ieee.org/document/8578640/},
	doi = {10.1109/CVPR.2018.00542},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Andriluka, Mykhaylo and Iqbal, Umar and Insafutdinov, Eldar and Pishchulin, Leonid and Milan, Anton and Gall, Juergen and Schiele, Bernt},
	month = jun,
	year = {2018},
	pages = {5167--5176},
	file = {Andriluka et al. - 2018 - PoseTrack A Benchmark for Human Pose Estimation a.pdf:/home/matthias/Zotero/storage/B9MSZF5G/B9MSZF5G.pdf:application/pdf}
}

@article{dantone_body_2014,
	title = {Body {Parts} {Dependent} {Joint} {Regressors} for {Human} {Pose} {Estimation} in {Still} {Images}},
	volume = {36},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/6802375/},
	doi = {10.1109/TPAMI.2014.2318702},
	abstract = {In this work, we address the problem of estimating 2d human pose from still images. Articulated body pose estimation is challenging due to the large variation in body poses and appearances of the different body parts. Recent methods that rely on the pictorial structure framework have shown to be very successful in solving this task. They model the body part appearances using discriminatively trained, independent part templates and the spatial relations of the body parts using a tree model. Within such a framework, we address the problem of obtaining better part templates which are able to handle a very high variation in appearance. To this end, we introduce parts dependent body joint regressors which are random forests that operate over two layers. While the ﬁrst layer acts as an independent body part classiﬁer, the second layer takes the estimated class distributions of the ﬁrst one into account and is thereby able to predict joint locations by modeling the interdependence and co-occurrence of the parts. This helps to overcome typical ambiguities of tree structures, such as self-similarities of legs and arms. In addition, we introduce a novel dataset termed F ashionP ose that contains over 7, 000 images with a challenging variation of body part appearances due to a large variation of dressing styles. In the experiments, we demonstrate that the proposed parts dependent joint regressors outperform independent classiﬁers or regressors. The method also performs better or similar to the state-of-the-art in terms of accuracy, while running with a couple of frames per second.},
	language = {en},
	number = {11},
	urldate = {2019-04-25},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Dantone, Matthias and Gall, Juergen and Leistner, Christian and Van Gool, Luc},
	month = nov,
	year = {2014},
	pages = {2131--2143},
	file = {Dantone et al. - 2014 - Body Parts Dependent Joint Regressors for Human Po.pdf:/home/matthias/Zotero/storage/MA3YZMKS/Dantone et al. - 2014 - Body Parts Dependent Joint Regressors for Human Po.pdf:application/pdf}
}

@article{sigal_humaneva:_2010,
	title = {{HumanEva}: {Synchronized} {Video} and {Motion} {Capture} {Dataset} and {Baseline} {Algorithm} for {Evaluation} of {Articulated} {Human} {Motion}},
	volume = {87},
	issn = {0920-5691, 1573-1405},
	shorttitle = {{HumanEva}},
	url = {http://link.springer.com/10.1007/s11263-009-0273-6},
	doi = {10.1007/s11263-009-0273-6},
	language = {en},
	number = {1-2},
	urldate = {2019-04-25},
	journal = {International Journal of Computer Vision},
	author = {Sigal, Leonid and Balan, Alexandru O. and Black, Michael J.},
	month = mar,
	year = {2010},
	pages = {4--27},
	file = {Sigal et al. - 2010 - HumanEva Synchronized Video and Motion Capture Da.pdf:/home/matthias/Zotero/storage/4YD9UFS2/Sigal et al. - 2010 - HumanEva Synchronized Video and Motion Capture Da.pdf:application/pdf}
}

@inproceedings{jhuang_towards_2013,
	title = {Towards understanding action recognition},
	abstract = {Although action recognition in videos is widely studied, current methods often fail on real-world datasets. Many recent approaches improve accuracy and robustness to cope with challenging video sequences, but it is often unclear what affects the results most. This paper attempts to provide insights based on a systematic performance evaluation using thoroughly-annotated data of human actions. We annotate human Joints for the HMDB dataset (J-HMDB). This annotation can be used to derive ground truth optical ﬂow and segmentation. We evaluate current methods using this dataset and systematically replace the output of various algorithms with ground truth. This enables us to discover what is important – for example, should we work on improving ﬂow algorithms, estimating human bounding boxes, or enabling pose estimation? In summary, we ﬁnd that highlevel pose features greatly outperform low/mid level features; in particular, pose over time is critical. While current pose estimation algorithms are far from perfect, features extracted from estimated pose on a subset of J-HMDB, in which the full body is visible, outperform low/mid-level features. We also ﬁnd that the accuracy of the action recognition framework can be greatly increased by reﬁning the underlying low/mid level features; this suggests it is important to improve optical ﬂow and human detection algorithms. Our analysis and J-HMDB dataset should facilitate a deeper understanding of action recognition algorithms.},
	language = {en},
	booktitle = {Proceedings of the {IEEE} international conference on computer vision},
	author = {Jhuang, Hueihan and Gall, Juergen and Zufﬁ, Silvia and Schmid, Cordelia and Black, Michael J},
	year = {2013},
	pages = {3192--3199},
	file = {Jhuang et al. - Towards understanding action recognition.pdf:/home/matthias/Zotero/storage/7J7C65KD/Jhuang et al. - Towards understanding action recognition.pdf:application/pdf}
}

@inproceedings{zhang_actemes_2013,
	title = {From {Actemes} to {Action}: {A} {Strongly}-{Supervised} {Representation} for {Detailed} {Action} {Understanding}},
	shorttitle = {From {Actemes} to {Action}},
	doi = {10.1109/ICCV.2013.280},
	abstract = {This paper presents a novel approach for analyzing human actions in non-scripted, unconstrained video settings based on volumetric, x-y-t, patch classifiers, termed actemes. Unlike previous action-related work, the discovery of patch classifiers is posed as a strongly-supervised process. Specifically, key point labels (e.g., position) across space time are used in a data-driven training process to discover patches that are highly clustered in the space time key point configuration space. To support this process, a new human action dataset consisting of challenging consumer videos is introduced, where notably the action label, the 2D position of a set of key points and their visibilities are provided for each video frame. On a novel input video, each acteme is used in a sliding volume scheme to yield a set of sparse, non-overlapping detections. These detections provide the intermediate substrate for segmenting out the action. For action classification, the proposed representation shows significant improvement over state-of-the-art low-level features, while providing spatiotemporal localization as additional output, which sheds further light into detailed action understanding.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	author = {Zhang, W. and Zhu, M. and Derpanis, K. G.},
	month = dec,
	year = {2013},
	keywords = {gesture recognition, image representation, video signal processing, Training, action classification, image classification, Penn Action, actemes, action detection, action label, action understanding, action-related work, Cameras, consumer videos, Context, data-driven training process, human action dataset, human actions analyzing, patch classifiers discovery, Semantics, sliding volume scheme, space time, spacetime key-point configuration space, sparse nonoverlapping detections, spatiotemporal localization, Spatiotemporal phenomena, supervised representation, Trajectory, unconstrained video settings, Visualization},
	pages = {2248--2255},
	file = {IEEE Xplore Abstract Record:/home/matthias/Zotero/storage/JH7MIMKB/6751390.html:text/html;IEEE Xplore Full Text PDF:/home/matthias/Zotero/storage/6B6GJATY/Zhang et al. - 2013 - From Actemes to Action A Strongly-Supervised Repr.pdf:application/pdf}
}

@incollection{tompson_joint_2014,
	title = {Joint {Training} of a {Convolutional} {Network} and a {Graphical} {Model} for {Human} {Pose} {Estimation}},
	url = {http://papers.nips.cc/paper/5573-joint-training-of-a-convolutional-network-and-a-graphical-model-for-human-pose-estimation.pdf},
	urldate = {2019-04-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Tompson, Jonathan J and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {1799--1807},
	file = {NIPS Full Text PDF:/home/matthias/Zotero/storage/WT2YDKQR/Tompson et al. - 2014 - Joint Training of a Convolutional Network and a Gr.pdf:application/pdf;NIPS Snapshot:/home/matthias/Zotero/storage/DHH68QVG/5573-joint-training-of-a-convolutional-network-and-a-graphical-model-for-human-pose-estimation.html:text/html}
}

@inproceedings{carreira_human_2016,
	address = {Las Vegas, NV, USA},
	title = {Human {Pose} {Estimation} with {Iterative} {Error} {Feedback}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780881/},
	doi = {10.1109/CVPR.2016.512},
	abstract = {Hierarchical feature extractors such as Convolutional Networks (ConvNets) have achieved impressive performance on a variety of classiﬁcation tasks using purely feedforward processing. Feedforward architectures can learn rich representations of the input space but do not explicitly model dependencies in the output spaces, that are quite structured for tasks such as articulated human pose estimation or object segmentation. Here we propose a framework that expands the expressive power of hierarchical feature extractors to encompass both input and output spaces, by introducing top-down feedback. Instead of directly predicting the outputs in one go, we use a self-correcting model that progressively changes an initial solution by feeding back error predictions, in a process we call Iterative Error Feedback (IEF). IEF shows excellent performance on the task of articulated pose estimation in the challenging MPII and LSP benchmarks, matching the state-of-the-art without requiring ground truth scale annotation.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Carreira, Joao and Agrawal, Pulkit and Fragkiadaki, Katerina and Malik, Jitendra},
	month = jun,
	year = {2016},
	pages = {4733--4742},
	file = {Carreira et al. - 2016 - Human Pose Estimation with Iterative Error Feedbac.pdf:/home/matthias/Zotero/storage/MDWP3UJ5/Carreira et al. - 2016 - Human Pose Estimation with Iterative Error Feedbac.pdf:application/pdf}
}

@inproceedings{hu_bottom-up_2016,
	address = {Las Vegas, NV, USA},
	title = {Bottom-{Up} and {Top}-{Down} {Reasoning} with {Hierarchical} {Rectified} {Gaussians}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780973/},
	doi = {10.1109/CVPR.2016.604},
	abstract = {Convolutional neural nets (CNNs) have demonstrated remarkable performance in recent history. Such approaches tend to work in a “unidirectional” bottom-up feed-forward fashion. However, practical experience and biological evidence tells us that feedback plays a crucial role, particularly for detailed spatial understanding tasks. This work explores “bidirectional” architectures that also reason with top-down feedback: neural units are inﬂuenced by both lower and higher-level units.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Hu, Peiyun and Ramanan, Deva},
	month = jun,
	year = {2016},
	pages = {5600--5609},
	file = {Hu and Ramanan - 2016 - Bottom-Up and Top-Down Reasoning with Hierarchical.pdf:/home/matthias/Zotero/storage/W52WHAQF/Hu and Ramanan - 2016 - Bottom-Up and Top-Down Reasoning with Hierarchical.pdf:application/pdf}
}

@article{bulat_human_2016,
	title = {Human pose estimation via {Convolutional} {Part} {Heatmap} {Regression}},
	url = {http://arxiv.org/abs/1609.01743},
	doi = {10.1007/978-3-319-46478-7_44},
	abstract = {This paper is on human pose estimation using Convolutional Neural Networks. Our main contribution is a CNN cascaded architecture specifically designed for learning part relationships and spatial context, and robustly inferring pose even for the case of severe part occlusions. To this end, we propose a detection-followed-by-regression CNN cascade. The first part of our cascade outputs part detection heatmaps and the second part performs regression on these heatmaps. The benefits of the proposed architecture are multi-fold: It guides the network where to focus in the image and effectively encodes part constraints and context. More importantly, it can effectively cope with occlusions because part detection heatmaps for occluded parts provide low confidence scores which subsequently guide the regression part of our network to rely on contextual information in order to predict the location of these parts. Additionally, we show that the proposed cascade is flexible enough to readily allow the integration of various CNN architectures for both detection and regression, including recent ones based on residual learning. Finally, we illustrate that our cascade achieves top performance on the MPII and LSP data sets. Code can be downloaded from http://www.cs.nott.ac.uk/{\textasciitilde}psxab5/},
	urldate = {2019-04-25},
	journal = {arXiv:1609.01743 [cs]},
	author = {Bulat, Adrian and Tzimiropoulos, Georgios},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.01743},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: accepted to ECCV 2016},
	file = {arXiv\:1609.01743 PDF:/home/matthias/Zotero/storage/WIPV68ZS/Bulat and Tzimiropoulos - 2016 - Human pose estimation via Convolutional Part Heatm.pdf:application/pdf;arXiv.org Snapshot:/home/matthias/Zotero/storage/GWTEGHCA/1609.html:text/html}
}

@inproceedings{rafi_efficient_2016,
	address = {York, UK},
	title = {An {Efficient} {Convolutional} {Network} for {Human} {Pose} {Estimation}},
	isbn = {978-1-901725-59-9},
	url = {http://www.bmva.org/bmvc/2016/papers/paper109/index.html},
	doi = {10.5244/C.30.109},
	abstract = {In recent years, human pose estimation has greatly beneﬁted from deep learning and huge gains in performance have been achieved. The trend to maximise the accuracy on benchmarks, however, resulted in computationally expensive deep network architectures that require expensive hardware and pre-training on large datasets. This makes it difﬁcult to compare different methods and to reproduce existing results. In this paper, we therefore propose an efﬁcient deep network architecture that can be efﬁciently trained on mid-range GPUs without the need of any pre-training. Despite the low computational requirements of our network, it is on par with much more complex models on popular benchmarks for human pose estimation.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2016},
	publisher = {British Machine Vision Association},
	author = {Rafi, Umer and Leibe, Bastian and Gall, Juergen and Kostrikov, Ilya},
	year = {2016},
	pages = {109.1--109.11},
	file = {Rafi et al. - 2016 - An Efficient Convolutional Network for Human Pose .pdf:/home/matthias/Zotero/storage/VLD48VTN/Rafi et al. - 2016 - An Efficient Convolutional Network for Human Pose .pdf:application/pdf}
}

@inproceedings{pfister_flowing_2015,
	address = {Santiago, Chile},
	title = {Flowing {ConvNets} for {Human} {Pose} {Estimation} in {Videos}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410579/},
	doi = {10.1109/ICCV.2015.222},
	abstract = {The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to beneﬁt from temporal context by combining information across the multiple frames using optical ﬂow.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Pfister, Tomas and Charles, James and Zisserman, Andrew},
	month = dec,
	year = {2015},
	pages = {1913--1921},
	file = {Pfister et al. - 2015 - Flowing ConvNets for Human Pose Estimation in Vide.pdf:/home/matthias/Zotero/storage/S2PBL27W/Pfister et al. - 2015 - Flowing ConvNets for Human Pose Estimation in Vide.pdf:application/pdf}
}

@inproceedings{charles_personalizing_2016,
	title = {Personalizing {Human} {Video} {Pose} {Estimation}},
	abstract = {We propose a personalized ConvNet pose estimator that automatically adapts itself to the uniqueness of a person’s appearance to improve pose estimation in long videos.},
	language = {en},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Charles, James and Pfister, Tomas and Magee, Derek and Hogg, David and Zisserman, Andrew},
	year = {2016},
	pages = {3063--3072},
	file = {Charles et al. - Personalizing Human Video Pose Estimation.pdf:/home/matthias/Zotero/storage/ZL7IF5ZC/Charles et al. - Personalizing Human Video Pose Estimation.pdf:application/pdf}
}

@article{iqbal_pose_2016,
	title = {Pose for {Action} - {Action} for {Pose}},
	url = {http://arxiv.org/abs/1603.04037},
	abstract = {In this work we propose to utilize information about human actions to improve pose estimation in monocular videos. To this end, we present a pictorial structure model that exploits high-level information about activities to incorporate higher-order part dependencies by modeling action specific appearance models and pose priors. However, instead of using an additional expensive action recognition framework, the action priors are efficiently estimated by our pose estimation framework. This is achieved by starting with a uniform action prior and updating the action prior during pose estimation. We also show that learning the right amount of appearance sharing among action classes improves the pose estimation. We demonstrate the effectiveness of the proposed method on two challenging datasets for pose estimation and action recognition with over 80,000 test images.},
	urldate = {2019-04-25},
	journal = {arXiv:1603.04037 [cs]},
	author = {Iqbal, Umar and Garbade, Martin and Gall, Juergen},
	month = mar,
	year = {2016},
	note = {arXiv: 1603.04037},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to FG-2017},
	file = {arXiv\:1603.04037 PDF:/home/matthias/Zotero/storage/YFP952VJ/Iqbal et al. - 2016 - Pose for Action - Action for Pose.pdf:application/pdf;arXiv.org Snapshot:/home/matthias/Zotero/storage/977VHYE5/1603.html:text/html}
}

@inproceedings{gkioxari_chained_2016,
	address = {Cham},
	title = {Chained {Predictions} {Using} {Convolutional} {Neural} {Networks}},
	isbn = {978-3-319-46493-0},
	abstract = {In this work, we present an adaptation of the sequence-to-sequence model for structured vision tasks. In this model, the output variables for a given input are predicted sequentially using neural networks. The prediction for each output variable depends not only on the input but also on the previously predicted output variables. The model is applied to spatial localization tasks and uses convolutional neural networks (CNNs) for processing input images and a multi-scale deconvolutional architecture for making spatial predictions at each step. We explore the impact of weight sharing with a recurrent connection matrix between consecutive predictions, and compare it to a formulation where these weights are not tied. Untied weights are particularly suited for problems with a fixed sized structure, where different classes of output are predicted at different steps. We show that chain models achieve top performing results on human pose estimation from images and videos.},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Gkioxari, Georgia and Toshev, Alexander and Jaitly, Navdeep},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	pages = {728--743},
	file = {Gkioxari et al. - 2016 - Chained Predictions Using Convolutional Neural Net.pdf:/home/matthias/Zotero/storage/F2CKZYDX/F2CKZYDX.pdf:application/pdf}
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
	file = {He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:/home/matthias/Zotero/storage/HTIPXHQM/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@incollection{newell_associative_2017,
	title = {Associative {Embedding}: {End}-to-{End} {Learning} for {Joint} {Detection} and {Grouping}},
	shorttitle = {Associative {Embedding}},
	url = {http://papers.nips.cc/paper/6822-associative-embedding-end-to-end-learning-for-joint-detection-and-grouping.pdf},
	urldate = {2019-04-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Newell, Alejandro and Huang, Zhiao and Deng, Jia},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {2277--2287},
	file = {Newell et al. - 2017 - Associative Embedding End-to-End Learning for Joi.pdf:/home/matthias/Zotero/storage/SVIP8TAR/Newell et al. - 2017 - Associative Embedding End-to-End Learning for Joi.pdf:application/pdf;NIPS Snapshot:/home/matthias/Zotero/storage/PP4EEZS3/6822-associative-embedding-end-to-end-learning-for-joint-detection-and-grouping.html:text/html}
}

@inproceedings{chu_multi-context_2017,
	address = {Honolulu, HI},
	title = {Multi-context {Attention} for {Human} {Pose} {Estimation}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100084/},
	doi = {10.1109/CVPR.2017.601},
	abstract = {In this paper, we propose to incorporate convolutional neural networks with a multi-context attention mechanism into an end-to-end framework for human pose estimation. We adopt stacked hourglass networks to generate attention maps from features at multiple resolutions with various semantics. The Conditional Random Field (CRF) is utilized to model the correlations among neighboring regions in the attention map. We further combine the holistic attention model, which focuses on the global consistency of the full human body, and the body part attention model, which focuses on detailed descriptions for different body parts. Hence our model has the ability to focus on different granularity from local salient regions to global semanticconsistent spaces. Additionally, we design novel Hourglass Residual Units (HRUs) to increase the receptive ﬁeld of the network. These units are extensions of residual units with a side branch incorporating ﬁlters with larger receptive ﬁeld, hence features with various scales are learned and combined within the HRUs. The effectiveness of the proposed multi-context attention mechanism and the hourglass residual units is evaluated on two widely used human pose estimation benchmarks. Our approach outperforms all existing methods on both benchmarks over all the body parts. Code has been made publicly available.},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chu, Xiao and Yang, Wei and Ouyang, Wanli and Ma, Cheng and Yuille, Alan L. and Wang, Xiaogang},
	month = jul,
	year = {2017},
	pages = {5669--5678}
}

@inproceedings{girdhar_detect-and-track:_2018,
	address = {Salt Lake City, UT},
	title = {Detect-and-{Track}: {Efficient} {Pose} {Estimation} in {Videos}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {Detect-and-{Track}},
	url = {https://ieeexplore.ieee.org/document/8578142/},
	doi = {10.1109/CVPR.2018.00044},
	abstract = {This paper addresses the problem of estimating and tracking human body keypoints in complex, multi-person video. We propose an extremely lightweight yet highly effective approach that builds upon the latest advancements in human detection [17] and video understanding [5]. Our method operates in two-stages: keypoint estimation in frames or short clips, followed by lightweight tracking to generate keypoint predictions linked over the entire video. For frame-level pose estimation we experiment with Mask R-CNN, as well as our own proposed 3D extension of this model, which leverages temporal information over small clips to generate more robust frame predictions. We conduct extensive ablative experiments on the newly released multi-person video pose estimation benchmark, PoseTrack, to validate various design choices of our model. Our approach achieves an accuracy of 55.2\% on the validation and 51.8\% on the test set using the Multi-Object Tracking Accuracy (MOTA) metric, and achieves state of the art performance on the ICCV 2017 PoseTrack keypoint tracking challenge [1].},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Girdhar, Rohit and Gkioxari, Georgia and Torresani, Lorenzo and Paluri, Manohar and Tran, Du},
	month = jun,
	year = {2018},
	pages = {350--359},
	file = {Girdhar et al. - 2018 - Detect-and-Track Efficient Pose Estimation in Vid.pdf:/home/matthias/Zotero/storage/FZYMAANH/Girdhar et al. - 2018 - Detect-and-Track Efficient Pose Estimation in Vid.pdf:application/pdf}
}

@inproceedings{song_thin-slicing_2017,
	address = {Honolulu, HI},
	title = {Thin-{Slicing} {Network}: {A} {Deep} {Structured} {Model} for {Pose} {Estimation} in {Videos}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {Thin-{Slicing} {Network}},
	url = {http://ieeexplore.ieee.org/document/8100073/},
	doi = {10.1109/CVPR.2017.590},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Song, Jie and Wang, Limin and Gool, Luc Van and Hilliges, Otmar},
	month = jul,
	year = {2017},
	pages = {5563--5572},
	file = {Song et al. - 2017 - Thin-Slicing Network A Deep Structured Model for .pdf:/home/matthias/Zotero/storage/B47GRJVG/Song et al. - 2017 - Thin-Slicing Network A Deep Structured Model for .pdf:application/pdf}
}

@inproceedings{zhou_sparseness_2016,
	address = {Las Vegas, NV, USA},
	title = {Sparseness {Meets} {Deepness}: 3D {Human} {Pose} {Estimation} from {Monocular} {Video}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {Sparseness {Meets} {Deepness}},
	url = {http://ieeexplore.ieee.org/document/7780906/},
	doi = {10.1109/CVPR.2016.537},
	abstract = {This paper addresses the challenge of 3D full-body human pose estimation from a monocular image sequence. Here, two cases are considered: (i) the image locations of the human joints are provided and (ii) the image locations of joints are unknown. In the former case, a novel approach is introduced that integrates a sparsity-driven 3D geometric prior and temporal smoothness. In the latter case, the former case is extended by treating the image locations of the joints as latent variables to take into account considerable uncertainties in 2D joint locations. A deep fully convolutional network is trained to predict the uncertainty maps of the 2D joint locations. The 3D pose estimates are realized via an Expectation-Maximization algorithm over the entire sequence, where it is shown that the 2D joint location uncertainties can be conveniently marginalized out during inference. Empirical evaluation on the Human3.6M dataset shows that the proposed approaches achieve greater 3D pose estimation accuracy over state-of-the-art baselines. Further, the proposed approach outperforms a publicly available 2D pose estimation baseline on the challenging PennAction dataset.},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhou, Xiaowei and Zhu, Menglong and Leonardos, Spyridon and Derpanis, Konstantinos G. and Daniilidis, Kostas},
	month = jun,
	year = {2016},
	pages = {4966--4975},
	file = {Zhou et al. - 2016 - Sparseness Meets Deepness 3D Human Pose Estimatio.pdf:/home/matthias/Zotero/storage/AQTIA2RF/Zhou et al. - 2016 - Sparseness Meets Deepness 3D Human Pose Estimatio.pdf:application/pdf}
}

@article{mehta_vnect:_2017,
	title = {{VNect}: real-time 3D human pose estimation with a single {RGB} camera},
	volume = {36},
	issn = {07300301},
	shorttitle = {{VNect}},
	url = {http://dl.acm.org/citation.cfm?doid=3072959.3073596},
	doi = {10.1145/3072959.3073596},
	language = {en},
	number = {4},
	urldate = {2019-04-30},
	journal = {ACM Transactions on Graphics},
	author = {Mehta, Dushyant and Sridhar, Srinath and Sotnychenko, Oleksandr and Rhodin, Helge and Shafiei, Mohammad and Seidel, Hans-Peter and Xu, Weipeng and Casas, Dan and Theobalt, Christian},
	month = jul,
	year = {2017},
	pages = {1--14},
	file = {Mehta et al. - 2017 - VNect real-time 3D human pose estimation with a s.pdf:/home/matthias/Zotero/storage/IEVSDEWU/Mehta et al. - 2017 - VNect real-time 3D human pose estimation with a s.pdf:application/pdf}
}

@article{zhou_monocap:_2019,
	title = {{MonoCap}: {Monocular} {Human} {Motion} {Capture} using a {CNN} {Coupled} with a {Geometric} {Prior}},
	volume = {41},
	issn = {0162-8828},
	shorttitle = {{MonoCap}},
	doi = {10.1109/TPAMI.2018.2816031},
	abstract = {Recovering 3D full-body human pose is a challenging problem with many applications. It has been successfully addressed by motion capture systems with body worn markers and multiple cameras. In this paper, we address the more challenging case of not only using a single camera but also not leveraging markers: going directly from 2D appearance to 3D geometry. Deep learning approaches have shown remarkable abilities to discriminatively learn 2D appearance features. The missing piece is how to integrate 2D, 3D, and temporal information to recover 3D geometry and account for the uncertainties arising from the discriminative model. We introduce a novel approach that treats 2D joint locations as latent variables whose uncertainty distributions are given by a deep fully convolutional neural network. The unknown 3D poses are modeled by a sparse representation and the 3D parameter estimates are realized via an Expectation-Maximization algorithm, where it is shown that the 2D joint location uncertainties can be conveniently marginalized out during inference. Extensive evaluation on benchmark datasets shows that the proposed approach achieves greater accuracy over state-of-the-art baselines. Notably, the proposed approach does not require synchronized 2D-3D data for training and is applicable to “in-the-wild” images, which is demonstrated with the MPII dataset.},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhou, X. and Zhu, M. and Pavlakos, G. and Leonardos, S. and Derpanis, K. G. and Daniilidis, K.},
	month = apr,
	year = {2019},
	keywords = {image motion analysis, image representation, pose estimation, Three-dimensional displays, Two dimensional displays, Cameras, 2D appearance features, 2D joint locations, 3D full-body human pose, 3D parameter estimates, body worn markers, cameras, CNN, convolutional neural nets, convolutional neural network, deep learning, discriminative model, expectation-maximisation algorithm, Expectation-Maximization algorithm, geometric prior, human pose, image capture, Image reconstruction, in-the-wild images, learning (artificial intelligence), MonoCap, monocular human motion capture, Motion capture, motion capture systems, MPII dataset, parameter estimation, Pose estimation, Solid modeling, sparse representation, stereo image processing, temporal information, Uncertainty},
	pages = {901--914},
	file = {IEEE Xplore Abstract Record:/home/matthias/Zotero/storage/SWFGZVHP/8316924.html:text/html;IEEE Xplore Full Text PDF:/home/matthias/Zotero/storage/GNT6ZSLJ/Zhou et al. - 2019 - MonoCap Monocular Human Motion Capture using a CN.pdf:application/pdf}
}

@inproceedings{kanazawa_end--end_2018,
	address = {Salt Lake City, UT},
	title = {End-to-{End} {Recovery} of {Human} {Shape} and {Pose}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578842/},
	doi = {10.1109/CVPR.2018.00744},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Kanazawa, Angjoo and Black, Michael J. and Jacobs, David W. and Malik, Jitendra},
	month = jun,
	year = {2018},
	pages = {7122--7131},
	file = {Kanazawa et al. - 2018 - End-to-End Recovery of Human Shape and Pose.pdf:/home/matthias/Zotero/storage/4BQSI3VH/Kanazawa et al. - 2018 - End-to-End Recovery of Human Shape and Pose.pdf:application/pdf}
}

@article{ionescu_human3.6m:_2014,
	title = {Human3.6M: {Large} {Scale} {Datasets} and {Predictive} {Methods} for 3D {Human} {Sensing} in {Natural} {Environments}},
	volume = {36},
	issn = {0162-8828},
	shorttitle = {Human3.6M},
	doi = {10.1109/TPAMI.2013.248},
	abstract = {We introduce a new dataset, Human3.6M, of 3.6 Million accurate 3D Human poses, acquired by recording the performance of 5 female and 6 male subjects, under 4 different viewpoints, for training realistic human sensing systems and for evaluating the next generation of human pose estimation models and algorithms. Besides increasing the size of the datasets in the current state-of-the-art by several orders of magnitude, we also aim to complement such datasets with a diverse set of motions and poses encountered as part of typical human activities (taking photos, talking on the phone, posing, greeting, eating, etc.), with additional synchronized image, human motion capture, and time of flight (depth) data, and with accurate 3D body scans of all the subject actors involved. We also provide controlled mixed reality evaluation scenarios where 3D human models are animated using motion capture and inserted using correct 3D geometry, in complex real environments, viewed with moving cameras, and under occlusion. Finally, we provide a set of large-scale statistical models and detailed evaluation baselines for the dataset illustrating its diversity and the scope for improvement by future work in the research community. Our experiments show that our best large-scale model can leverage our full training set to obtain a 20\% improvement in performance compared to a training set of the scale of the largest existing public dataset for this problem. Yet the potential for improvement by leveraging higher capacity, more complex models with our large dataset, is substantially vaster and should stimulate future research. The dataset together with code for the associated large-scale learning models, features, visualization tools, as well as the evaluation server, is available online at http://vision.imar.ro/human3.6m.},
	number = {7},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ionescu, C. and Papava, D. and Olaru, V. and Sminchisescu, C.},
	month = jul,
	year = {2014},
	keywords = {Estimation, image motion analysis, Joints, pose estimation, Humans, Training, Three-dimensional displays, Cameras, learning (artificial intelligence), Solid modeling, 3D geometry, 3D human models, 3D human pose estimation, 3D human poses, 3D human sensing, Algorithms, articulated body modeling, Biometry, data visualisation, Databases, Factual, Ecosystem, evaluation server, Fourier kernel approximations, human motion capture, human motion capture data, human pose estimation models, Human3.6M, Image Enhancement, Image Interpretation, Computer-Assisted, image sensors, Imaging, Three-Dimensional, Information Storage and Retrieval, large scale datasets, large-scale learning, large-scale learning models, large-scale statistical models, Modeling and recovery of physical attributes, Motion, motion capture, moving cameras, natural environments, optimization, Pattern Recognition, Automated, Photography, Posture, predictive methods, realistic human sensing systems, Reproducibility of Results, Sensitivity and Specificity, Sensors, statistical analysis, structured prediction, Subtraction Technique, synchronized image, visualization tools, Whole Body Imaging},
	pages = {1325--1339},
	file = {IEEE Xplore Abstract Record:/home/matthias/Zotero/storage/87CPZJ6S/6682899.html:text/html;IEEE Xplore Full Text PDF:/home/matthias/Zotero/storage/K5I2QP6G/Ionescu et al. - 2014 - Human3.6M Large Scale Datasets and Predictive Met.pdf:application/pdf}
}

@inproceedings{radwan_monocular_2013,
	address = {Sydney, Australia},
	title = {Monocular {Image} 3D {Human} {Pose} {Estimation} under {Self}-{Occlusion}},
	isbn = {978-1-4799-2840-8},
	url = {http://ieeexplore.ieee.org/document/6751345/},
	doi = {10.1109/ICCV.2013.237},
	abstract = {In this paper, an automatic approach for 3D pose reconstruction from a single image is proposed. The presence of human body articulation, hallucinated parts and cluttered background leads to ambiguity during the pose inference, which makes the problem non-trivial. Researchers have explored various methods based on motion and shading in order to reduce the ambiguity and reconstruct the 3D pose. The key idea of our algorithm is to impose both kinematic and orientation constraints. The former is imposed by projecting a 3D model onto the input image and pruning the parts, which are incompatible with the anthropomorphism. The latter is applied by creating synthetic views via regressing the input view to multiple oriented views. After applying the constraints, the 3D model is projected onto the initial and synthetic views, which further reduces the ambiguity. Finally, we borrow the direction of the unambiguous parts from the synthetic views to the initial one, which results in the 3D pose. Quantitative experiments are performed on the HumanEva-I dataset and qualitatively on unconstrained images from the Image Parse dataset. The results show the robustness of the proposed approach to accurately reconstruct the 3D pose form a single image.},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Radwan, Ibrahim and Dhall, Abhinav and Goecke, Roland},
	month = dec,
	year = {2013},
	pages = {1888--1895},
	file = {Radwan et al. - 2013 - Monocular Image 3D Human Pose Estimation under Sel.pdf:/home/matthias/Zotero/storage/UA8XYVPK/Radwan et al. - 2013 - Monocular Image 3D Human Pose Estimation under Sel.pdf:application/pdf}
}

@inproceedings{pavlakos_learning_2018,
	address = {Salt Lake City, UT},
	title = {Learning to {Estimate} 3D {Human} {Pose} and {Shape} from a {Single} {Color} {Image}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578153/},
	doi = {10.1109/CVPR.2018.00055},
	abstract = {This work addresses the problem of estimating the full body 3D human pose and shape from a single color image. This is a task where iterative optimization-based solutions have typically prevailed, while Convolutional Networks (ConvNets) have suffered because of the lack of training data and their low resolution 3D predictions. Our work aims to bridge this gap and proposes an efﬁcient and effective direct prediction method based on ConvNets. Central part to our approach is the incorporation of a parametric statistical body shape model (SMPL) within our end-to-end framework. This allows us to get very detailed 3D mesh results, while requiring estimation only of a small number of parameters, making it friendly for direct network prediction. Interestingly, we demonstrate that these parameters can be predicted reliably only from 2D keypoints and masks. These are typical outputs of generic 2D human analysis ConvNets, allowing us to relax the massive requirement that images with 3D shape ground truth are available for training. Simultaneously, by maintaining differentiability, at training time we generate the 3D mesh from the estimated parameters and optimize explicitly for the surface using a 3D per-vertex loss. Finally, a differentiable renderer is employed to project the 3D mesh to the image, which enables further reﬁnement of the network, by optimizing for the consistency of the projection with 2D annotations (i.e., 2D keypoints or masks). The proposed approach outperforms previous baselines on this task and offers an attractive solution for direct prediction of 3D shape from a single color image.},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Pavlakos, Georgios and Zhu, Luyang and Zhou, Xiaowei and Daniilidis, Kostas},
	month = jun,
	year = {2018},
	pages = {459--468},
	file = {Pavlakos et al. - 2018 - Learning to Estimate 3D Human Pose and Shape from .pdf:/home/matthias/Zotero/storage/AJCHUH6M/Pavlakos et al. - 2018 - Learning to Estimate 3D Human Pose and Shape from .pdf:application/pdf}
}

@inproceedings{luvizon_2d/3d_2018,
	address = {Salt Lake City, UT, USA},
	title = {2D/3D {Pose} {Estimation} and {Action} {Recognition} {Using} {Multitask} {Deep} {Learning}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578637/},
	doi = {10.1109/CVPR.2018.00539},
	abstract = {Action recognition and human pose estimation are closely related but both problems are generally handled as distinct tasks in the literature. In this work, we propose a multitask framework for jointly 2D and 3D pose estimation from still images and human action recognition from video sequences. We show that a single architecture can be used to solve the two problems in an efﬁcient way and still achieves state-of-the-art results. Additionally, we demonstrate that optimization from end-toend leads to signiﬁcantly higher accuracy than separated learning. The proposed architecture can be trained with data from different categories simultaneously in a seamlessly way. The reported results on four datasets (MPII, Human3.6M, Penn Action and NTU) demonstrate the effectiveness of our method on the targeted tasks.},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Luvizon, Diogo C. and Picard, David and Tabia, Hedi},
	month = jun,
	year = {2018},
	pages = {5137--5146},
	file = {Luvizon et al. - 2018 - 2D3D Pose Estimation and Action Recognition Using.pdf:/home/matthias/Zotero/storage/MPTJBEIM/Luvizon et al. - 2018 - 2D3D Pose Estimation and Action Recognition Using.pdf:application/pdf}
}

@inproceedings{kazemi_multi-view_2013,
	address = {Bristol},
	title = {Multi-view {Body} {Part} {Recognition} with {Random} {Forests}},
	isbn = {978-1-901725-49-0},
	url = {http://www.bmva.org/bmvc/2013/Papers/paper0048/index.html},
	doi = {10.5244/C.27.48},
	abstract = {This paper addresses the problem of human pose estimation, given images taken from multiple dynamic but calibrated cameras. We consider solving this task using a part-based model and focus on the part appearance component of such a model. We use a random forest classiﬁer to capture the variation in appearance of body parts in 2D images. The result of these 2D part detectors are then aggregated across views to produce consistent 3D hypotheses for parts. We solve correspondences across views for mirror symmetric parts by introducing a latent variable. We evaluate our part detectors qualitatively and quantitatively on a dataset gathered from a professional football game.},
	language = {en},
	urldate = {2019-05-01},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2013},
	publisher = {British Machine Vision Association},
	author = {Kazemi, Vahid and Burenius, Magnus and Azizpour, Hossein and Sullivan, Josephine},
	year = {2013},
	pages = {48.1--48.11},
	file = {Kazemi et al. - 2013 - Multi-view Body Part Recognition with Random Fores.pdf:/home/matthias/Zotero/storage/3X8GRJSN/Kazemi et al. - 2013 - Multi-view Body Part Recognition with Random Fores.pdf:application/pdf}
}

@article{rogez_lcr-net++:_2019,
	title = {{LCR}-{Net}++: {Multi}-person 2D and 3D {Pose} {Detection} in {Natural} {Images}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {{LCR}-{Net}++},
	url = {http://arxiv.org/abs/1803.00455},
	doi = {10.1109/TPAMI.2019.2892985},
	abstract = {We propose an end-to-end architecture for joint 2D and 3D human pose estimation in natural images. Key to our approach is the generation and scoring of a number of pose proposals per image, which allows us to predict 2D and 3D poses of multiple people simultaneously. Hence, our approach does not require an approximate localization of the humans for initialization. Our Localization-Classification-Regression architecture, named LCR-Net, contains 3 main components: 1) the pose proposal generator that suggests candidate poses at different locations in the image; 2) a classifier that scores the different pose proposals; and 3) a regressor that refines pose proposals both in 2D and 3D. All three stages share the convolutional feature layers and are trained jointly. The final pose estimation is obtained by integrating over neighboring pose hypotheses, which is shown to improve over a standard non maximum suppression algorithm. Our method recovers full-body 2D and 3D poses, hallucinating plausible body parts when the persons are partially occluded or truncated by the image boundary. Our approach significantly outperforms the state of the art in 3D pose estimation on Human3.6M, a controlled environment. Moreover, it shows promising results on real images for both single and multi-person subsets of the MPII 2D pose benchmark and demonstrates satisfying 3D pose results even for multi-person images.},
	urldate = {2019-05-01},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Rogez, Gregory and Weinzaepfel, Philippe and Schmid, Cordelia},
	year = {2019},
	note = {arXiv: 1803.00455},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1--1},
	annote = {Comment: journal version of the CVPR 2017 paper, accepted to appear in IEEE Trans. PAMI},
	file = {arXiv\:1803.00455 PDF:/home/matthias/Zotero/storage/AD5GCHCJ/Rogez et al. - 2019 - LCR-Net++ Multi-person 2D and 3D Pose Detection i.pdf:application/pdf;arXiv.org Snapshot:/home/matthias/Zotero/storage/VIHI6XL3/1803.html:text/html}
}

@inproceedings{yang_3d_2018,
	address = {Salt Lake City, UT, USA},
	title = {3D {Human} {Pose} {Estimation} in the {Wild} by {Adversarial} {Learning}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578649/},
	doi = {10.1109/CVPR.2018.00551},
	abstract = {Recently, remarkable advances have been achieved in 3D human pose estimation from monocular images because of the powerful Deep Convolutional Neural Networks (DCNNs). Despite their success on large-scale datasets collected in the constrained lab environment, it is difﬁcult to obtain the 3D pose annotations for in-the-wild images. Therefore, 3D human pose estimation in the wild is still a challenge. In this paper, we propose an adversarial learning framework, which distills the 3D human pose structures learned from the fully annotated dataset to in-the-wild images with only 2D pose annotations. Instead of deﬁning hard-coded rules to constrain the pose estimation results, we design a novel multi-source discriminator to distinguish the predicted 3D poses from the ground-truth, which helps to enforce the pose estimator to generate anthropometrically valid poses even with images in the wild. We also observe that a carefully designed information source for the discriminator is essential to boost the performance. Thus, we design a geometric descriptor, which computes the pairwise relative locations and distances between body joints, as a new information source for the discriminator. The efﬁcacy of our adversarial learning framework with the new geometric descriptor has been demonstrated through extensive experiments on widely used public benchmarks. Our approach signiﬁcantly improves the performance compared with previous state-of-the-art approaches.},
	language = {en},
	urldate = {2019-05-01},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Yang, Wei and Ouyang, Wanli and Wang, Xiaolong and Ren, Jimmy and Li, Hongsheng and Wang, Xiaogang},
	month = jun,
	year = {2018},
	pages = {5255--5264},
	file = {Yang et al. - 2018 - 3D Human Pose Estimation in the Wild by Adversaria.pdf:/home/matthias/Zotero/storage/TB4K635W/Yang et al. - 2018 - 3D Human Pose Estimation in the Wild by Adversaria.pdf:application/pdf}
}

@article{fang_learning_2017,
	title = {Learning {Pose} {Grammar} to {Encode} {Human} {Body} {Configuration} for 3D {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1710.06513},
	abstract = {In this paper, we propose a pose grammar to tackle the problem of 3D human pose estimation. Our model directly takes 2D pose as input and learns a generalized 2D-3D mapping function. The proposed model consists of a base network which efficiently captures pose-aligned features and a hierarchy of Bi-directional RNNs (BRNN) on the top to explicitly incorporate a set of knowledge regarding human body configuration (i.e., kinematics, symmetry, motor coordination). The proposed model thus enforces high-level constraints over human poses. In learning, we develop a pose sample simulator to augment training samples in virtual camera views, which further improves our model generalizability. We validate our method on public 3D human pose benchmarks and propose a new evaluation protocol working on cross-view setting to verify the generalization capability of different methods. We empirically observe that most state-of-the-art methods encounter difficulty under such setting while our method can well handle such challenges.},
	urldate = {2019-05-01},
	journal = {arXiv:1710.06513 [cs]},
	author = {Fang, Haoshu and Xu, Yuanlu and Wang, Wenguan and Liu, Xiaobai and Zhu, Song-Chun},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.06513},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted by AAAI 2018},
	file = {arXiv\:1710.06513 PDF:/home/matthias/Zotero/storage/6B323LJL/Fang et al. - 2017 - Learning Pose Grammar to Encode Human Body Configu.pdf:application/pdf;arXiv.org Snapshot:/home/matthias/Zotero/storage/YUY3KQHD/1710.html:text/html}
}

@inproceedings{zhou_towards_2017,
	address = {Venice},
	title = {Towards 3D {Human} {Pose} {Estimation} in the {Wild}: {A} {Weakly}-{Supervised} {Approach}},
	isbn = {978-1-5386-1032-9},
	shorttitle = {Towards 3D {Human} {Pose} {Estimation} in the {Wild}},
	url = {http://ieeexplore.ieee.org/document/8237313/},
	doi = {10.1109/ICCV.2017.51},
	abstract = {In this paper, we study the task of 3D human pose estimation in the wild. This task is challenging due to lack of training data, as existing datasets are either in the wild images with 2D pose or in the lab images with 3D pose.},
	language = {en},
	urldate = {2019-05-01},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhou, Xingyi and Huang, Qixing and Sun, Xiao and Xue, Xiangyang and Wei, Yichen},
	month = oct,
	year = {2017},
	pages = {398--407},
	file = {Zhou et al. - 2017 - Towards 3D Human Pose Estimation in the Wild A We.pdf:/home/matthias/Zotero/storage/VZHCF8W7/Zhou et al. - 2017 - Towards 3D Human Pose Estimation in the Wild A We.pdf:application/pdf}
}

@article{ronchi_its_2018,
	title = {It's all {Relative}: {Monocular} 3D {Human} {Pose} {Estimation} from {Weakly} {Supervised} {Data}},
	shorttitle = {It's all {Relative}},
	url = {http://arxiv.org/abs/1805.06880},
	abstract = {We address the problem of 3D human pose estimation from 2D input images using only weakly supervised training data. Despite showing considerable success for 2D pose estimation, the application of supervised machine learning to 3D pose estimation in real world images is currently hampered by the lack of varied training images with corresponding 3D poses. Most existing 3D pose estimation algorithms train on data that has either been collected in carefully controlled studio settings or has been generated synthetically. Instead, we take a different approach, and propose a 3D human pose estimation algorithm that only requires relative estimates of depth at training time. Such training signal, although noisy, can be easily collected from crowd annotators, and is of sufficient quality for enabling successful training and evaluation of 3D pose algorithms. Our results are competitive with fully supervised regression based approaches on the Human3.6M dataset, despite using significantly weaker training data. Our proposed algorithm opens the door to using existing widespread 2D datasets for 3D pose estimation by allowing fine-tuning with noisy relative constraints, resulting in more accurate 3D poses.},
	urldate = {2019-05-01},
	journal = {arXiv:1805.06880 [cs]},
	author = {Ronchi, Matteo Ruggero and Mac Aodha, Oisin and Eng, Robert and Perona, Pietro},
	month = may,
	year = {2018},
	note = {arXiv: 1805.06880},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: BMVC 2018. Project page available at http://www.vision.caltech.edu/{\textasciitilde}mronchi/projects/RelativePose},
	file = {arXiv\:1805.06880 PDF:/home/matthias/Zotero/storage/75XSYMMB/Ronchi et al. - 2018 - It's all Relative Monocular 3D Human Pose Estimat.pdf:application/pdf;arXiv.org Snapshot:/home/matthias/Zotero/storage/ZA2JHYGF/1805.html:text/html}
}

@article{feldhorst_human_2016,
	title = {Human {Activity} {Recognition} in der {Kommissionierung} – {Charakterisierung} des {Kommissionierprozesses} als {Ausgangsbasis} für die {Methodenentwicklung}},
	copyright = {fDPPL},
	url = {http://www.logistics-journal.de/proceedings/2016/fachkolloquium2016/4473},
	doi = {10.2195/lj_proc_feldhorst_de_201610_01},
	abstract = {While human activity recognition (HAR) has been successfully deployed to different application sce-narios in terms of every day or sport activities, it is rather new in the field of industrial order picking which is an important process in the field of logistics. Consequently, there are currently no dedicated datasets nor activity taxonomies available to build HAR solutions upon. To overcome this and to allow for the development of specialized HAR-approaches for order picking scenarios, the following contribution presents results of comprehensive field studies carried out in real world systems. During this studies measurements of inertial sensors as well as con-text information were gathered and the process was recorded on video for a later analysis.},
	language = {de},
	urldate = {2019-05-01},
	journal = {Volume 2016},
	author = {Feldhorst, Sascha and Aniol, Sandra and Ten Hompel, Michael},
	year = {2016},
	keywords = {620, Aktivitätserkennung, Datensätze, Feldstudien, HAR (Human Activity Recognition), Kommissionierung},
	pages = {Issue 10},
	annote = {SeriesInformation
Volume 2016, Issue 10},
	file = {Feldhorst et al. - 2016 - Human Activity Recognition in der Kommissionierung.pdf:/home/matthias/Zotero/storage/2VCGQRMY/Feldhorst et al. - 2016 - Human Activity Recognition in der Kommissionierung.pdf:application/pdf}
}

@inproceedings{pavlakos_coarse--fine_2017,
	address = {Honolulu, HI},
	title = {Coarse-to-{Fine} {Volumetric} {Prediction} for {Single}-{Image} 3D {Human} {Pose}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099622/},
	doi = {10.1109/CVPR.2017.139},
	abstract = {This paper addresses the challenge of 3D human pose estimation from a single color image. Despite the general success of the end-to-end learning paradigm, top performing approaches employ a two-step solution consisting of a Convolutional Network (ConvNet) for 2D joint localization and a subsequent optimization step to recover 3D pose. In this paper, we identify the representation of 3D pose as a critical issue with current ConvNet approaches and make two important contributions towards validating the value of end-to-end learning for this task. First, we propose a ﬁne discretization of the 3D space around the subject and train a ConvNet to predict per voxel likelihoods for each joint. This creates a natural representation for 3D pose and greatly improves performance over the direct regression of joint coordinates. Second, to further improve upon initial estimates, we employ a coarse-to-ﬁne prediction scheme. This step addresses the large dimensionality increase and enables iterative reﬁnement and repeated processing of the image features. The proposed approach outperforms all state-of-theart methods on standard benchmarks achieving a relative error reduction greater than 30\% on average. Additionally, we investigate using our volumetric representation in a related architecture which is suboptimal compared to our endto-end approach, but is of practical interest, since it enables training when no image with corresponding 3D groundtruth is available, and allows us to present compelling results for in-the-wild images.},
	language = {en},
	urldate = {2019-05-01},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Pavlakos, Georgios and Zhou, Xiaowei and Derpanis, Konstantinos G. and Daniilidis, Kostas},
	month = jul,
	year = {2017},
	pages = {1263--1272},
	file = {Pavlakos et al. - 2017 - Coarse-to-Fine Volumetric Prediction for Single-Im.pdf:/home/matthias/Zotero/storage/K5U5N76R/Pavlakos et al. - 2017 - Coarse-to-Fine Volumetric Prediction for Single-Im.pdf:application/pdf}
}

@inproceedings{pavllo_3d_2019,
	title = {3D human pose estimation in video with temporal convolutions and semi-supervised training},
	url = {http://arxiv.org/abs/1811.11742},
	abstract = {In this work, we demonstrate that 3D poses in video can be effectively estimated with a fully convolutional model based on dilated temporal convolutions over 2D keypoints. We also introduce back-projection, a simple and effective semi-supervised training method that leverages unlabeled video data. We start with predicted 2D keypoints for unlabeled video, then estimate 3D poses and finally back-project to the input 2D keypoints. In the supervised setting, our fully-convolutional model outperforms the previous best result from the literature by 6 mm mean per-joint position error on Human3.6M, corresponding to an error reduction of 11\%, and the model also shows significant improvements on HumanEva-I. Moreover, experiments with back-projection show that it comfortably outperforms previous state-of-the-art results in semi-supervised settings where labeled data is scarce. Code and models are available at https://github.com/facebookresearch/VideoPose3D},
	urldate = {2019-05-01},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Pavllo, Dario and Feichtenhofer, Christoph and Grangier, David and Auli, Michael},
	year = {2019},
	note = {arXiv: 1811.11742},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2019},
	file = {arXiv\:1811.11742 PDF:/home/matthias/Zotero/storage/XV86KFQZ/Pavllo et al. - 2018 - 3D human pose estimation in video with temporal co.pdf:application/pdf;arXiv.org Snapshot:/home/matthias/Zotero/storage/Y3MJI289/1811.html:text/html}
}

@incollection{ferrari_propagating_2018,
	address = {Cham},
	title = {Propagating {LSTM}: 3D {Pose} {Estimation} {Based} on {Joint} {Interdependency}},
	volume = {11211},
	isbn = {978-3-030-01233-5 978-3-030-01234-2},
	shorttitle = {Propagating {LSTM}},
	url = {http://link.springer.com/10.1007/978-3-030-01234-2_8},
	abstract = {We present a novel 3D pose estimation method based on joint interdependency (JI) for acquiring 3D joints from the human pose of an RGB image. The JI incorporates the body part based structural connectivity of joints to learn the high spatial correlation of human posture on our method. Towards this goal, we propose a new long short-term memory (LSTM)-based deep learning architecture named propagating LSTM networks (p-LSTMs), where each LSTM is connected sequentially to reconstruct 3D depth from the centroid to edge joints through learning the intrinsic JI. In the ﬁrst LSTM, the seed joints of 3D pose are created and reconstructed into the whole-body joints through the connected LSTMs. Utilizing the p-LSTMs, we achieve the higher accuracy of about 11.2\% than state-of-the-art methods on the largest publicly available database. Importantly, we demonstrate that the JI drastically reduces the structural errors at body edges, thereby leads to a signiﬁcant improvement.},
	language = {en},
	urldate = {2019-05-01},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Lee, Kyoungoh and Lee, Inwoong and Lee, Sanghoon},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01234-2_8},
	pages = {123--141},
	file = {Lee et al. - 2018 - Propagating LSTM 3D Pose Estimation Based on Join.pdf:/home/matthias/Zotero/storage/HDNNY4MB/Lee et al. - 2018 - Propagating LSTM 3D Pose Estimation Based on Join.pdf:application/pdf}
}

@inproceedings{lin_recurrent_2017,
	title = {Recurrent 3D {Pose} {Sequence} {Machines}},
	abstract = {3D human articulated pose recovery from monocular image sequences is very challenging due to the diverse appearances, viewpoints, occlusions, and also the human 3D pose is inherently ambiguous from the monocular imagery. It is thus critical to exploit rich spatial and temporal long-range dependencies among body joints for accurate 3D pose sequence prediction. Existing approaches usually manually design some elaborate prior terms and human body kinematic constraints for capturing structures, which are often insufficient to exploit all intrinsic structures and not scalable for all scenarios. In contrast, this paper presents a Recurrent 3D Pose Sequence Machine(RPSM) to automatically learn the image-dependent structural constraint and sequence-dependent temporal context by using a multi-stage sequential refinement. At each stage, our RPSM is composed of three modules to predict the 3D pose sequences based on the previously learned 2D pose representations and 3D poses: (i) a 2D pose module extracting the image-dependent pose representations, (ii) a 3D pose recurrent module regressing 3D poses and (iii) a feature adaption module serving as a bridge between module (i) and (ii) to enable the representation transformation from 2D to 3D domain. These three modules are then assembled into a sequential prediction framework to refine the predicted poses with multiple recurrent stages. Extensive evaluations on the Human3.6M dataset and HumanEva-I dataset show that our RPSM outperforms all state-of-the-art approaches for 3D pose estimation.},
	language = {en},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Lin, Mude and Lin, Liang and Liang, Xiaodan and Wang, Keze and Cheng, Hui},
	year = {2017},
	file = {Lin et al. - Recurrent 3D Pose Sequence Machines.pdf:/home/matthias/Zotero/storage/KJCW8AIZ/Lin et al. - Recurrent 3D Pose Sequence Machines.pdf:application/pdf}
}

@inproceedings{hossain_exploiting_2018,
	title = {Exploiting temporal information for 3D pose estimation},
	volume = {11214},
	url = {http://arxiv.org/abs/1711.08585},
	doi = {10.1007/978-3-030-01249-6_5},
	abstract = {In this work, we address the problem of 3D human pose estimation from a sequence of 2D human poses. Although the recent success of deep networks has led many state-of-the-art methods for 3D pose estimation to train deep networks end-to-end to predict from images directly, the top-performing approaches have shown the effectiveness of dividing the task of 3D pose estimation into two steps: using a state-of-the-art 2D pose estimator to estimate the 2D pose from images and then mapping them into 3D space. They also showed that a low-dimensional representation like 2D locations of a set of joints can be discriminative enough to estimate 3D pose with high accuracy. However, estimation of 3D pose for individual frames leads to temporally incoherent estimates due to independent error in each frame causing jitter. Therefore, in this work we utilize the temporal information across a sequence of 2D joint locations to estimate a sequence of 3D poses. We designed a sequence-to-sequence network composed of layer-normalized LSTM units with shortcut connections connecting the input to the output on the decoder side and imposed temporal smoothness constraint during training. We found that the knowledge of temporal consistency improves the best reported result on Human3.6M dataset by approximately \$12.2{\textbackslash}\%\$ and helps our network to recover temporally consistent 3D poses over a sequence of images even when the 2D pose detector fails.},
	urldate = {2019-05-01},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Hossain, Mir Rayat Imtiaz and Little, James J.},
	year = {2018},
	note = {arXiv: 1711.08585},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {69--86},
	file = {arXiv\:1711.08585 PDF:/home/matthias/Zotero/storage/XZF2LE7S/Hossain and Little - 2018 - Exploiting temporal information for 3D pose estima.pdf:application/pdf;arXiv.org Snapshot:/home/matthias/Zotero/storage/6RZKREM6/1711.html:text/html}
}

@inproceedings{chen_cascaded_2018,
	address = {Salt Lake City, UT},
	title = {Cascaded {Pyramid} {Network} for {Multi}-person {Pose} {Estimation}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578840/},
	doi = {10.1109/CVPR.2018.00742},
	abstract = {The topic of multi-person pose estimation has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as occluded keypoints, invisible keypoints and complex background, which cannot be well addressed. In this paper, we present a novel network structure called Cascaded Pyramid Network (CPN) which targets to relieve the problem from these “hard” keypoints. More speciﬁcally, our algorithm includes two stages: GlobalNet and ReﬁneNet. GlobalNet is a feature pyramid network which can successfully localize the “simple” keypoints like eyes and hands but may fail to precisely recognize the occluded or invisible keypoints. Our ReﬁneNet tries explicitly handling the “hard” keypoints by integrating all levels of feature representations from the GlobalNet together with an online hard keypoint mining loss. In general, to address the multi-person pose estimation problem, a top-down pipeline is adopted to ﬁrst generate a set of human bounding boxes based on a detector, followed by our CPN for keypoint localization in each human bounding box. Based on the proposed algorithm, we achieve state-ofart results on the COCO keypoint benchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1 on the COCO test-challenge dataset, which is a 19\% relative improvement compared with 60.5 from the COCO 2016 keypoint challenge. Code1 and the detection results for person used will be publicly available for further research.},
	language = {en},
	urldate = {2019-05-01},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Chen, Yilun and Wang, Zhicheng and Peng, Yuxiang and Zhang, Zhiqiang and Yu, Gang and Sun, Jian},
	month = jun,
	year = {2018},
	pages = {7103--7112},
	file = {Chen et al. - 2018 - Cascaded Pyramid Network for Multi-person Pose Est.pdf:/home/matthias/Zotero/storage/WH96P26J/Chen et al. - 2018 - Cascaded Pyramid Network for Multi-person Pose Est.pdf:application/pdf}
}

@inproceedings{he_mask_2017,
	title = {Mask {R}-{CNN}},
	doi = {10.1109/ICCV.2017.322},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {He, K. and Gkioxari, G. and Dollár, P. and Girshick, R.},
	year = {2017},
	keywords = {bounding-box object detection, called Mask R-CNN, conceptually simple framework, Faster R-CNN, feature extraction, Feature extraction, high-quality segmentation mask, image segmentation, Image segmentation, object detection, Object detection, object instance segmentation, object mask, pose estimation, Quantization (signal), Robustness, Semantics},
	pages = {2980--2988},
	file = {He et al. - 2017 - Mask R-CNN.pdf:/home/matthias/Zotero/storage/CSVAFRTR/He et al. - 2017 - Mask R-CNN.pdf:application/pdf}
}

@inproceedings{grzeszick_deep_2017,
	address = {Rostock, Germany},
	title = {Deep {Neural} {Network} based {Human} {Activity} {Recognition} for the {Order} {Picking} {Process}},
	isbn = {978-1-4503-5223-9},
	url = {http://dl.acm.org/citation.cfm?doid=3134230.3134231},
	doi = {10.1145/3134230.3134231},
	abstract = {Although the fourth industrial revolution is already in pro-gress and advances have been made in automating factories, completely automated facilities are still far in the future. Human work is still an important factor in many factories and warehouses, especially in the ﬁeld of logistics. Manual processes are, therefore, often subject to optimization efforts. In order to aid these optimization efforts, methods like human activity recognition (HAR) became of increasing interest in industrial settings. In this work a novel deep neural network architecture for HAR is introduced. A convolutional neural network (CNN), which employs temporal convolutions, is applied to the sequential data of multiple intertial measurement units (IMUs). The network is designed to separately handle different sensor values and IMUs, joining the information step-by-step within the architecture. An evaluation is performed using data from the order picking process recorded in two different warehouses. The inﬂuence of different design choices in the network architecture, as well as pre- and post-processing, will be evaluated. Crucial steps for learning a good classiﬁcation network for the task of HAR in a complex industrial setting will be shown. Ultimately, it can be shown that traditional approaches based on statistical features as well as recent CNN architectures are outperformed.},
	language = {en},
	urldate = {2019-05-03},
	booktitle = {Proceedings of the 4th international {Workshop} on {Sensor}-based {Activity} {Recognition} and {Interaction}  - {iWOAR} '17},
	publisher = {ACM Press},
	author = {Grzeszick, Rene and Lenk, Jan Marius and Rueda, Fernando Moya and Fink, Gernot A. and Feldhorst, Sascha and ten Hompel, Michael},
	year = {2017},
	pages = {1--6},
	file = {Grzeszick et al. - 2017 - Deep Neural Network based Human Activity Recogniti.pdf:/home/matthias/Zotero/storage/CRZV79NB/Grzeszick et al. - 2017 - Deep Neural Network based Human Activity Recogniti.pdf:application/pdf}
}

@article{ordonez_deep_2016,
	title = {Deep {Convolutional} and {LSTM} {Recurrent} {Neural} {Networks} for {Multimodal} {Wearable} {Activity} {Recognition}},
	volume = {16},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/16/1/115},
	doi = {10.3390/s16010115},
	abstract = {Human activity recognition (HAR) tasks have traditionally been solved using engineered features obtained by heuristic processes. Current research suggests that deep convolutional neural networks are suited to automate feature extraction from raw sensor inputs. However, human activities are made of complex sequences of motor movements, and capturing this temporal dynamics is fundamental for successful HAR. Based on the recent success of recurrent neural networks for time series domains, we propose a generic deep framework for activity recognition based on convolutional and LSTM recurrent units, which: (i) is suitable for multimodal wearable sensors; (ii) can perform sensor fusion naturally; (iii) does not require expert knowledge in designing features; and (iv) explicitly models the temporal dynamics of feature activations. We evaluate our framework on two datasets, one of which has been used in a public activity recognition challenge. Our results show that our framework outperforms competing deep non-recurrent networks on the challenge dataset by 4\% on average; outperforming some of the previous reported results by up to 9\%. Our results show that the framework can be applied to homogeneous sensor modalities, but can also fuse multimodal sensors to improve performance. We characterise key architectural hyperparameters’ influence on performance to provide insights about their optimisation.},
	language = {en},
	number = {1},
	urldate = {2019-05-03},
	journal = {Sensors},
	author = {Ordóñez, Francisco Javier and Roggen, Daniel},
	month = jan,
	year = {2016},
	keywords = {deep learning, human activity recognition, LSTM, machine learning, neural network, sensor fusion, wearable sensors},
	pages = {115},
	file = {Full Text PDF:/home/matthias/Zotero/storage/977BV59L/Ordóñez and Roggen - 2016 - Deep Convolutional and LSTM Recurrent Neural Netwo.pdf:application/pdf;Snapshot:/home/matthias/Zotero/storage/CSZEEVZF/htm.html:text/html}
}

@article{hammerla_deep_2016,
	title = {Deep, {Convolutional}, and {Recurrent} {Models} for {Human} {Activity} {Recognition} using {Wearables}},
	url = {http://arxiv.org/abs/1604.08880},
	abstract = {Human activity recognition (HAR) in ubiquitous computing is beginning to adopt deep learning to substitute for well-established analysis techniques that rely on hand-crafted feature extraction and classification techniques. From these isolated applications of custom deep architectures it is, however, difficult to gain an overview of their suitability for problems ranging from the recognition of manipulative gestures to the segmentation and identification of physical activities like running or ascending stairs. In this paper we rigorously explore deep, convolutional, and recurrent approaches across three representative datasets that contain movement data captured with wearable sensors. We describe how to train recurrent approaches in this setting, introduce a novel regularisation approach, and illustrate how they outperform the state-of-the-art on a large benchmark dataset. Across thousands of recognition experiments with randomly sampled model configurations we investigate the suitability of each model for different tasks in HAR, explore the impact of hyperparameters using the fANOVA framework, and provide guidelines for the practitioner who wants to apply deep learning in their problem setting.},
	urldate = {2019-05-03},
	journal = {arXiv:1604.08880 [cs, stat]},
	author = {Hammerla, Nils Y. and Halloran, Shane and Ploetz, Thomas},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.08880},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Extended version has been accepted for publication at International Joint Conference on Artificial Intelligence (IJCAI)},
	file = {arXiv\:1604.08880 PDF:/home/matthias/Zotero/storage/5R74EMIW/Hammerla et al. - 2016 - Deep, Convolutional, and Recurrent Models for Huma.pdf:application/pdf;arXiv.org Snapshot:/home/matthias/Zotero/storage/GEWLUX24/1604.html:text/html}
}

@inproceedings{rueda_learning_2018,
	title = {Learning {Attribute} {Representation} for {Human} {Activity} {Recognition}},
	doi = {10.1109/ICPR.2018.8545146},
	abstract = {Attribute representations became relevant in image recognition and word spotting, providing support under the presence of unbalance and disjoint datasets. However, for human activity recognition using sequential data from on-body sensors, human-labeled attributes are lacking. This paper introduces a search for attributes that represent favorably signal segments for recognizing human activities. It presents three deep architectures, including temporal-convolutions and an IMU centered design, for predicting attributes. An empiric evaluation of random and learned attribute representations, and as well as the networks is carried out on two datasets, outperforming the state-of-the art.},
	booktitle = {2018 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Rueda, F. M. and Fink, G. A.},
	month = aug,
	year = {2018},
	keywords = {Activity recognition, attribute representation, Computer architecture, disjoint datasets, Feature extraction, Foot, human activity recognition, human-labeled attributes, image motion analysis, image recognition, image representation, image segmentation, image sequences, IMU centered design, learning (artificial intelligence), on-body sensors, signal segments, Task analysis, temporal-convolutions, Time series analysis, word spotting},
	pages = {523--528},
	file = {IEEE Xplore Abstract Record:/home/matthias/Zotero/storage/VC4858KI/8545146.html:text/html;IEEE Xplore Full Text PDF:/home/matthias/Zotero/storage/JSKBFP8A/Rueda and Fink - 2018 - Learning Attribute Representation for Human Activi.pdf:application/pdf}
}