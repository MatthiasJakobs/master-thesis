
@inproceedings{yang_articulated_2011,
	address = {Colorado Springs, CO},
	title = {Articulated pose estimation with flexible mixtures-of-parts},
	abstract = {We describe a method for human pose estimation in static images based on a novel representation of part models. Notably, we do not use articulated limb parts, but rather capture orientation with a mixture of templates for each part. We describe a general, flexible mixture model for capturing contextual co-occurrence relations between parts, augmenting standard spring models that encode spatial relations. We show that such relations can capture notions of local rigidity. When co-occurrence and spatial relations are tree-structured, our model can be efficiently optimized with dynamic programming. We present experimental results on standard benchmarks for pose estimation that indicate our approach is the state-of-the-art system for pose estimation, outperforming past work by 50\% while being orders of magnitude faster.},
	booktitle = {Proceedings of the 2011 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Yang, Yi and Ramanan, Deva},
	month = jun,
	year = {2011},
	keywords = {articulated limb parts, Computational modeling, contextual co-occurrence relation, Deformable models, dynamic programming, Estimation, flexible mixture model, flexible mixture of parts, human pose estimation, Humans, image coding, image representation, Joints, pose estimation, spatial relations, Springs, standard spring models, static images, Training, tree structure, trees (mathematics)},
	pages = {1385--1392},
	file = {Yang and Ramanan - 2011 - Articulated pose estimation with flexible mixtures.pdf:/Users/Matthias/Zotero/storage/LHPQS7SY/Yang and Ramanan - 2011 - Articulated pose estimation with flexible mixtures.pdf:application/pdf}
}

@inproceedings{wang_approach_2013,
	address = {Portland},
	title = {An {Approach} to {Pose}-{Based} {Action} {Recognition}},
	doi = {10.1109/CVPR.2013.123},
	abstract = {We address action recognition in videos by modeling the spatial-temporal structures of human poses. We start by improving a state of the art method for estimating human joint locations from videos. More precisely, we obtain the K-best estimations output by the existing method and incorporate additional segmentation cues and temporal constraints to select the “best” one. Then we group the estimated joints into five body parts (e.g. the left arm) and apply data mining techniques to obtain a representation for the spatial-temporal structures of human actions. This representation captures the spatial configurations of body parts in one frame (by spatial-part-sets) as well as the body part movements(by temporal-part-sets) which are characteristic of human actions. It is interpretable, compact, and also robust to errors on joint estimations. Experimental results first show that our approach is able to localize body joints more accurately than existing methods. Next we show that it outperforms state of the art action recognizers on the UCF sport, the Keck Gesture and the MSR-Action3D datasets.},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Wang, Chunyu and Wang, Yizhou and Yuille, Alan L.},
	year = {2013},
	keywords = {Estimation, image representation, Joints, pose estimation, feature learning, action recognition, body part movements, body parts spatial configurations, data mining, Data mining, Dictionaries, gesture recognition, Histograms, human actions, human joint locations estimation, human poses, Image color analysis, image motion analysis, image segmentation, Itemsets, joint estimations, K-best estimations, Keck gesture, MSR-action3D datasets, pose-based action recognition, segmentation cues, spatial-part-sets, spatial-temporal structures modeling, spatial-temporal structures representation, temporal constraints, temporal-part-sets, UCF sport, video signal processing, videos},
	pages = {915--922},
	file = {Wang et al. - 2013 - An Approach to Pose-Based Action Recognition.pdf:/Users/Matthias/Zotero/storage/ERSPKM4H/ERSPKM4H.pdf:application/pdf}
}

@article{chou_self_2017,
	title = {Self {Adversarial} {Training} for {Human} {Pose} {Estimation}},
	url = {https://arxiv.org/abs/1707.02439v2},
	abstract = {This paper presents a deep learning based approach to the problem of human
pose estimation. We employ generative adversarial networks as our learning
paradigm in which we set up two stacked hourglass networks with the same
architecture, one as the generator and the other as the discriminator. The
generator is used as a human pose estimator after the training is done. The
discriminator distinguishes ground-truth heatmaps from generated ones, and
back-propagates the adversarial loss to the generator. This process enables the
generator to learn plausible human body configurations and is shown to be
useful for improving the prediction accuracy.},
	language = {en},
	urldate = {2019-04-20},
	author = {Chou, Chia-Jung and Chien, Jui-Ting and Chen, Hwann-Tzong},
	month = jul,
	year = {2017},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/VSZPQT5E/VSZPQT5E.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/33W6P3BH/Chou et al. - 2017 - Self Adversarial Training for Human Pose Estimatio.html:text/html}
}

@inproceedings{reining_towards_2018,
	address = {Poznań, PL},
	title = {Towards a {Framework} for {Semi}-{Automated} {Annotation} of {Human} {Order} {Picking} {Activities} {Using} {Motion} {Capturing}},
	abstract = {Data creation for Human Activity Recognition (HAR) requires an immense human effort and contextual knowledge for manual annotation. This paper proposes a framework for semi-automated annotation of sequential data in the order picking process using a motion capturing system. Additionally, it introduces proper annotation labels by defining process steps, human activities and simple human movements in order picking scenarios. An attribute representation based on simple human movements meets the challenges set by the versatility of activities in warehousing.},
	booktitle = {Proceedings of the 2018 {Federated} {Conference} on {Computer} {Science} and {Information} {Systems} ({FedCSIS})},
	author = {Reining, Christopher and Moya Rueda, Fernando and ten Hompel, Michael and Fink, Gernot A.},
	month = sep,
	year = {2018},
	keywords = {annotation labels, attribute representation, Computer architecture, contextual knowledge, data creation, human activities, human activity recognition, human movements, human order picking activities, image motion analysis, image recognition, image representation, immense human effort, Legged locomotion, manual annotation, Manuals, motion capturing system, order picking, production engineering computing, semiautomated annotation, sequential data, Skeleton, Task analysis, Videos, warehousing, Warehousing},
	pages = {817--821},
	file = {Reining et al. - 2018 - Towards a Framework for Semi-Automated Annotation .pdf:/Users/Matthias/Zotero/storage/MAEVS2GC/Reining et al. - 2018 - Towards a Framework for Semi-Automated Annotation .pdf:application/pdf}
}

@inproceedings{insafutdinov_deepercut:_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{DeeperCut}: {A} {Deeper}, {Stronger}, and {Faster} {Multi}-person {Pose} {Estimation} {Model}},
	isbn = {978-3-319-46466-4},
	shorttitle = {{DeeperCut}},
	abstract = {The goal of this paper is to advance the state-of-the-art of articulated pose estimation in scenes with multiple people. To that end we contribute on three fronts. We propose (1) improved body part detectors that generate effective bottom-up proposals for body parts; (2) novel image-conditioned pairwise terms that allow to assemble the proposals into a variable number of consistent body part configurations; and (3) an incremental optimization strategy that explores the search space more efficiently thus leading both to better performance and significant speed-up factors. Evaluation is done on two single-person and two multi-person pose estimation benchmarks. The proposed approach significantly outperforms best known multi-person pose estimation results while demonstrating competitive performance on the task of single person pose estimation (Models and code available at http://pose.mpi-inf.mpg.de).},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	publisher = {Springer International Publishing},
	author = {Insafutdinov, Eldar and Pishchulin, Leonid and Andres, Bjoern and Andriluka, Mykhaylo and Schiele, Bernt},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	keywords = {Area Under Curve, Body Part, Conv4 Bank, Integer Linear Programming, Part Detector},
	pages = {34--50},
	file = {Insafutdinov et al. - 2016 - DeeperCut A Deeper, Stronger, and Faster Multi-pe.pdf:/Users/Matthias/Zotero/storage/Q4K7LMFG/Q4K7LMFG.pdf:application/pdf}
}

@inproceedings{andriluka_posetrack:_2018,
	address = {Salt Lake City, UT, USA},
	title = {{PoseTrack}: {A} {Benchmark} for {Human} {Pose} {Estimation} and {Tracking}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{PoseTrack}},
	url = {https://ieeexplore.ieee.org/document/8578640/},
	doi = {10.1109/CVPR.2018.00542},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Andriluka, Mykhaylo and Iqbal, Umar and Insafutdinov, Eldar and Pishchulin, Leonid and Milan, Anton and Gall, Juergen and Schiele, Bernt},
	month = jun,
	year = {2018},
	pages = {5167--5176},
	file = {Andriluka et al. - 2018 - PoseTrack A Benchmark for Human Pose Estimation a.pdf:/Users/Matthias/Zotero/storage/B9MSZF5G/B9MSZF5G.pdf:application/pdf}
}

@inproceedings{cao_realtime_2017,
	address = {Honolulu, HI},
	title = {Realtime {Multi}-person 2D {Pose} {Estimation} {Using} {Part} {Affinity} {Fields}},
	url = {http://ieeexplore.ieee.org/document/8099626/},
	doi = {10.1109/CVPR.2017.143},
	abstract = {We present an approach to efﬁciently detect the 2D pose of multiple people in an image. The approach uses a nonparametric representation, which we refer to as Part Afﬁnity Fields (PAFs), to learn to associate body parts with individuals in the image. The architecture encodes global context, allowing a greedy bottom-up parsing step that maintains high accuracy while achieving realtime performance, irrespective of the number of people in the image. The architecture is designed to jointly learn part locations and their association via two branches of the same sequential prediction process. Our method placed ﬁrst in the inaugural COCO 2016 keypoints challenge, and signiﬁcantly exceeds the previous state-of-the-art result on the MPII MultiPerson benchmark, both in performance and efﬁciency.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {Proceedings of the 2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Cao, Zhe and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
	month = jul,
	year = {2017},
	pages = {1302--1310},
	file = {Cao et al. - 2017 - Realtime Multi-person 2D Pose Estimation Using Par.pdf:/Users/Matthias/Zotero/storage/XP9UTUIT/Cao et al. - 2017 - Realtime Multi-person 2D Pose Estimation Using Par.pdf:application/pdf}
}

@incollection{simonyan_two-stream_2014,
	title = {Two-{Stream} {Convolutional} {Networks} for {Action} {Recognition} in {Videos}},
	urldate = {2019-04-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2014},
	pages = {568--576},
	file = {NIPS Full Text PDF:/Users/Matthias/Zotero/storage/YQZ9UN6X/Simonyan and Zisserman - 2014 - Two-Stream Convolutional Networks for Action Recog.pdf:application/pdf;NIPS Snapshot:/Users/Matthias/Zotero/storage/BVM68IDS/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.html:text/html}
}

@inproceedings{khalid_multi-modal_2018,
	address = {Beijing, CHN},
	title = {Multi-{Modal} {Three}-{Stream} {Network} for {Action} {Recognition}},
	abstract = {Human action recognition in video is an active yet challenging research topic due to high variation and complexity of data. In this paper, a novel video based action recognition framework utilizing complementary cues is proposed to handle this complex problem. Inspired by the successful two stream networks for action classification, additional pose features are studied and fused to enhance understanding of human action in a more abstract and semantic way. Towards practices, not only ground truth poses but also noisy estimated poses are incorporated in the framework with our proposed pre-processing module. The whole framework and each cue are evaluated on varied benchmarking datasets as JHMDB, sub-JHMDB and Penn Action. Our results outperform state-of-the-art performance on these datasets and show the strength of complementary cues.},
	booktitle = {Proceedings of the 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Khalid, Muhammad U. and Yu, Jie},
	month = aug,
	year = {2018},
	keywords = {action classification, active yet challenging research topic, complementary cues, complex problem, feature extraction, Feature extraction, ground truth poses, human action recognition, image classification, image motion analysis, image recognition, image sequences, Interpolation, multimodal three-stream network, noisy estimated poses, object detection, Pattern recognition, Penn Action, pose estimation, successful two stream networks, Tensile stress, Three-dimensional displays, Training, Two dimensional displays, video based action recognition framework, video signal processing},
	pages = {3210--3215},
	file = {IEEE Xplore Abstract Record:/Users/Matthias/Zotero/storage/4L4TMCIJ/8546131.html:text/html;IEEE Xplore Full Text PDF:/Users/Matthias/Zotero/storage/TJG5VBLH/Khalid and Yu - 2018 - Multi-Modal Three-Stream Network for Action Recogn.pdf:application/pdf}
}

@inproceedings{guler_densepose:_2018,
	address = {Salt Lake City, UT, USA},
	title = {{DensePose}: {Dense} {Human} {Pose} {Estimation} in the {Wild}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{DensePose}},
	url = {https://ieeexplore.ieee.org/document/8578860/},
	doi = {10.1109/CVPR.2018.00762},
	abstract = {In this work we establish dense correspondences between an RGB image and a surface-based representation of the human body, a task we refer to as dense human pose estimation. We gather dense correspondences for 50K persons appearing in the COCO dataset by introducing an efﬁcient annotation pipeline. We then use our dataset to train CNN-based systems that deliver dense correspondence ‘in the wild’, namely in the presence of background, occlusions and scale variations. We improve our training set’s effectiveness by training an inpainting network that can ﬁll in missing ground truth values and report improvements with respect to the best results that would be achievable in the past. We experiment with fully-convolutional networks and region-based models and observe a superiority of the latter. We further improve accuracy through cascading, obtaining a system that delivers highly-accurate results at multiple frames per second on a single gpu. Supplementary materials, data, code, and videos are provided on the project page http://densepose.org.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Guler, Riza Alp and Neverova, Natalia and Kokkinos, Iasonas},
	month = jun,
	year = {2018},
	pages = {7297--7306},
	file = {Guler et al. - 2018 - DensePose Dense Human Pose Estimation in the Wild.pdf:/Users/Matthias/Zotero/storage/H4N36ZJ9/Guler et al. - 2018 - DensePose Dense Human Pose Estimation in the Wild.pdf:application/pdf}
}

@inproceedings{martinez_simple_2017,
	address = {Venice},
	title = {A {Simple} {Yet} {Effective} {Baseline} for 3d {Human} {Pose} {Estimation}},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237550/},
	doi = {10.1109/ICCV.2017.288},
	abstract = {Following the success of deep convolutional networks, state-of-the-art methods for 3d human pose estimation have focused on deep end-to-end systems that predict 3d joint locations given raw image pixels. Despite their excellent performance, it is often not easy to understand whether their remaining error stems from a limited 2d pose (visual) understanding, or from a failure to map 2d poses into 3dimensional positions.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Martinez, Julieta and Hossain, Rayat and Romero, Javier and Little, James J.},
	year = {2017},
	pages = {2659--2668},
	file = {Martinez et al. - 2017 - A Simple Yet Effective Baseline for 3d Human Pose .pdf:/Users/Matthias/Zotero/storage/Z2TTE5AP/Martinez et al. - 2017 - A Simple Yet Effective Baseline for 3d Human Pose .pdf:application/pdf}
}

@inproceedings{newell_stacked_2016,
	address = {Amsterdam, NL},
	title = {Stacked {Hourglass} {Networks} for {Human} {Pose} {Estimation}},
	abstract = {This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a “stacked hourglass” network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.},
	language = {en},
	booktitle = {Proceedings of the 14th {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Newell, Alejandro and Yang, Kaiyu and Deng, Jia},
	month = oct,
	year = {2016},
	keywords = {Human pose estimation, occlusion},
	pages = {483--499},
	file = {Newell et al. - 2016 - Stacked Hourglass Networks for Human Pose Estimati.pdf:/Users/Matthias/Zotero/storage/3UCQ7PCS/Newell et al. - 2016 - Stacked Hourglass Networks for Human Pose Estimati.pdf:application/pdf}
}

@inproceedings{wei_convolutional_2016,
	address = {Las Vegas. NV},
	title = {Convolutional {Pose} {Machines}},
	abstract = {Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly reﬁned estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difﬁculty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {Proceedings of the 2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Wei, Shih-En and Ramakrishna, Varun and Kanade, Takeo and Sheikh, Yaser},
	month = jun,
	year = {2016},
	pages = {4724--4732},
	file = {Wei et al. - 2016 - Convolutional Pose Machines.pdf:/Users/Matthias/Zotero/storage/YCBLGD9Z/Wei et al. - 2016 - Convolutional Pose Machines.pdf:application/pdf}
}

@inproceedings{gong_look_2017,
	address = {Honolulu, HI},
	title = {Look into {Person}: {Self}-{Supervised} {Structure}-{Sensitive} {Learning} and a {New} {Benchmark} for {Human} {Parsing}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {Look into {Person}},
	url = {http://ieeexplore.ieee.org/document/8100198/},
	doi = {10.1109/CVPR.2017.715},
	abstract = {Human parsing has recently attracted a lot of research interests due to its huge application potentials. However existing datasets have limited number of images and annotations, and lack the variety of human appearances and the coverage of challenging cases in unconstrained environment. In this paper, we introduce a new benchmark1 “Look into Person (LIP)” that makes a signiﬁcant advance in terms of scalability, diversity and difﬁculty, a contribution that we feel is crucial for future developments in humancentric analysis. This comprehensive dataset contains over 50,000 elaborately annotated images with 19 semantic part labels, which are captured from a wider range of viewpoints, occlusions and background complexity. Given these rich annotations we perform detailed analyses of the leading human parsing approaches, gaining insights into the success and failures of these methods. Furthermore, in contrast to the existing efforts on improving the feature discriminative capability, we solve human parsing by exploring a novel self-supervised structure-sensitive learning approach, which imposes human pose structures into parsing results without resorting to extra supervision (i.e., no need for speciﬁcally labeling human joints in model training). Our self-supervised learning framework can be injected into any advanced neural networks to help incorporate rich high-level knowledge regarding human joints from a global perspective and improve the parsing results. Extensive evaluations on our LIP and the public PASCAL-PersonPart dataset demonstrate the superiority of our method.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Gong, Ke and Liang, Xiaodan and Zhang, Dongyu and Shen, Xiaohui and Lin, Liang},
	month = jul,
	year = {2017},
	pages = {6757--6765},
	file = {Gong et al. - 2017 - Look into Person Self-Supervised Structure-Sensit.pdf:/Users/Matthias/Zotero/storage/H6S978WI/Gong et al. - 2017 - Look into Person Self-Supervised Structure-Sensit.pdf:application/pdf}
}

@inproceedings{toshev_deeppose:_2014,
	address = {Columbus, OH},
	title = {{DeepPose}: {Human} {Pose} {Estimation} via {Deep} {Neural} {Networks}},
	abstract = {We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regressors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formulation which capitalizes on recent advances in Deep Learning. We present a detailed empirical analysis with state-ofart or better performance on four academic benchmarks of diverse real-world images.},
	language = {en},
	urldate = {2019-04-23},
	booktitle = {Proceedings of the 2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Toshev, Alexander and Szegedy, Christian},
	month = jun,
	year = {2014},
	pages = {1653--1660},
	file = {Toshev and Szegedy - 2014 - DeepPose Human Pose Estimation via Deep Neural Ne.pdf:/Users/Matthias/Zotero/storage/Q36UJJG7/Toshev and Szegedy - 2014 - DeepPose Human Pose Estimation via Deep Neural Ne.pdf:application/pdf}
}

@inproceedings{andriluka_2d_2014,
	address = {Columbus, OH},
	title = {2D {Human} {Pose} {Estimation}: {New} {Benchmark} and {State} of the {Art} {Analysis}},
	booktitle = {Proceedings of the 2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Andriluka, Mykhaylo and Pishchulin, Leonid and Gehler, Peter and Schiele, Bernt},
	month = jun,
	year = {2014},
	pages = {3686--3693},
	file = {Andriluka et al. - 2014 - 2D Human Pose Estimation New Benchmark and State .pdf:/Users/Matthias/Zotero/storage/5H69WV9S/Andriluka et al. - 2014 - 2D Human Pose Estimation New Benchmark and State .pdf:application/pdf}
}

@inproceedings{johnson_learning_2011,
	title = {Learning {Effective} {Human} {Pose} {Estimation} from {Inaccurate} {Annotation}},
	booktitle = {Proceedings of {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Johnson, Sam and Everingham, Mark},
	year = {2011}
}

@inproceedings{johnson_clustered_2010,
	address = {Aberystwyth, UK},
	title = {Clustered {Pose} and {Nonlinear} {Appearance} {Models} for {Human} {Pose} {Estimation}},
	booktitle = {Proceedings of the 2010 {British} {Machine} {Vision} {Conference} ({BMVC})},
	author = {Johnson, Sam and Everingham, Mark},
	month = aug,
	year = {2010},
	pages = {12.1--12.11},
	annote = {doi:10.5244/C.24.12}
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, NV},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {Proceedings of the 2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
	file = {He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:/Users/Matthias/Zotero/storage/HTIPXHQM/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@inproceedings{jhuang_towards_2013,
	address = {Sydney, AU},
	title = {Towards understanding action recognition},
	abstract = {Although action recognition in videos is widely studied, current methods often fail on real-world datasets. Many recent approaches improve accuracy and robustness to cope with challenging video sequences, but it is often unclear what affects the results most. This paper attempts to provide insights based on a systematic performance evaluation using thoroughly-annotated data of human actions. We annotate human Joints for the HMDB dataset (J-HMDB). This annotation can be used to derive ground truth optical ﬂow and segmentation. We evaluate current methods using this dataset and systematically replace the output of various algorithms with ground truth. This enables us to discover what is important – for example, should we work on improving ﬂow algorithms, estimating human bounding boxes, or enabling pose estimation? In summary, we ﬁnd that highlevel pose features greatly outperform low/mid level features; in particular, pose over time is critical. While current pose estimation algorithms are far from perfect, features extracted from estimated pose on a subset of J-HMDB, in which the full body is visible, outperform low/mid-level features. We also ﬁnd that the accuracy of the action recognition framework can be greatly increased by reﬁning the underlying low/mid level features; this suggests it is important to improve optical ﬂow and human detection algorithms. Our analysis and J-HMDB dataset should facilitate a deeper understanding of action recognition algorithms.},
	language = {en},
	booktitle = {Proceedings of the 2013 {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Jhuang, Hueihan and Gall, Juergen and Zuffi, Silvia and Schmid, Cordelia and Black, Michael J.},
	month = dec,
	year = {2013},
	pages = {3192--3199},
	file = {Jhuang et al. - Towards understanding action recognition.pdf:/Users/Matthias/Zotero/storage/7J7C65KD/Jhuang et al. - Towards understanding action recognition.pdf:application/pdf}
}

@inproceedings{gkioxari_chained_2016,
	address = {Cham},
	title = {Chained {Predictions} {Using} {Convolutional} {Neural} {Networks}},
	abstract = {In this work, we present an adaptation of the sequence-to-sequence model for structured vision tasks. In this model, the output variables for a given input are predicted sequentially using neural networks. The prediction for each output variable depends not only on the input but also on the previously predicted output variables. The model is applied to spatial localization tasks and uses convolutional neural networks (CNNs) for processing input images and a multi-scale deconvolutional architecture for making spatial predictions at each step. We explore the impact of weight sharing with a recurrent connection matrix between consecutive predictions, and compare it to a formulation where these weights are not tied. Untied weights are particularly suited for problems with a fixed sized structure, where different classes of output are predicted at different steps. We show that chain models achieve top performing results on human pose estimation from images and videos.},
	booktitle = {Computer {Vision} – {ECCV} 2016},
	author = {Gkioxari, Georgia and Toshev, Alexander and Jaitly, Navdeep},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	pages = {728--743},
	file = {Gkioxari et al. - 2016 - Chained Predictions Using Convolutional Neural Net.pdf:/Users/Matthias/Zotero/storage/F2CKZYDX/F2CKZYDX.pdf:application/pdf}
}

@inproceedings{iqbal_pose_2016,
	address = {Washington, DC},
	title = {Pose for {Action} - {Action} for {Pose}},
	abstract = {In this work we propose to utilize information about human actions to improve pose estimation in monocular videos. To this end, we present a pictorial structure model that exploits high-level information about activities to incorporate higher-order part dependencies by modeling action specific appearance models and pose priors. However, instead of using an additional expensive action recognition framework, the action priors are efficiently estimated by our pose estimation framework. This is achieved by starting with a uniform action prior and updating the action prior during pose estimation. We also show that learning the right amount of appearance sharing among action classes improves the pose estimation. We demonstrate the effectiveness of the proposed method on two challenging datasets for pose estimation and action recognition with over 80,000 test images.},
	urldate = {2019-04-25},
	booktitle = {Proceedings of the 12th {IEEE} {International} {Conference} on {Automatic} {Face} and {Gesture} {Recognition} ({FG})},
	author = {Iqbal, Umar and Garbade, Martin and Gall, Juergen},
	month = may,
	year = {2016},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to FG-2017},
	file = {arXiv\:1603.04037 PDF:/Users/Matthias/Zotero/storage/YFP952VJ/Iqbal et al. - 2016 - Pose for Action - Action for Pose.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/977VHYE5/1603.html:text/html}
}

@inproceedings{charles_personalizing_2016,
	title = {Personalizing {Human} {Video} {Pose} {Estimation}},
	abstract = {We propose a personalized ConvNet pose estimator that automatically adapts itself to the uniqueness of a person’s appearance to improve pose estimation in long videos.},
	language = {en},
	booktitle = {Proceedings of the {IEEE} conference on computer vision and pattern recognition},
	author = {Charles, James and Pfister, Tomas and Magee, Derek and Hogg, David and Zisserman, Andrew},
	year = {2016},
	pages = {3063--3072},
	file = {Charles et al. - Personalizing Human Video Pose Estimation.pdf:/Users/Matthias/Zotero/storage/ZL7IF5ZC/Charles et al. - Personalizing Human Video Pose Estimation.pdf:application/pdf}
}

@inproceedings{pfister_flowing_2015,
	address = {Santiago, Chile},
	title = {Flowing {ConvNets} for {Human} {Pose} {Estimation} in {Videos}},
	isbn = {978-1-4673-8391-2},
	url = {http://ieeexplore.ieee.org/document/7410579/},
	doi = {10.1109/ICCV.2015.222},
	abstract = {The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to beneﬁt from temporal context by combining information across the multiple frames using optical ﬂow.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Pfister, Tomas and Charles, James and Zisserman, Andrew},
	month = dec,
	year = {2015},
	pages = {1913--1921},
	file = {Pfister et al. - 2015 - Flowing ConvNets for Human Pose Estimation in Vide.pdf:/Users/Matthias/Zotero/storage/S2PBL27W/Pfister et al. - 2015 - Flowing ConvNets for Human Pose Estimation in Vide.pdf:application/pdf}
}

@inproceedings{rafi_efficient_2016,
	address = {York, UK},
	title = {An {Efficient} {Convolutional} {Network} for {Human} {Pose} {Estimation}},
	isbn = {978-1-901725-59-9},
	url = {http://www.bmva.org/bmvc/2016/papers/paper109/index.html},
	doi = {10.5244/C.30.109},
	abstract = {In recent years, human pose estimation has greatly beneﬁted from deep learning and huge gains in performance have been achieved. The trend to maximise the accuracy on benchmarks, however, resulted in computationally expensive deep network architectures that require expensive hardware and pre-training on large datasets. This makes it difﬁcult to compare different methods and to reproduce existing results. In this paper, we therefore propose an efﬁcient deep network architecture that can be efﬁciently trained on mid-range GPUs without the need of any pre-training. Despite the low computational requirements of our network, it is on par with much more complex models on popular benchmarks for human pose estimation.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {Procedings of the {British} {Machine} {Vision} {Conference} 2016},
	publisher = {British Machine Vision Association},
	author = {Rafi, Umer and Leibe, Bastian and Gall, Juergen and Kostrikov, Ilya},
	year = {2016},
	pages = {109.1--109.11},
	file = {Rafi et al. - 2016 - An Efficient Convolutional Network for Human Pose .pdf:/Users/Matthias/Zotero/storage/VLD48VTN/Rafi et al. - 2016 - An Efficient Convolutional Network for Human Pose .pdf:application/pdf}
}

@article{bulat_human_2016,
	title = {Human pose estimation via {Convolutional} {Part} {Heatmap} {Regression}},
	url = {http://arxiv.org/abs/1609.01743},
	doi = {10.1007/978-3-319-46478-7_44},
	abstract = {This paper is on human pose estimation using Convolutional Neural Networks. Our main contribution is a CNN cascaded architecture specifically designed for learning part relationships and spatial context, and robustly inferring pose even for the case of severe part occlusions. To this end, we propose a detection-followed-by-regression CNN cascade. The first part of our cascade outputs part detection heatmaps and the second part performs regression on these heatmaps. The benefits of the proposed architecture are multi-fold: It guides the network where to focus in the image and effectively encodes part constraints and context. More importantly, it can effectively cope with occlusions because part detection heatmaps for occluded parts provide low confidence scores which subsequently guide the regression part of our network to rely on contextual information in order to predict the location of these parts. Additionally, we show that the proposed cascade is flexible enough to readily allow the integration of various CNN architectures for both detection and regression, including recent ones based on residual learning. Finally, we illustrate that our cascade achieves top performance on the MPII and LSP data sets. Code can be downloaded from http://www.cs.nott.ac.uk/{\textasciitilde}psxab5/},
	urldate = {2019-04-25},
	journal = {arXiv:1609.01743 [cs]},
	author = {Bulat, Adrian and Tzimiropoulos, Georgios},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.01743},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: accepted to ECCV 2016},
	file = {arXiv\:1609.01743 PDF:/Users/Matthias/Zotero/storage/WIPV68ZS/Bulat and Tzimiropoulos - 2016 - Human pose estimation via Convolutional Part Heatm.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/GWTEGHCA/1609.html:text/html}
}

@inproceedings{hu_bottom-up_2016,
	address = {Las Vegas, NV, USA},
	title = {Bottom-{Up} and {Top}-{Down} {Reasoning} with {Hierarchical} {Rectified} {Gaussians}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780973/},
	doi = {10.1109/CVPR.2016.604},
	abstract = {Convolutional neural nets (CNNs) have demonstrated remarkable performance in recent history. Such approaches tend to work in a “unidirectional” bottom-up feed-forward fashion. However, practical experience and biological evidence tells us that feedback plays a crucial role, particularly for detailed spatial understanding tasks. This work explores “bidirectional” architectures that also reason with top-down feedback: neural units are inﬂuenced by both lower and higher-level units.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Hu, Peiyun and Ramanan, Deva},
	month = jun,
	year = {2016},
	pages = {5600--5609},
	file = {Hu and Ramanan - 2016 - Bottom-Up and Top-Down Reasoning with Hierarchical.pdf:/Users/Matthias/Zotero/storage/W52WHAQF/Hu and Ramanan - 2016 - Bottom-Up and Top-Down Reasoning with Hierarchical.pdf:application/pdf}
}

@inproceedings{carreira_human_2016,
	address = {Las Vegas, NV, USA},
	title = {Human {Pose} {Estimation} with {Iterative} {Error} {Feedback}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780881/},
	doi = {10.1109/CVPR.2016.512},
	abstract = {Hierarchical feature extractors such as Convolutional Networks (ConvNets) have achieved impressive performance on a variety of classiﬁcation tasks using purely feedforward processing. Feedforward architectures can learn rich representations of the input space but do not explicitly model dependencies in the output spaces, that are quite structured for tasks such as articulated human pose estimation or object segmentation. Here we propose a framework that expands the expressive power of hierarchical feature extractors to encompass both input and output spaces, by introducing top-down feedback. Instead of directly predicting the outputs in one go, we use a self-correcting model that progressively changes an initial solution by feeding back error predictions, in a process we call Iterative Error Feedback (IEF). IEF shows excellent performance on the task of articulated pose estimation in the challenging MPII and LSP benchmarks, matching the state-of-the-art without requiring ground truth scale annotation.},
	language = {en},
	urldate = {2019-04-25},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Carreira, Joao and Agrawal, Pulkit and Fragkiadaki, Katerina and Malik, Jitendra},
	month = jun,
	year = {2016},
	pages = {4733--4742},
	file = {Carreira et al. - 2016 - Human Pose Estimation with Iterative Error Feedbac.pdf:/Users/Matthias/Zotero/storage/MDWP3UJ5/Carreira et al. - 2016 - Human Pose Estimation with Iterative Error Feedbac.pdf:application/pdf}
}

@inproceedings{zhang_actemes_2013,
	address = {Portland, OR},
	title = {From {Actemes} to {Action}: {A} {Strongly}-{Supervised} {Representation} for {Detailed} {Action} {Understanding}},
	shorttitle = {From {Actemes} to {Action}},
	abstract = {This paper presents a novel approach for analyzing human actions in non-scripted, unconstrained video settings based on volumetric, x-y-t, patch classifiers, termed actemes. Unlike previous action-related work, the discovery of patch classifiers is posed as a strongly-supervised process. Specifically, key point labels (e.g., position) across space time are used in a data-driven training process to discover patches that are highly clustered in the space time key point configuration space. To support this process, a new human action dataset consisting of challenging consumer videos is introduced, where notably the action label, the 2D position of a set of key points and their visibilities are provided for each video frame. On a novel input video, each acteme is used in a sliding volume scheme to yield a set of sparse, non-overlapping detections. These detections provide the intermediate substrate for segmenting out the action. For action classification, the proposed representation shows significant improvement over state-of-the-art low-level features, while providing spatiotemporal localization as additional output, which sheds further light into detailed action understanding.},
	booktitle = {Proceedings of the 2013 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Zhang, Weiyu and Zhu, Menglong and Derpanis, Konstantinos G.},
	month = dec,
	year = {2013},
	keywords = {actemes, action classification, action detection, action label, action understanding, action-related work, Cameras, consumer videos, Context, data-driven training process, gesture recognition, human action dataset, human actions analyzing, image classification, image representation, patch classifiers discovery, Penn Action, Semantics, sliding volume scheme, space time, spacetime key-point configuration space, sparse nonoverlapping detections, spatiotemporal localization, Spatiotemporal phenomena, supervised representation, Training, Trajectory, unconstrained video settings, video signal processing, Visualization},
	pages = {2248--2255},
	file = {IEEE Xplore Abstract Record:/Users/Matthias/Zotero/storage/JH7MIMKB/6751390.html:text/html;IEEE Xplore Full Text PDF:/Users/Matthias/Zotero/storage/6B6GJATY/Zhang et al. - 2013 - From Actemes to Action A Strongly-Supervised Repr.pdf:application/pdf}
}

@article{sigal_humaneva:_2010,
	title = {{HumanEva}: {Synchronized} {Video} and {Motion} {Capture} {Dataset} and {Baseline} {Algorithm} for {Evaluation} of {Articulated} {Human} {Motion}},
	volume = {87},
	issn = {0920-5691, 1573-1405},
	shorttitle = {{HumanEva}},
	url = {http://link.springer.com/10.1007/s11263-009-0273-6},
	doi = {10.1007/s11263-009-0273-6},
	language = {en},
	number = {1-2},
	urldate = {2019-04-25},
	journal = {International Journal of Computer Vision},
	author = {Sigal, Leonid and Balan, Alexandru O. and Black, Michael J.},
	month = mar,
	year = {2010},
	pages = {4--27},
	file = {Sigal et al. - 2010 - HumanEva Synchronized Video and Motion Capture Da.pdf:/Users/Matthias/Zotero/storage/4YD9UFS2/Sigal et al. - 2010 - HumanEva Synchronized Video and Motion Capture Da.pdf:application/pdf}
}

@article{dantone_body_2014,
	title = {Body {Parts} {Dependent} {Joint} {Regressors} for {Human} {Pose} {Estimation} in {Still} {Images}},
	volume = {36},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/6802375/},
	doi = {10.1109/TPAMI.2014.2318702},
	abstract = {In this work, we address the problem of estimating 2d human pose from still images. Articulated body pose estimation is challenging due to the large variation in body poses and appearances of the different body parts. Recent methods that rely on the pictorial structure framework have shown to be very successful in solving this task. They model the body part appearances using discriminatively trained, independent part templates and the spatial relations of the body parts using a tree model. Within such a framework, we address the problem of obtaining better part templates which are able to handle a very high variation in appearance. To this end, we introduce parts dependent body joint regressors which are random forests that operate over two layers. While the ﬁrst layer acts as an independent body part classiﬁer, the second layer takes the estimated class distributions of the ﬁrst one into account and is thereby able to predict joint locations by modeling the interdependence and co-occurrence of the parts. This helps to overcome typical ambiguities of tree structures, such as self-similarities of legs and arms. In addition, we introduce a novel dataset termed F ashionP ose that contains over 7, 000 images with a challenging variation of body part appearances due to a large variation of dressing styles. In the experiments, we demonstrate that the proposed parts dependent joint regressors outperform independent classiﬁers or regressors. The method also performs better or similar to the state-of-the-art in terms of accuracy, while running with a couple of frames per second.},
	language = {en},
	number = {11},
	urldate = {2019-04-25},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Dantone, Matthias and Gall, Juergen and Leistner, Christian and Van Gool, Luc},
	month = nov,
	year = {2014},
	pages = {2131--2143},
	file = {Dantone et al. - 2014 - Body Parts Dependent Joint Regressors for Human Po.pdf:/Users/Matthias/Zotero/storage/MA3YZMKS/Dantone et al. - 2014 - Body Parts Dependent Joint Regressors for Human Po.pdf:application/pdf}
}

@inproceedings{luvizon_2d/3d_2018,
	address = {Salt Lake City, UT},
	title = {2D/3D {Pose} {Estimation} and {Action} {Recognition} {Using} {Multitask} {Deep} {Learning}},
	abstract = {Action recognition and human pose estimation are closely related but both problems are generally handled as distinct tasks in the literature. In this work, we propose a multitask framework for jointly 2D and 3D pose estimation from still images and human action recognition from video sequences. We show that a single architecture can be used to solve the two problems in an efﬁcient way and still achieves state-of-the-art results. Additionally, we demonstrate that optimization from end-toend leads to signiﬁcantly higher accuracy than separated learning. The proposed architecture can be trained with data from different categories simultaneously in a seamlessly way. The reported results on four datasets (MPII, Human3.6M, Penn Action and NTU) demonstrate the effectiveness of our method on the targeted tasks.},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {Proceedings of the 2018 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Luvizon, Diogo C. and Picard, David and Tabia, Hedi},
	month = jun,
	year = {2018},
	pages = {5137--5146},
	file = {Luvizon et al. - 2018 - 2D3D Pose Estimation and Action Recognition Using.pdf:/Users/Matthias/Zotero/storage/MPTJBEIM/Luvizon et al. - 2018 - 2D3D Pose Estimation and Action Recognition Using.pdf:application/pdf}
}

@inproceedings{pavlakos_learning_2018,
	address = {Salt Lake City, UT},
	title = {Learning to {Estimate} 3D {Human} {Pose} and {Shape} from a {Single} {Color} {Image}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578153/},
	doi = {10.1109/CVPR.2018.00055},
	abstract = {This work addresses the problem of estimating the full body 3D human pose and shape from a single color image. This is a task where iterative optimization-based solutions have typically prevailed, while Convolutional Networks (ConvNets) have suffered because of the lack of training data and their low resolution 3D predictions. Our work aims to bridge this gap and proposes an efﬁcient and effective direct prediction method based on ConvNets. Central part to our approach is the incorporation of a parametric statistical body shape model (SMPL) within our end-to-end framework. This allows us to get very detailed 3D mesh results, while requiring estimation only of a small number of parameters, making it friendly for direct network prediction. Interestingly, we demonstrate that these parameters can be predicted reliably only from 2D keypoints and masks. These are typical outputs of generic 2D human analysis ConvNets, allowing us to relax the massive requirement that images with 3D shape ground truth are available for training. Simultaneously, by maintaining differentiability, at training time we generate the 3D mesh from the estimated parameters and optimize explicitly for the surface using a 3D per-vertex loss. Finally, a differentiable renderer is employed to project the 3D mesh to the image, which enables further reﬁnement of the network, by optimizing for the consistency of the projection with 2D annotations (i.e., 2D keypoints or masks). The proposed approach outperforms previous baselines on this task and offers an attractive solution for direct prediction of 3D shape from a single color image.},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Pavlakos, Georgios and Zhu, Luyang and Zhou, Xiaowei and Daniilidis, Kostas},
	month = jun,
	year = {2018},
	pages = {459--468},
	file = {Pavlakos et al. - 2018 - Learning to Estimate 3D Human Pose and Shape from .pdf:/Users/Matthias/Zotero/storage/AJCHUH6M/Pavlakos et al. - 2018 - Learning to Estimate 3D Human Pose and Shape from .pdf:application/pdf}
}

@inproceedings{radwan_monocular_2013,
	address = {Sydney, Australia},
	title = {Monocular {Image} 3D {Human} {Pose} {Estimation} under {Self}-{Occlusion}},
	isbn = {978-1-4799-2840-8},
	url = {http://ieeexplore.ieee.org/document/6751345/},
	doi = {10.1109/ICCV.2013.237},
	abstract = {In this paper, an automatic approach for 3D pose reconstruction from a single image is proposed. The presence of human body articulation, hallucinated parts and cluttered background leads to ambiguity during the pose inference, which makes the problem non-trivial. Researchers have explored various methods based on motion and shading in order to reduce the ambiguity and reconstruct the 3D pose. The key idea of our algorithm is to impose both kinematic and orientation constraints. The former is imposed by projecting a 3D model onto the input image and pruning the parts, which are incompatible with the anthropomorphism. The latter is applied by creating synthetic views via regressing the input view to multiple oriented views. After applying the constraints, the 3D model is projected onto the initial and synthetic views, which further reduces the ambiguity. Finally, we borrow the direction of the unambiguous parts from the synthetic views to the initial one, which results in the 3D pose. Quantitative experiments are performed on the HumanEva-I dataset and qualitatively on unconstrained images from the Image Parse dataset. The results show the robustness of the proposed approach to accurately reconstruct the 3D pose form a single image.},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Radwan, Ibrahim and Dhall, Abhinav and Goecke, Roland},
	month = dec,
	year = {2013},
	pages = {1888--1895},
	file = {Radwan et al. - 2013 - Monocular Image 3D Human Pose Estimation under Sel.pdf:/Users/Matthias/Zotero/storage/UA8XYVPK/Radwan et al. - 2013 - Monocular Image 3D Human Pose Estimation under Sel.pdf:application/pdf}
}

@article{ionescu_human3.6m:_2014,
	title = {Human3.6M: {Large} {Scale} {Datasets} and {Predictive} {Methods} for 3D {Human} {Sensing} in {Natural} {Environments}},
	volume = {36},
	issn = {0162-8828},
	shorttitle = {Human3.6M},
	doi = {10.1109/TPAMI.2013.248},
	abstract = {We introduce a new dataset, Human3.6M, of 3.6 Million accurate 3D Human poses, acquired by recording the performance of 5 female and 6 male subjects, under 4 different viewpoints, for training realistic human sensing systems and for evaluating the next generation of human pose estimation models and algorithms. Besides increasing the size of the datasets in the current state-of-the-art by several orders of magnitude, we also aim to complement such datasets with a diverse set of motions and poses encountered as part of typical human activities (taking photos, talking on the phone, posing, greeting, eating, etc.), with additional synchronized image, human motion capture, and time of flight (depth) data, and with accurate 3D body scans of all the subject actors involved. We also provide controlled mixed reality evaluation scenarios where 3D human models are animated using motion capture and inserted using correct 3D geometry, in complex real environments, viewed with moving cameras, and under occlusion. Finally, we provide a set of large-scale statistical models and detailed evaluation baselines for the dataset illustrating its diversity and the scope for improvement by future work in the research community. Our experiments show that our best large-scale model can leverage our full training set to obtain a 20\% improvement in performance compared to a training set of the scale of the largest existing public dataset for this problem. Yet the potential for improvement by leveraging higher capacity, more complex models with our large dataset, is substantially vaster and should stimulate future research. The dataset together with code for the associated large-scale learning models, features, visualization tools, as well as the evaluation server, is available online at http://vision.imar.ro/human3.6m.},
	number = {7},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ionescu, C. and Papava, D. and Olaru, V. and Sminchisescu, C.},
	month = jul,
	year = {2014},
	keywords = {Estimation, Humans, Joints, pose estimation, Training, image motion analysis, Three-dimensional displays, Cameras, 3D geometry, 3D human models, 3D human pose estimation, 3D human poses, 3D human sensing, Algorithms, articulated body modeling, Biometry, data visualisation, Databases, Factual, Ecosystem, evaluation server, Fourier kernel approximations, human motion capture, human motion capture data, human pose estimation models, Human3.6M, Image Enhancement, Image Interpretation, Computer-Assisted, image sensors, Imaging, Three-Dimensional, Information Storage and Retrieval, large scale datasets, large-scale learning, large-scale learning models, large-scale statistical models, learning (artificial intelligence), Modeling and recovery of physical attributes, Motion, motion capture, moving cameras, natural environments, optimization, Pattern Recognition, Automated, Photography, Posture, predictive methods, realistic human sensing systems, Reproducibility of Results, Sensitivity and Specificity, Sensors, Solid modeling, statistical analysis, structured prediction, Subtraction Technique, synchronized image, visualization tools, Whole Body Imaging},
	pages = {1325--1339},
	file = {IEEE Xplore Abstract Record:/Users/Matthias/Zotero/storage/87CPZJ6S/6682899.html:text/html;IEEE Xplore Full Text PDF:/Users/Matthias/Zotero/storage/K5I2QP6G/Ionescu et al. - 2014 - Human3.6M Large Scale Datasets and Predictive Met.pdf:application/pdf}
}

@inproceedings{kanazawa_end--end_2018,
	address = {Salt Lake City, UT},
	title = {End-to-{End} {Recovery} of {Human} {Shape} and {Pose}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578842/},
	doi = {10.1109/CVPR.2018.00744},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Kanazawa, Angjoo and Black, Michael J. and Jacobs, David W. and Malik, Jitendra},
	month = jun,
	year = {2018},
	pages = {7122--7131},
	file = {Kanazawa et al. - 2018 - End-to-End Recovery of Human Shape and Pose.pdf:/Users/Matthias/Zotero/storage/4BQSI3VH/Kanazawa et al. - 2018 - End-to-End Recovery of Human Shape and Pose.pdf:application/pdf}
}

@article{zhou_monocap:_2019,
	title = {{MonoCap}: {Monocular} {Human} {Motion} {Capture} using a {CNN} {Coupled} with a {Geometric} {Prior}},
	volume = {41},
	issn = {0162-8828},
	shorttitle = {{MonoCap}},
	doi = {10.1109/TPAMI.2018.2816031},
	abstract = {Recovering 3D full-body human pose is a challenging problem with many applications. It has been successfully addressed by motion capture systems with body worn markers and multiple cameras. In this paper, we address the more challenging case of not only using a single camera but also not leveraging markers: going directly from 2D appearance to 3D geometry. Deep learning approaches have shown remarkable abilities to discriminatively learn 2D appearance features. The missing piece is how to integrate 2D, 3D, and temporal information to recover 3D geometry and account for the uncertainties arising from the discriminative model. We introduce a novel approach that treats 2D joint locations as latent variables whose uncertainty distributions are given by a deep fully convolutional neural network. The unknown 3D poses are modeled by a sparse representation and the 3D parameter estimates are realized via an Expectation-Maximization algorithm, where it is shown that the 2D joint location uncertainties can be conveniently marginalized out during inference. Extensive evaluation on benchmark datasets shows that the proposed approach achieves greater accuracy over state-of-the-art baselines. Notably, the proposed approach does not require synchronized 2D-3D data for training and is applicable to “in-the-wild” images, which is demonstrated with the MPII dataset.},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhou, X. and Zhu, M. and Pavlakos, G. and Leonardos, S. and Derpanis, K. G. and Daniilidis, K.},
	month = apr,
	year = {2019},
	keywords = {image representation, pose estimation, image motion analysis, Three-dimensional displays, Two dimensional displays, Cameras, learning (artificial intelligence), Solid modeling, 2D appearance features, 2D joint locations, 3D full-body human pose, 3D parameter estimates, body worn markers, cameras, CNN, convolutional neural nets, convolutional neural network, deep learning, discriminative model, expectation-maximisation algorithm, Expectation-Maximization algorithm, geometric prior, human pose, image capture, Image reconstruction, in-the-wild images, MonoCap, monocular human motion capture, Motion capture, motion capture systems, MPII dataset, parameter estimation, Pose estimation, sparse representation, stereo image processing, temporal information, Uncertainty},
	pages = {901--914},
	file = {IEEE Xplore Abstract Record:/Users/Matthias/Zotero/storage/SWFGZVHP/8316924.html:text/html;IEEE Xplore Full Text PDF:/Users/Matthias/Zotero/storage/GNT6ZSLJ/Zhou et al. - 2019 - MonoCap Monocular Human Motion Capture using a CN.pdf:application/pdf}
}

@article{mehta_vnect:_2017,
	title = {{VNect}: real-time 3D human pose estimation with a single {RGB} camera},
	volume = {36},
	issn = {07300301},
	shorttitle = {{VNect}},
	url = {http://dl.acm.org/citation.cfm?doid=3072959.3073596},
	doi = {10.1145/3072959.3073596},
	language = {en},
	number = {4},
	urldate = {2019-04-30},
	journal = {ACM Transactions on Graphics},
	author = {Mehta, Dushyant and Sridhar, Srinath and Sotnychenko, Oleksandr and Rhodin, Helge and Shafiei, Mohammad and Seidel, Hans-Peter and Xu, Weipeng and Casas, Dan and Theobalt, Christian},
	month = jul,
	year = {2017},
	pages = {1--14},
	file = {Mehta et al. - 2017 - VNect real-time 3D human pose estimation with a s.pdf:/Users/Matthias/Zotero/storage/IEVSDEWU/Mehta et al. - 2017 - VNect real-time 3D human pose estimation with a s.pdf:application/pdf}
}

@inproceedings{zhou_sparseness_2016,
	address = {Las Vegas, NV, USA},
	title = {Sparseness {Meets} {Deepness}: 3D {Human} {Pose} {Estimation} from {Monocular} {Video}},
	isbn = {978-1-4673-8851-1},
	shorttitle = {Sparseness {Meets} {Deepness}},
	url = {http://ieeexplore.ieee.org/document/7780906/},
	doi = {10.1109/CVPR.2016.537},
	abstract = {This paper addresses the challenge of 3D full-body human pose estimation from a monocular image sequence. Here, two cases are considered: (i) the image locations of the human joints are provided and (ii) the image locations of joints are unknown. In the former case, a novel approach is introduced that integrates a sparsity-driven 3D geometric prior and temporal smoothness. In the latter case, the former case is extended by treating the image locations of the joints as latent variables to take into account considerable uncertainties in 2D joint locations. A deep fully convolutional network is trained to predict the uncertainty maps of the 2D joint locations. The 3D pose estimates are realized via an Expectation-Maximization algorithm over the entire sequence, where it is shown that the 2D joint location uncertainties can be conveniently marginalized out during inference. Empirical evaluation on the Human3.6M dataset shows that the proposed approaches achieve greater 3D pose estimation accuracy over state-of-the-art baselines. Further, the proposed approach outperforms a publicly available 2D pose estimation baseline on the challenging PennAction dataset.},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Zhou, Xiaowei and Zhu, Menglong and Leonardos, Spyridon and Derpanis, Konstantinos G. and Daniilidis, Kostas},
	month = jun,
	year = {2016},
	pages = {4966--4975},
	file = {Zhou et al. - 2016 - Sparseness Meets Deepness 3D Human Pose Estimatio.pdf:/Users/Matthias/Zotero/storage/AQTIA2RF/Zhou et al. - 2016 - Sparseness Meets Deepness 3D Human Pose Estimatio.pdf:application/pdf}
}

@inproceedings{song_thin-slicing_2017,
	address = {Honolulu},
	title = {Thin-{Slicing} {Network}: {A} {Deep} {Structured} {Model} for {Pose} {Estimation} in {Videos}},
	isbn = {978-1-5386-0457-1},
	shorttitle = {Thin-{Slicing} {Network}},
	url = {http://ieeexplore.ieee.org/document/8100073/},
	doi = {10.1109/CVPR.2017.590},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Song, Jie and Wang, Limin and Gool, Luc Van and Hilliges, Otmar},
	year = {2017},
	pages = {5563--5572},
	file = {Song et al. - 2017 - Thin-Slicing Network A Deep Structured Model for .pdf:/Users/Matthias/Zotero/storage/B47GRJVG/Song et al. - 2017 - Thin-Slicing Network A Deep Structured Model for .pdf:application/pdf}
}

@inproceedings{girdhar_detect-and-track:_2018,
	address = {Salt Lake City, UT},
	title = {Detect-and-{Track}: {Efficient} {Pose} {Estimation} in {Videos}},
	shorttitle = {Detect-and-{Track}},
	abstract = {This paper addresses the problem of estimating and tracking human body keypoints in complex, multi-person video. We propose an extremely lightweight yet highly effective approach that builds upon the latest advancements in human detection [17] and video understanding [5]. Our method operates in two-stages: keypoint estimation in frames or short clips, followed by lightweight tracking to generate keypoint predictions linked over the entire video. For frame-level pose estimation we experiment with Mask R-CNN, as well as our own proposed 3D extension of this model, which leverages temporal information over small clips to generate more robust frame predictions. We conduct extensive ablative experiments on the newly released multi-person video pose estimation benchmark, PoseTrack, to validate various design choices of our model. Our approach achieves an accuracy of 55.2\% on the validation and 51.8\% on the test set using the Multi-Object Tracking Accuracy (MOTA) metric, and achieves state of the art performance on the ICCV 2017 PoseTrack keypoint tracking challenge [1].},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {Proceedings of the 2018 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Girdhar, Rohit and Gkioxari, Georgia and Torresani, Lorenzo and Paluri, Manohar and Tran, Du},
	month = jun,
	year = {2018},
	pages = {350--359},
	file = {Girdhar et al. - 2018 - Detect-and-Track Efficient Pose Estimation in Vid.pdf:/Users/Matthias/Zotero/storage/FZYMAANH/Girdhar et al. - 2018 - Detect-and-Track Efficient Pose Estimation in Vid.pdf:application/pdf}
}

@inproceedings{chu_multi-context_2017,
	address = {Honolulu, HI},
	title = {Multi-context {Attention} for {Human} {Pose} {Estimation}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8100084/},
	doi = {10.1109/CVPR.2017.601},
	abstract = {In this paper, we propose to incorporate convolutional neural networks with a multi-context attention mechanism into an end-to-end framework for human pose estimation. We adopt stacked hourglass networks to generate attention maps from features at multiple resolutions with various semantics. The Conditional Random Field (CRF) is utilized to model the correlations among neighboring regions in the attention map. We further combine the holistic attention model, which focuses on the global consistency of the full human body, and the body part attention model, which focuses on detailed descriptions for different body parts. Hence our model has the ability to focus on different granularity from local salient regions to global semanticconsistent spaces. Additionally, we design novel Hourglass Residual Units (HRUs) to increase the receptive ﬁeld of the network. These units are extensions of residual units with a side branch incorporating ﬁlters with larger receptive ﬁeld, hence features with various scales are learned and combined within the HRUs. The effectiveness of the proposed multi-context attention mechanism and the hourglass residual units is evaluated on two widely used human pose estimation benchmarks. Our approach outperforms all existing methods on both benchmarks over all the body parts. Code has been made publicly available.},
	language = {en},
	urldate = {2019-04-30},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chu, Xiao and Yang, Wei and Ouyang, Wanli and Ma, Cheng and Yuille, Alan L. and Wang, Xiaogang},
	month = jul,
	year = {2017},
	pages = {5669--5678}
}

@incollection{newell_associative_2017,
	title = {Associative {Embedding}: {End}-to-{End} {Learning} for {Joint} {Detection} and {Grouping}},
	shorttitle = {Associative {Embedding}},
	url = {http://papers.nips.cc/paper/6822-associative-embedding-end-to-end-learning-for-joint-detection-and-grouping.pdf},
	urldate = {2019-04-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Newell, Alejandro and Huang, Zhiao and Deng, Jia},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {2277--2287},
	file = {Newell et al. - 2017 - Associative Embedding End-to-End Learning for Joi.pdf:/Users/Matthias/Zotero/storage/SVIP8TAR/Newell et al. - 2017 - Associative Embedding End-to-End Learning for Joi.pdf:application/pdf;NIPS Snapshot:/Users/Matthias/Zotero/storage/PP4EEZS3/6822-associative-embedding-end-to-end-learning-for-joint-detection-and-grouping.html:text/html}
}

@inproceedings{chen_cascaded_2018,
	address = {Salt Lake City, UT},
	title = {Cascaded {Pyramid} {Network} for {Multi}-person {Pose} {Estimation}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578840/},
	doi = {10.1109/CVPR.2018.00742},
	abstract = {The topic of multi-person pose estimation has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as occluded keypoints, invisible keypoints and complex background, which cannot be well addressed. In this paper, we present a novel network structure called Cascaded Pyramid Network (CPN) which targets to relieve the problem from these “hard” keypoints. More speciﬁcally, our algorithm includes two stages: GlobalNet and ReﬁneNet. GlobalNet is a feature pyramid network which can successfully localize the “simple” keypoints like eyes and hands but may fail to precisely recognize the occluded or invisible keypoints. Our ReﬁneNet tries explicitly handling the “hard” keypoints by integrating all levels of feature representations from the GlobalNet together with an online hard keypoint mining loss. In general, to address the multi-person pose estimation problem, a top-down pipeline is adopted to ﬁrst generate a set of human bounding boxes based on a detector, followed by our CPN for keypoint localization in each human bounding box. Based on the proposed algorithm, we achieve state-ofart results on the COCO keypoint benchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1 on the COCO test-challenge dataset, which is a 19\% relative improvement compared with 60.5 from the COCO 2016 keypoint challenge. Code1 and the detection results for person used will be publicly available for further research.},
	language = {en},
	urldate = {2019-05-01},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Chen, Yilun and Wang, Zhicheng and Peng, Yuxiang and Zhang, Zhiqiang and Yu, Gang and Sun, Jian},
	month = jun,
	year = {2018},
	pages = {7103--7112},
	file = {Chen et al. - 2018 - Cascaded Pyramid Network for Multi-person Pose Est.pdf:/Users/Matthias/Zotero/storage/WH96P26J/Chen et al. - 2018 - Cascaded Pyramid Network for Multi-person Pose Est.pdf:application/pdf}
}

@inproceedings{hossain_exploiting_2018,
	title = {Exploiting temporal information for 3D pose estimation},
	volume = {11214},
	url = {http://arxiv.org/abs/1711.08585},
	doi = {10.1007/978-3-030-01249-6_5},
	abstract = {In this work, we address the problem of 3D human pose estimation from a sequence of 2D human poses. Although the recent success of deep networks has led many state-of-the-art methods for 3D pose estimation to train deep networks end-to-end to predict from images directly, the top-performing approaches have shown the effectiveness of dividing the task of 3D pose estimation into two steps: using a state-of-the-art 2D pose estimator to estimate the 2D pose from images and then mapping them into 3D space. They also showed that a low-dimensional representation like 2D locations of a set of joints can be discriminative enough to estimate 3D pose with high accuracy. However, estimation of 3D pose for individual frames leads to temporally incoherent estimates due to independent error in each frame causing jitter. Therefore, in this work we utilize the temporal information across a sequence of 2D joint locations to estimate a sequence of 3D poses. We designed a sequence-to-sequence network composed of layer-normalized LSTM units with shortcut connections connecting the input to the output on the decoder side and imposed temporal smoothness constraint during training. We found that the knowledge of temporal consistency improves the best reported result on Human3.6M dataset by approximately \$12.2{\textbackslash}\%\$ and helps our network to recover temporally consistent 3D poses over a sequence of images even when the 2D pose detector fails.},
	urldate = {2019-05-01},
	booktitle = {European {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Hossain, Mir Rayat Imtiaz and Little, James J.},
	year = {2018},
	note = {arXiv: 1711.08585},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {69--86},
	file = {arXiv\:1711.08585 PDF:/Users/Matthias/Zotero/storage/XZF2LE7S/Hossain and Little - 2018 - Exploiting temporal information for 3D pose estima.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/6RZKREM6/1711.html:text/html}
}

@inproceedings{lin_recurrent_2017,
	title = {Recurrent 3D {Pose} {Sequence} {Machines}},
	abstract = {3D human articulated pose recovery from monocular image sequences is very challenging due to the diverse appearances, viewpoints, occlusions, and also the human 3D pose is inherently ambiguous from the monocular imagery. It is thus critical to exploit rich spatial and temporal long-range dependencies among body joints for accurate 3D pose sequence prediction. Existing approaches usually manually design some elaborate prior terms and human body kinematic constraints for capturing structures, which are often insufficient to exploit all intrinsic structures and not scalable for all scenarios. In contrast, this paper presents a Recurrent 3D Pose Sequence Machine(RPSM) to automatically learn the image-dependent structural constraint and sequence-dependent temporal context by using a multi-stage sequential refinement. At each stage, our RPSM is composed of three modules to predict the 3D pose sequences based on the previously learned 2D pose representations and 3D poses: (i) a 2D pose module extracting the image-dependent pose representations, (ii) a 3D pose recurrent module regressing 3D poses and (iii) a feature adaption module serving as a bridge between module (i) and (ii) to enable the representation transformation from 2D to 3D domain. These three modules are then assembled into a sequential prediction framework to refine the predicted poses with multiple recurrent stages. Extensive evaluations on the Human3.6M dataset and HumanEva-I dataset show that our RPSM outperforms all state-of-the-art approaches for 3D pose estimation.},
	language = {en},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Lin, Mude and Lin, Liang and Liang, Xiaodan and Wang, Keze and Cheng, Hui},
	year = {2017},
	file = {Lin et al. - Recurrent 3D Pose Sequence Machines.pdf:/Users/Matthias/Zotero/storage/KJCW8AIZ/Lin et al. - Recurrent 3D Pose Sequence Machines.pdf:application/pdf}
}

@incollection{ferrari_propagating_2018,
	address = {Cham},
	title = {Propagating {LSTM}: 3D {Pose} {Estimation} {Based} on {Joint} {Interdependency}},
	volume = {11211},
	isbn = {978-3-030-01233-5 978-3-030-01234-2},
	shorttitle = {Propagating {LSTM}},
	url = {http://link.springer.com/10.1007/978-3-030-01234-2_8},
	abstract = {We present a novel 3D pose estimation method based on joint interdependency (JI) for acquiring 3D joints from the human pose of an RGB image. The JI incorporates the body part based structural connectivity of joints to learn the high spatial correlation of human posture on our method. Towards this goal, we propose a new long short-term memory (LSTM)-based deep learning architecture named propagating LSTM networks (p-LSTMs), where each LSTM is connected sequentially to reconstruct 3D depth from the centroid to edge joints through learning the intrinsic JI. In the ﬁrst LSTM, the seed joints of 3D pose are created and reconstructed into the whole-body joints through the connected LSTMs. Utilizing the p-LSTMs, we achieve the higher accuracy of about 11.2\% than state-of-the-art methods on the largest publicly available database. Importantly, we demonstrate that the JI drastically reduces the structural errors at body edges, thereby leads to a signiﬁcant improvement.},
	language = {en},
	urldate = {2019-05-01},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Lee, Kyoungoh and Lee, Inwoong and Lee, Sanghoon},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01234-2_8},
	pages = {123--141},
	file = {Lee et al. - 2018 - Propagating LSTM 3D Pose Estimation Based on Join.pdf:/Users/Matthias/Zotero/storage/HDNNY4MB/Lee et al. - 2018 - Propagating LSTM 3D Pose Estimation Based on Join.pdf:application/pdf}
}

@inproceedings{pavllo_3d_2019,
	title = {3D human pose estimation in video with temporal convolutions and semi-supervised training},
	url = {http://arxiv.org/abs/1811.11742},
	abstract = {In this work, we demonstrate that 3D poses in video can be effectively estimated with a fully convolutional model based on dilated temporal convolutions over 2D keypoints. We also introduce back-projection, a simple and effective semi-supervised training method that leverages unlabeled video data. We start with predicted 2D keypoints for unlabeled video, then estimate 3D poses and finally back-project to the input 2D keypoints. In the supervised setting, our fully-convolutional model outperforms the previous best result from the literature by 6 mm mean per-joint position error on Human3.6M, corresponding to an error reduction of 11\%, and the model also shows significant improvements on HumanEva-I. Moreover, experiments with back-projection show that it comfortably outperforms previous state-of-the-art results in semi-supervised settings where labeled data is scarce. Code and models are available at https://github.com/facebookresearch/VideoPose3D},
	urldate = {2019-05-01},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Pavllo, Dario and Feichtenhofer, Christoph and Grangier, David and Auli, Michael},
	year = {2019},
	note = {arXiv: 1811.11742},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2019},
	file = {arXiv\:1811.11742 PDF:/Users/Matthias/Zotero/storage/XV86KFQZ/Pavllo et al. - 2018 - 3D human pose estimation in video with temporal co.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/Y3MJI289/1811.html:text/html}
}

@inproceedings{pavlakos_coarse--fine_2017,
	address = {Honolulu, HI},
	title = {Coarse-to-{Fine} {Volumetric} {Prediction} for {Single}-{Image} 3D {Human} {Pose}},
	isbn = {978-1-5386-0457-1},
	url = {http://ieeexplore.ieee.org/document/8099622/},
	doi = {10.1109/CVPR.2017.139},
	abstract = {This paper addresses the challenge of 3D human pose estimation from a single color image. Despite the general success of the end-to-end learning paradigm, top performing approaches employ a two-step solution consisting of a Convolutional Network (ConvNet) for 2D joint localization and a subsequent optimization step to recover 3D pose. In this paper, we identify the representation of 3D pose as a critical issue with current ConvNet approaches and make two important contributions towards validating the value of end-to-end learning for this task. First, we propose a ﬁne discretization of the 3D space around the subject and train a ConvNet to predict per voxel likelihoods for each joint. This creates a natural representation for 3D pose and greatly improves performance over the direct regression of joint coordinates. Second, to further improve upon initial estimates, we employ a coarse-to-ﬁne prediction scheme. This step addresses the large dimensionality increase and enables iterative reﬁnement and repeated processing of the image features. The proposed approach outperforms all state-of-theart methods on standard benchmarks achieving a relative error reduction greater than 30\% on average. Additionally, we investigate using our volumetric representation in a related architecture which is suboptimal compared to our endto-end approach, but is of practical interest, since it enables training when no image with corresponding 3D groundtruth is available, and allows us to present compelling results for in-the-wild images.},
	language = {en},
	urldate = {2019-05-01},
	booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Pavlakos, Georgios and Zhou, Xiaowei and Derpanis, Konstantinos G. and Daniilidis, Kostas},
	month = jul,
	year = {2017},
	pages = {1263--1272},
	file = {Pavlakos et al. - 2017 - Coarse-to-Fine Volumetric Prediction for Single-Im.pdf:/Users/Matthias/Zotero/storage/K5U5N76R/Pavlakos et al. - 2017 - Coarse-to-Fine Volumetric Prediction for Single-Im.pdf:application/pdf}
}

@article{feldhorst_human_2016,
	title = {Human {Activity} {Recognition} in der {Kommissionierung} – {Charakterisierung} des {Kommissionierprozesses} als {Ausgangsbasis} für die {Methodenentwicklung}},
	copyright = {fDPPL},
	url = {http://www.logistics-journal.de/proceedings/2016/fachkolloquium2016/4473},
	doi = {10.2195/lj_proc_feldhorst_de_201610_01},
	abstract = {While human activity recognition (HAR) has been successfully deployed to different application sce-narios in terms of every day or sport activities, it is rather new in the field of industrial order picking which is an important process in the field of logistics. Consequently, there are currently no dedicated datasets nor activity taxonomies available to build HAR solutions upon. To overcome this and to allow for the development of specialized HAR-approaches for order picking scenarios, the following contribution presents results of comprehensive field studies carried out in real world systems. During this studies measurements of inertial sensors as well as con-text information were gathered and the process was recorded on video for a later analysis.},
	language = {de},
	urldate = {2019-05-01},
	journal = {Volume 2016},
	author = {Feldhorst, Sascha and Aniol, Sandra and Ten Hompel, Michael},
	year = {2016},
	keywords = {620, Aktivitätserkennung, Datensätze, Feldstudien, HAR (Human Activity Recognition), Kommissionierung},
	pages = {Issue 10},
	annote = {SeriesInformation
Volume 2016, Issue 10},
	file = {Feldhorst et al. - 2016 - Human Activity Recognition in der Kommissionierung.pdf:/Users/Matthias/Zotero/storage/2VCGQRMY/Feldhorst et al. - 2016 - Human Activity Recognition in der Kommissionierung.pdf:application/pdf}
}

@article{ronchi_its_2018,
	title = {It's all {Relative}: {Monocular} 3D {Human} {Pose} {Estimation} from {Weakly} {Supervised} {Data}},
	shorttitle = {It's all {Relative}},
	url = {http://arxiv.org/abs/1805.06880},
	abstract = {We address the problem of 3D human pose estimation from 2D input images using only weakly supervised training data. Despite showing considerable success for 2D pose estimation, the application of supervised machine learning to 3D pose estimation in real world images is currently hampered by the lack of varied training images with corresponding 3D poses. Most existing 3D pose estimation algorithms train on data that has either been collected in carefully controlled studio settings or has been generated synthetically. Instead, we take a different approach, and propose a 3D human pose estimation algorithm that only requires relative estimates of depth at training time. Such training signal, although noisy, can be easily collected from crowd annotators, and is of sufficient quality for enabling successful training and evaluation of 3D pose algorithms. Our results are competitive with fully supervised regression based approaches on the Human3.6M dataset, despite using significantly weaker training data. Our proposed algorithm opens the door to using existing widespread 2D datasets for 3D pose estimation by allowing fine-tuning with noisy relative constraints, resulting in more accurate 3D poses.},
	urldate = {2019-05-01},
	journal = {arXiv:1805.06880 [cs]},
	author = {Ronchi, Matteo Ruggero and Mac Aodha, Oisin and Eng, Robert and Perona, Pietro},
	month = may,
	year = {2018},
	note = {arXiv: 1805.06880},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: BMVC 2018. Project page available at http://www.vision.caltech.edu/{\textasciitilde}mronchi/projects/RelativePose},
	file = {arXiv\:1805.06880 PDF:/Users/Matthias/Zotero/storage/75XSYMMB/Ronchi et al. - 2018 - It's all Relative Monocular 3D Human Pose Estimat.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/ZA2JHYGF/1805.html:text/html}
}

@inproceedings{zhou_towards_2017,
	address = {Venice},
	title = {Towards 3D {Human} {Pose} {Estimation} in the {Wild}: {A} {Weakly}-{Supervised} {Approach}},
	isbn = {978-1-5386-1032-9},
	shorttitle = {Towards 3D {Human} {Pose} {Estimation} in the {Wild}},
	url = {http://ieeexplore.ieee.org/document/8237313/},
	doi = {10.1109/ICCV.2017.51},
	abstract = {In this paper, we study the task of 3D human pose estimation in the wild. This task is challenging due to lack of training data, as existing datasets are either in the wild images with 2D pose or in the lab images with 3D pose.},
	language = {en},
	urldate = {2019-05-01},
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhou, Xingyi and Huang, Qixing and Sun, Xiao and Xue, Xiangyang and Wei, Yichen},
	month = oct,
	year = {2017},
	pages = {398--407},
	file = {Zhou et al. - 2017 - Towards 3D Human Pose Estimation in the Wild A We.pdf:/Users/Matthias/Zotero/storage/VZHCF8W7/Zhou et al. - 2017 - Towards 3D Human Pose Estimation in the Wild A We.pdf:application/pdf}
}

@article{fang_learning_2017,
	title = {Learning {Pose} {Grammar} to {Encode} {Human} {Body} {Configuration} for 3D {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1710.06513},
	abstract = {In this paper, we propose a pose grammar to tackle the problem of 3D human pose estimation. Our model directly takes 2D pose as input and learns a generalized 2D-3D mapping function. The proposed model consists of a base network which efficiently captures pose-aligned features and a hierarchy of Bi-directional RNNs (BRNN) on the top to explicitly incorporate a set of knowledge regarding human body configuration (i.e., kinematics, symmetry, motor coordination). The proposed model thus enforces high-level constraints over human poses. In learning, we develop a pose sample simulator to augment training samples in virtual camera views, which further improves our model generalizability. We validate our method on public 3D human pose benchmarks and propose a new evaluation protocol working on cross-view setting to verify the generalization capability of different methods. We empirically observe that most state-of-the-art methods encounter difficulty under such setting while our method can well handle such challenges.},
	urldate = {2019-05-01},
	journal = {arXiv:1710.06513 [cs]},
	author = {Fang, Haoshu and Xu, Yuanlu and Wang, Wenguan and Liu, Xiaobai and Zhu, Song-Chun},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.06513},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted by AAAI 2018},
	file = {arXiv\:1710.06513 PDF:/Users/Matthias/Zotero/storage/6B323LJL/Fang et al. - 2017 - Learning Pose Grammar to Encode Human Body Configu.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/YUY3KQHD/1710.html:text/html}
}

@inproceedings{yang_3d_2018,
	address = {Salt Lake City, UT, USA},
	title = {3D {Human} {Pose} {Estimation} in the {Wild} by {Adversarial} {Learning}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578649/},
	doi = {10.1109/CVPR.2018.00551},
	abstract = {Recently, remarkable advances have been achieved in 3D human pose estimation from monocular images because of the powerful Deep Convolutional Neural Networks (DCNNs). Despite their success on large-scale datasets collected in the constrained lab environment, it is difﬁcult to obtain the 3D pose annotations for in-the-wild images. Therefore, 3D human pose estimation in the wild is still a challenge. In this paper, we propose an adversarial learning framework, which distills the 3D human pose structures learned from the fully annotated dataset to in-the-wild images with only 2D pose annotations. Instead of deﬁning hard-coded rules to constrain the pose estimation results, we design a novel multi-source discriminator to distinguish the predicted 3D poses from the ground-truth, which helps to enforce the pose estimator to generate anthropometrically valid poses even with images in the wild. We also observe that a carefully designed information source for the discriminator is essential to boost the performance. Thus, we design a geometric descriptor, which computes the pairwise relative locations and distances between body joints, as a new information source for the discriminator. The efﬁcacy of our adversarial learning framework with the new geometric descriptor has been demonstrated through extensive experiments on widely used public benchmarks. Our approach signiﬁcantly improves the performance compared with previous state-of-the-art approaches.},
	language = {en},
	urldate = {2019-05-01},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Yang, Wei and Ouyang, Wanli and Wang, Xiaolong and Ren, Jimmy and Li, Hongsheng and Wang, Xiaogang},
	month = jun,
	year = {2018},
	pages = {5255--5264},
	file = {Yang et al. - 2018 - 3D Human Pose Estimation in the Wild by Adversaria.pdf:/Users/Matthias/Zotero/storage/TB4K635W/Yang et al. - 2018 - 3D Human Pose Estimation in the Wild by Adversaria.pdf:application/pdf}
}

@article{rogez_lcr-net++:_2019,
	title = {{LCR}-{Net}++: {Multi}-person 2D and 3D {Pose} {Detection} in {Natural} {Images}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {{LCR}-{Net}++},
	url = {http://arxiv.org/abs/1803.00455},
	doi = {10.1109/TPAMI.2019.2892985},
	abstract = {We propose an end-to-end architecture for joint 2D and 3D human pose estimation in natural images. Key to our approach is the generation and scoring of a number of pose proposals per image, which allows us to predict 2D and 3D poses of multiple people simultaneously. Hence, our approach does not require an approximate localization of the humans for initialization. Our Localization-Classification-Regression architecture, named LCR-Net, contains 3 main components: 1) the pose proposal generator that suggests candidate poses at different locations in the image; 2) a classifier that scores the different pose proposals; and 3) a regressor that refines pose proposals both in 2D and 3D. All three stages share the convolutional feature layers and are trained jointly. The final pose estimation is obtained by integrating over neighboring pose hypotheses, which is shown to improve over a standard non maximum suppression algorithm. Our method recovers full-body 2D and 3D poses, hallucinating plausible body parts when the persons are partially occluded or truncated by the image boundary. Our approach significantly outperforms the state of the art in 3D pose estimation on Human3.6M, a controlled environment. Moreover, it shows promising results on real images for both single and multi-person subsets of the MPII 2D pose benchmark and demonstrates satisfying 3D pose results even for multi-person images.},
	urldate = {2019-05-01},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Rogez, Gregory and Weinzaepfel, Philippe and Schmid, Cordelia},
	year = {2019},
	note = {arXiv: 1803.00455},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {1--1},
	annote = {Comment: journal version of the CVPR 2017 paper, accepted to appear in IEEE Trans. PAMI},
	file = {arXiv\:1803.00455 PDF:/Users/Matthias/Zotero/storage/AD5GCHCJ/Rogez et al. - 2019 - LCR-Net++ Multi-person 2D and 3D Pose Detection i.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/VIHI6XL3/1803.html:text/html}
}

@article{han_space-time_2016,
	title = {Space-{Time} {Representation} of {People} {Based} on 3D {Skeletal} {Data}: {A} {Review}},
	shorttitle = {Space-{Time} {Representation} of {People} {Based} on 3D {Skeletal} {Data}},
	url = {http://arxiv.org/abs/1601.01006},
	abstract = {Spatiotemporal human representation based on 3D visual perception data is a rapidly growing research area. Based on the information sources, these representations can be broadly categorized into two groups based on RGB-D information or 3D skeleton data. Recently, skeleton-based human representations have been intensively studied and kept attracting an increasing attention, due to their robustness to variations of viewpoint, human body scale and motion speed as well as the realtime, online performance. This paper presents a comprehensive survey of existing space-time representations of people based on 3D skeletal data, and provides an informative categorization and analysis of these methods from the perspectives, including information modality, representation encoding, structure and transition, and feature engineering. We also provide a brief overview of skeleton acquisition devices and construction methods, enlist a number of public benchmark datasets with skeleton data, and discuss potential future research directions.},
	urldate = {2019-05-03},
	journal = {arXiv:1601.01006 [cs]},
	author = {Han, Fei and Reily, Brian and Hoff, William and Zhang, Hao},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.01006},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Our paper has been accepted by the journal Computer Vision and Image Understanding, see http://www.sciencedirect.com/science/article/pii/S1077314217300279, Computer Vision and Image Understanding, 2017},
	file = {arXiv\:1601.01006 PDF:/Users/Matthias/Zotero/storage/CTYIZG6A/Han et al. - 2016 - Space-Time Representation of People Based on 3D Sk.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/L557JXTW/1601.html:text/html}
}

@inproceedings{he_mask_2017,
	title = {Mask {R}-{CNN}},
	doi = {10.1109/ICCV.2017.322},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.},
	booktitle = {{IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {He, K. and Gkioxari, G. and Dollár, P. and Girshick, R.},
	year = {2017},
	keywords = {pose estimation, image segmentation, feature extraction, Feature extraction, object detection, Semantics, bounding-box object detection, called Mask R-CNN, conceptually simple framework, Faster R-CNN, high-quality segmentation mask, Image segmentation, Object detection, object instance segmentation, object mask, Quantization (signal), Robustness},
	pages = {2980--2988},
	file = {He et al. - 2017 - Mask R-CNN.pdf:/Users/Matthias/Zotero/storage/CSVAFRTR/He et al. - 2017 - Mask R-CNN.pdf:application/pdf}
}

@inproceedings{rueda_learning_2018,
	title = {Learning {Attribute} {Representation} for {Human} {Activity} {Recognition}},
	doi = {10.1109/ICPR.2018.8545146},
	abstract = {Attribute representations became relevant in image recognition and word spotting, providing support under the presence of unbalance and disjoint datasets. However, for human activity recognition using sequential data from on-body sensors, human-labeled attributes are lacking. This paper introduces a search for attributes that represent favorably signal segments for recognizing human activities. It presents three deep architectures, including temporal-convolutions and an IMU centered design, for predicting attributes. An empiric evaluation of random and learned attribute representations, and as well as the networks is carried out on two datasets, outperforming the state-of-the art.},
	booktitle = {2018 24th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Rueda, F. M. and Fink, G. A.},
	month = aug,
	year = {2018},
	keywords = {image representation, image motion analysis, image segmentation, attribute representation, Computer architecture, human activity recognition, image recognition, Task analysis, Feature extraction, image sequences, learning (artificial intelligence), Activity recognition, disjoint datasets, Foot, human-labeled attributes, IMU centered design, on-body sensors, signal segments, temporal-convolutions, Time series analysis, word spotting},
	pages = {523--528},
	file = {IEEE Xplore Abstract Record:/Users/Matthias/Zotero/storage/VC4858KI/8545146.html:text/html;IEEE Xplore Full Text PDF:/Users/Matthias/Zotero/storage/JSKBFP8A/Rueda and Fink - 2018 - Learning Attribute Representation for Human Activi.pdf:application/pdf}
}

@article{hammerla_deep_2016,
	title = {Deep, {Convolutional}, and {Recurrent} {Models} for {Human} {Activity} {Recognition} using {Wearables}},
	url = {http://arxiv.org/abs/1604.08880},
	abstract = {Human activity recognition (HAR) in ubiquitous computing is beginning to adopt deep learning to substitute for well-established analysis techniques that rely on hand-crafted feature extraction and classification techniques. From these isolated applications of custom deep architectures it is, however, difficult to gain an overview of their suitability for problems ranging from the recognition of manipulative gestures to the segmentation and identification of physical activities like running or ascending stairs. In this paper we rigorously explore deep, convolutional, and recurrent approaches across three representative datasets that contain movement data captured with wearable sensors. We describe how to train recurrent approaches in this setting, introduce a novel regularisation approach, and illustrate how they outperform the state-of-the-art on a large benchmark dataset. Across thousands of recognition experiments with randomly sampled model configurations we investigate the suitability of each model for different tasks in HAR, explore the impact of hyperparameters using the fANOVA framework, and provide guidelines for the practitioner who wants to apply deep learning in their problem setting.},
	urldate = {2019-05-03},
	journal = {arXiv:1604.08880 [cs, stat]},
	author = {Hammerla, Nils Y. and Halloran, Shane and Ploetz, Thomas},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.08880},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Extended version has been accepted for publication at International Joint Conference on Artificial Intelligence (IJCAI)},
	file = {arXiv\:1604.08880 PDF:/Users/Matthias/Zotero/storage/5R74EMIW/Hammerla et al. - 2016 - Deep, Convolutional, and Recurrent Models for Huma.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/GEWLUX24/1604.html:text/html}
}

@article{ordonez_deep_2016,
	title = {Deep {Convolutional} and {LSTM} {Recurrent} {Neural} {Networks} for {Multimodal} {Wearable} {Activity} {Recognition}},
	volume = {16},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/16/1/115},
	doi = {10.3390/s16010115},
	abstract = {Human activity recognition (HAR) tasks have traditionally been solved using engineered features obtained by heuristic processes. Current research suggests that deep convolutional neural networks are suited to automate feature extraction from raw sensor inputs. However, human activities are made of complex sequences of motor movements, and capturing this temporal dynamics is fundamental for successful HAR. Based on the recent success of recurrent neural networks for time series domains, we propose a generic deep framework for activity recognition based on convolutional and LSTM recurrent units, which: (i) is suitable for multimodal wearable sensors; (ii) can perform sensor fusion naturally; (iii) does not require expert knowledge in designing features; and (iv) explicitly models the temporal dynamics of feature activations. We evaluate our framework on two datasets, one of which has been used in a public activity recognition challenge. Our results show that our framework outperforms competing deep non-recurrent networks on the challenge dataset by 4\% on average; outperforming some of the previous reported results by up to 9\%. Our results show that the framework can be applied to homogeneous sensor modalities, but can also fuse multimodal sensors to improve performance. We characterise key architectural hyperparameters’ influence on performance to provide insights about their optimisation.},
	language = {en},
	number = {1},
	urldate = {2019-05-03},
	journal = {Sensors},
	author = {Ordóñez, Francisco Javier and Roggen, Daniel},
	month = jan,
	year = {2016},
	keywords = {human activity recognition, deep learning, LSTM, machine learning, neural network, sensor fusion, wearable sensors},
	pages = {115},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/977BV59L/Ordóñez and Roggen - 2016 - Deep Convolutional and LSTM Recurrent Neural Netwo.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/CSZEEVZF/htm.html:text/html}
}

@inproceedings{grzeszick_deep_2017,
	address = {Rostock, Germany},
	title = {Deep {Neural} {Network} based {Human} {Activity} {Recognition} for the {Order} {Picking} {Process}},
	isbn = {978-1-4503-5223-9},
	url = {http://dl.acm.org/citation.cfm?doid=3134230.3134231},
	doi = {10.1145/3134230.3134231},
	abstract = {Although the fourth industrial revolution is already in pro-gress and advances have been made in automating factories, completely automated facilities are still far in the future. Human work is still an important factor in many factories and warehouses, especially in the ﬁeld of logistics. Manual processes are, therefore, often subject to optimization efforts. In order to aid these optimization efforts, methods like human activity recognition (HAR) became of increasing interest in industrial settings. In this work a novel deep neural network architecture for HAR is introduced. A convolutional neural network (CNN), which employs temporal convolutions, is applied to the sequential data of multiple intertial measurement units (IMUs). The network is designed to separately handle different sensor values and IMUs, joining the information step-by-step within the architecture. An evaluation is performed using data from the order picking process recorded in two different warehouses. The inﬂuence of different design choices in the network architecture, as well as pre- and post-processing, will be evaluated. Crucial steps for learning a good classiﬁcation network for the task of HAR in a complex industrial setting will be shown. Ultimately, it can be shown that traditional approaches based on statistical features as well as recent CNN architectures are outperformed.},
	language = {en},
	urldate = {2019-05-03},
	booktitle = {Proceedings of the 4th international {Workshop} on {Sensor}-based {Activity} {Recognition} and {Interaction}  - {iWOAR} '17},
	publisher = {ACM Press},
	author = {Grzeszick, Rene and Lenk, Jan Marius and Rueda, Fernando Moya and Fink, Gernot A. and Feldhorst, Sascha and ten Hompel, Michael},
	year = {2017},
	pages = {1--6},
	file = {Grzeszick et al. - 2017 - Deep Neural Network based Human Activity Recogniti.pdf:/Users/Matthias/Zotero/storage/CRZV79NB/Grzeszick et al. - 2017 - Deep Neural Network based Human Activity Recogniti.pdf:application/pdf}
}

@inproceedings{kuehne_hmdb:_2011,
	address = {Barcelona, ES},
	title = {{HMDB}: {A} large video database for human motion recognition},
	abstract = {With nearly one billion online videos viewed everyday, an emerging new frontier in computer vision research is recognition and search in video. While much effort has been devoted to the collection and annotation of large scalable static image datasets containing thousands of image categories, human action datasets lag far behind. Current action recognition databases contain on the order of ten different action categories collected under fairly controlled conditions. State-of-the-art performance on these datasets is now near ceiling and thus there is a need for the design and creation of new benchmarks. To address this issue we collected the largest action video database to-date with 51 action categories, which in total contain around 7,000 manually annotated clips extracted from a variety of sources ranging from digitized movies to YouTube. We use this database to evaluate the performance of two representative computer vision systems for action recognition and explore the robustness of these methods under various conditions such as camera motion, viewpoint, video quality and occlusion.},
	booktitle = {Proceedings of the 2011 {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Kuehne, Hildegard and Jhuang, Hueihan and Garrote, E. and Poggio, Tomaso and Serre, Thomas},
	month = nov,
	year = {2011},
	keywords = {action recognition databases, camera motion, Cameras, computer vision research, Databases, digitized movies, HMDB, human action datasets, human motion recognition, Humans, image categories, image motion analysis, large video database, Motion pictures, object recognition, occlusion, social networking (online), static image datasets, Training, video databases, video quality, viewpoint, Visualization, YouTube},
	pages = {2556--2563},
	file = {IEEE Xplore Abstract Record:/Users/Matthias/Zotero/storage/4KRZLVWR/6126543.html:text/html;IEEE Xplore Full Text PDF:/Users/Matthias/Zotero/storage/LQ7EL5BV/Kuehne et al. - 2011 - HMDB A large video database for human motion reco.pdf:application/pdf}
}

@article{yeung_every_2018,
	title = {Every {Moment} {Counts}: {Dense} {Detailed} {Labeling} of {Actions} in {Complex} {Videos}},
	volume = {126},
	issn = {1573-1405},
	shorttitle = {Every {Moment} {Counts}},
	url = {https://doi.org/10.1007/s11263-017-1013-y},
	doi = {10.1007/s11263-017-1013-y},
	abstract = {Every moment counts in action recognition. A comprehensive understanding of human activity in video requires labeling every frame according to the actions occurring, placing multiple labels densely over a video sequence. To study this problem we extend the existing THUMOS dataset and introduce MultiTHUMOS, a new dataset of dense labels over unconstrained internet videos. Modeling multiple, dense labels benefits from temporal relations within and across classes. We define a novel variant of long short-term memory deep networks for modeling these temporal relations via multiple input and output connections. We show that this model improves action labeling accuracy and further enables deeper understanding tasks ranging from structured retrieval to action prediction.},
	language = {en},
	number = {2},
	urldate = {2019-05-06},
	journal = {International Journal of Computer Vision},
	author = {Yeung, Serena and Russakovsky, Olga and Jin, Ning and Andriluka, Mykhaylo and Mori, Greg and Fei-Fei, Li},
	month = apr,
	year = {2018},
	pages = {375--389},
	file = {Springer Full Text PDF:/Users/Matthias/Zotero/storage/CI73ACVJ/Yeung et al. - 2018 - Every Moment Counts Dense Detailed Labeling of Ac.pdf:application/pdf}
}

@inproceedings{yeung_end--end_2016,
	address = {Las Vegas, NV, USA},
	title = {End-to-{End} {Learning} of {Action} {Detection} from {Frame} {Glimpses} in {Videos}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780662/},
	doi = {10.1109/CVPR.2016.293},
	abstract = {In this work we introduce a fully end-to-end approach for action detection in videos that learns to directly predict the temporal bounds of actions. Our intuition is that the process of detecting actions is naturally one of observation and reﬁnement: observing moments in video, and reﬁning hypotheses about when an action is occurring. Based on this insight, we formulate our model as a recurrent neural network-based agent that interacts with a video over time. The agent observes video frames and decides both where to look next and when to emit a prediction. Since backpropagation is not adequate in this non-differentiable setting, we use REINFORCE to learn the agent’s decision policy. Our model achieves state-of-the-art results on the THUMOS’14 and ActivityNet datasets while observing only a fraction (2\% or less) of the video frames.},
	language = {en},
	urldate = {2019-05-06},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Yeung, Serena and Russakovsky, Olga and Mori, Greg and Fei-Fei, Li},
	month = jun,
	year = {2016},
	pages = {2678--2687},
	file = {Yeung et al. - 2016 - End-to-End Learning of Action Detection from Frame.pdf:/Users/Matthias/Zotero/storage/TSH38YYB/Yeung et al. - 2016 - End-to-End Learning of Action Detection from Frame.pdf:application/pdf}
}

@inproceedings{zhang_actemes_2013-1,
	address = {Sydney, Australia},
	title = {From {Actemes} to {Action}: {A} {Strongly}-{Supervised} {Representation} for {Detailed} {Action} {Understanding}},
	isbn = {978-1-4799-2840-8},
	shorttitle = {From {Actemes} to {Action}},
	url = {http://ieeexplore.ieee.org/document/6751390/},
	doi = {10.1109/ICCV.2013.280},
	abstract = {This paper presents a novel approach for analyzing human actions in non-scripted, unconstrained video settings based on volumetric, x-y-t, patch classiﬁers, termed actemes. Unlike previous action-related work, the discovery of patch classiﬁers is posed as a strongly-supervised process. Speciﬁcally, keypoint labels (e.g., position) across spacetime are used in a data-driven training process to discover patches that are highly clustered in the spacetime keypoint conﬁguration space. To support this process, a new human action dataset consisting of challenging consumer videos is introduced, where notably the action label, the 2D position of a set of keypoints and their visibilities are provided for each video frame. On a novel input video, each acteme is used in a sliding volume scheme to yield a set of sparse, non-overlapping detections. Thessegemdenetatetiocntions provide the intermediate substrate for segmenting out the action. For action classiﬁcation, the proposed representation shows signiﬁcant improvement over state-of-the-art low-level features, while providing spatiotemporal localization as additional output. This output sheds further light into detailed action understanding.},
	language = {en},
	urldate = {2019-05-06},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Zhang, Weiyu and Zhu, Menglong and Derpanis, Konstantinos G.},
	month = dec,
	year = {2013},
	pages = {2248--2255},
	file = {Zhang et al. - 2013 - From Actemes to Action A Strongly-Supervised Repr.pdf:/Users/Matthias/Zotero/storage/SN8SG9TS/Zhang et al. - 2013 - From Actemes to Action A Strongly-Supervised Repr.pdf:application/pdf}
}

@inproceedings{shahroudy_ntu_2016,
	title = {{NTU} {RGB}+{D}: {A} {Large} {Scale} {Dataset} for 3D {Human} {Activity} {Analysis}},
	booktitle = {The {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Shahroudy, Amir and Liu, Jun and Ng, Tian-Tsong and Wang, Gang},
	month = jun,
	year = {2016}
}

@article{zhang_view_2019,
	title = {View {Adaptive} {Neural} {Networks} for {High} {Performance} {Skeleton}-based {Human} {Action} {Recognition}},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2019.2896631},
	abstract = {Skeleton-based human action recognition has recently attracted increasing attention thanks to the accessibility and the popularity of 3D skeleton data. One of the key challenges in skeleton-based action recognition lies in the large view variations when capturing data. In order to alleviate the effects of view variations, this paper introduces a novel view adaptation scheme, which automatically determines the virtual observation viewpoints in a learning based data driven manner. We design two view adaptive neural networks, i.e., VA-RNN based on RNNand VA-CNN based on CNN. For each network, a novel view adaptation module learns and determines the most suitable observation viewpoints, and transforms the skeletons to those viewpoints for the end-to-end recognition with a main classification network. Ablation studies find that the proposed view adaptive models are capable of transforming the skeletons of various viewpoints to much more consistent virtual viewpoints which largely eliminates the viewpoint influence. In addition, we design a two-stream scheme (referred to as VA-fusion) that fuses the scores of the two networks to provide the fused prediction. Extensive experimental evaluations on six challenging benchmarks demonstrate the effectiveness of the proposed view-adaptive networks and superior performance over state-of-the-art approaches.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Zhang, P. and Lan, C. and Xing, J. and Zeng, W. and Xue, J. and Zheng, N.},
	year = {2019},
	keywords = {action recognition, Skeleton, Three-dimensional displays, Cameras, CNN, Adaptation models, Adaptive systems, consistent, Recurrent neural networks, RNN, skeleton, View adaptation},
	pages = {1--1},
	file = {IEEE Xplore Abstract Record:/Users/Matthias/Zotero/storage/TS7EAGT7/8630687.html:text/html;IEEE Xplore Full Text PDF:/Users/Matthias/Zotero/storage/DV9D52KH/Zhang et al. - 2019 - View Adaptive Neural Networks for High Performance.pdf:application/pdf}
}

@inproceedings{weinland_making_2010,
	title = {Making action recognition robust to occlusions and viewpoint changes},
	booktitle = {European {Conference} on {Computer} {Vision}},
	publisher = {Springer},
	author = {Weinland, Daniel and Özuysal, Mustafa and Fua, Pascal},
	year = {2010},
	pages = {635--648},
	file = {Weinland et al. - 2010 - Making action recognition robust to occlusions and.pdf:/Users/Matthias/Zotero/storage/H4C9CYE5/Weinland et al. - 2010 - Making action recognition robust to occlusions and.pdf:application/pdf}
}

@article{sigurdsson_hollywood_2016,
	title = {Hollywood in {Homes}: {Crowdsourcing} {Data} {Collection} for {Activity} {Understanding}},
	shorttitle = {Hollywood in {Homes}},
	url = {http://arxiv.org/abs/1604.01753},
	abstract = {Computer vision has a great potential to help our daily lives by searching for lost keys, watering flowers or reminding us to take a pill. To succeed with such tasks, computer vision methods need to be trained from real and diverse examples of our daily dynamic scenes. While most of such scenes are not particularly exciting, they typically do not appear on YouTube, in movies or TV broadcasts. So how do we collect sufficiently many diverse but boring samples representing our lives? We propose a novel Hollywood in Homes approach to collect such data. Instead of shooting videos in the lab, we ensure diversity by distributing and crowdsourcing the whole process of video creation from script writing to video recording and annotation. Following this procedure we collect a new dataset, Charades, with hundreds of people recording videos in their own homes, acting out casual everyday activities. The dataset is composed of 9,848 annotated videos with an average length of 30 seconds, showing activities of 267 people from three continents. Each video is annotated by multiple free-text descriptions, action labels, action intervals and classes of interacted objects. In total, Charades provides 27,847 video descriptions, 66,500 temporally localized intervals for 157 action classes and 41,104 labels for 46 object classes. Using this rich data, we evaluate and provide baseline results for several tasks including action recognition and automatic description generation. We believe that the realism, diversity, and casual nature of this dataset will present unique challenges and new opportunities for computer vision community.},
	urldate = {2019-05-06},
	journal = {arXiv:1604.01753 [cs]},
	author = {Sigurdsson, Gunnar A. and Varol, Gül and Wang, Xiaolong and Farhadi, Ali and Laptev, Ivan and Gupta, Abhinav},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.01753},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1604.01753 PDF:/Users/Matthias/Zotero/storage/7728Q7KX/Sigurdsson et al. - 2016 - Hollywood in Homes Crowdsourcing Data Collection .pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/7QP88EFW/1604.html:text/html}
}

@article{tran_closer_2017,
	title = {A {Closer} {Look} at {Spatiotemporal} {Convolutions} for {Action} {Recognition}},
	url = {http://arxiv.org/abs/1711.11248},
	abstract = {In this paper we discuss several forms of spatiotemporal convolutions for video analysis and study their effects on action recognition. Our motivation stems from the observation that 2D CNNs applied to individual frames of the video have remained solid performers in action recognition. In this work we empirically demonstrate the accuracy advantages of 3D CNNs over 2D CNNs within the framework of residual learning. Furthermore, we show that factorizing the 3D convolutional filters into separate spatial and temporal components yields significantly advantages in accuracy. Our empirical study leads to the design of a new spatiotemporal convolutional block "R(2+1)D" which gives rise to CNNs that achieve results comparable or superior to the state-of-the-art on Sports-1M, Kinetics, UCF101 and HMDB51.},
	urldate = {2019-05-06},
	journal = {arXiv:1711.11248 [cs]},
	author = {Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.11248},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1711.11248 PDF:/Users/Matthias/Zotero/storage/RM4D5683/Tran et al. - 2017 - A Closer Look at Spatiotemporal Convolutions for A.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/3JL58KEL/1711.html:text/html}
}

@inproceedings{wang_video_2018,
	address = {Salt Lake City, UT},
	title = {Video {Representation} {Learning} {Using} {Discriminative} {Pooling}},
	isbn = {978-1-5386-6420-9},
	url = {https://ieeexplore.ieee.org/document/8578224/},
	doi = {10.1109/CVPR.2018.00126},
	abstract = {Popular deep models for action recognition in videos generate independent predictions for short clips, which are then pooled heuristically to assign an action label to the full video segment. As not all frames may characterize the underlying action—indeed, many are common across multiple actions—pooling schemes that impose equal importance on all frames might be unfavorable. In an attempt to tackle this problem, we propose discriminative pooling, based on the notion that among the deep features generated on all short clips, there is at least one that characterizes the action. To this end, we learn a (nonlinear) hyperplane that separates this unknown, yet discriminative, feature from the rest. Applying multiple instance learning in a large-margin setup, we use the parameters of this separating hyperplane as a descriptor for the full video segment. Since these parameters are directly related to the support vectors in a maxmargin framework, they serve as robust representations for pooling of the features. We formulate a joint objective and an efﬁcient solver that learns these hyperplanes per video and the corresponding action classiﬁers over the hyperplanes. Our pooling scheme is end-to-end trainable within a deep framework. We report results from experiments on three benchmark datasets spanning a variety of challenges and demonstrate state-of-the-art performance across these tasks.},
	language = {en},
	urldate = {2019-05-06},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Wang, Jue and Cherian, Anoop and Porikli, Fatih and Gould, Stephen},
	month = jun,
	year = {2018},
	pages = {1149--1158},
	file = {Wang et al. - 2018 - Video Representation Learning Using Discriminative.pdf:/Users/Matthias/Zotero/storage/NSN49N5Y/Wang et al. - 2018 - Video Representation Learning Using Discriminative.pdf:application/pdf}
}

@article{mehta_single-shot_2017,
	title = {Single-{Shot} {Multi}-{Person} 3D {Pose} {Estimation} {From} {Monocular} {RGB}},
	url = {http://arxiv.org/abs/1712.03453},
	abstract = {We propose a new single-shot method for multi-person 3D pose estimation in general scenes from a monocular RGB camera. Our approach uses novel occlusion-robust pose-maps (ORPM) which enable full body pose inference even under strong partial occlusions by other people and objects in the scene. ORPM outputs a fixed number of maps which encode the 3D joint locations of all people in the scene. Body part associations allow us to infer 3D pose for an arbitrary number of people without explicit bounding box prediction. To train our approach we introduce MuCo-3DHP, the first large scale training data set showing real images of sophisticated multi-person interactions and occlusions. We synthesize a large corpus of multi-person images by compositing images of individual people (with ground truth from mutli-view performance capture). We evaluate our method on our new challenging 3D annotated multi-person test set MuPoTs-3D where we achieve state-of-the-art performance. To further stimulate research in multi-person 3D pose estimation, we will make our new datasets, and associated code publicly available for research purposes.},
	urldate = {2019-05-06},
	journal = {arXiv:1712.03453 [cs]},
	author = {Mehta, Dushyant and Sotnychenko, Oleksandr and Mueller, Franziska and Xu, Weipeng and Sridhar, Srinath and Pons-Moll, Gerard and Theobalt, Christian},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.03453},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: International Conference on 3D Vision (3DV), 2018},
	file = {arXiv\:1712.03453 PDF:/Users/Matthias/Zotero/storage/4DIPG7X7/Mehta et al. - 2017 - Single-Shot Multi-Person 3D Pose Estimation From M.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/QYJD7FNC/1712.html:text/html}
}

@article{herath_going_2016,
	title = {Going {Deeper} into {Action} {Recognition}: {A} {Survey}},
	shorttitle = {Going {Deeper} into {Action} {Recognition}},
	url = {http://arxiv.org/abs/1605.04988},
	abstract = {Understanding human actions in visual data is tied to advances in complementary research areas including object recognition, human dynamics, domain adaptation and semantic segmentation. Over the last decade, human action analysis evolved from earlier schemes that are often limited to controlled environments to nowadays advanced solutions that can learn from millions of videos and apply to almost all daily activities. Given the broad range of applications from video surveillance to human-computer interaction, scientific milestones in action recognition are achieved more rapidly, eventually leading to the demise of what used to be good in a short time. This motivated us to provide a comprehensive review of the notable steps taken towards recognizing human actions. To this end, we start our discussion with the pioneering methods that use handcrafted representations, and then, navigate into the realm of deep learning based approaches. We aim to remain objective throughout this survey, touching upon encouraging improvements as well as inevitable fallbacks, in the hope of raising fresh questions and motivating new research directions for the reader.},
	urldate = {2019-05-06},
	journal = {arXiv:1605.04988 [cs]},
	author = {Herath, Samitha and Harandi, Mehrtash and Porikli, Fatih},
	month = may,
	year = {2016},
	note = {arXiv: 1605.04988},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1605.04988 PDF:/Users/Matthias/Zotero/storage/NJ7LJUGI/Herath et al. - 2016 - Going Deeper into Action Recognition A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/JSPRXIEN/1605.html:text/html}
}

@article{nunez_convolutional_2018,
	title = {Convolutional {Neural} {Networks} and {Long} {Short}-{Term} {Memory} for skeleton-based human activity and hand gesture recognition},
	volume = {76},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320317304405},
	doi = {10.1016/j.patcog.2017.10.033},
	abstract = {In this work, we address human activity and hand gesture recognition problems using 3D data sequences obtained from full-body and hand skeletons, respectively. To this aim, we propose a deep learning-based approach for temporal 3D pose recognition problems based on a combination of a Convolutional Neural Network (CNN) and a Long Short-Term Memory (LSTM) recurrent network. We also present a two-stage training strategy which firstly focuses on CNN training and, secondly, adjusts the full method (CNN+LSTM). Experimental testing demonstrated that our training method obtains better results than a single-stage training strategy. Additionally, we propose a data augmentation method that has also been validated experimentally. Finally, we perform an extensive experimental study on publicly available data benchmarks. The results obtained show how the proposed approach reaches state-of-the-art performance when compared to the methods identified in the literature. The best results were obtained for small datasets, where the proposed data augmentation strategy has greater impact.},
	urldate = {2019-05-06},
	journal = {Pattern Recognition},
	author = {Núñez, Juan C. and Cabido, Raúl and Pantrigo, Juan J. and Montemayor, Antonio S. and Vélez, José F.},
	month = apr,
	year = {2018},
	keywords = {Convolutional Neural Network, Deep learning, Hand gesture recognition, Human activity recognition, Long Short-Term Memory, Real-time, Recurrent neural network},
	pages = {80--94},
	file = {ScienceDirect Snapshot:/Users/Matthias/Zotero/storage/GZRY8JVM/S0031320317304405.html:text/html}
}

@article{zhang_review_2017,
	title = {A {Review} on {Human} {Activity} {Recognition} {Using} {Vision}-{Based} {Method}},
	volume = {2017},
	abstract = {Human activity recognition (HAR) aims to recognize activities from a series of observations on the actions of subjects and the environmental conditions. The vision-based HAR research is the basis of many applications including video surveillance, health care, and human-computer interaction (HCI). This review highlights the advances of state-of-the-art activity recognition approaches, especially for the activity representation and classification methods. For the representation methods, we sort out a chronological research trajectory from global representations to local representations, and recent depth-based representations. For the classification methods, we conform to the categorization of template-based methods, discriminative models, and generative models and review several prevalent methods. Next, representative and available datasets are introduced. Aiming to provide an overview of those methods and a convenient way of comparing them, we classify existing literatures with a detailed taxonomy including representation and classification methods, as well as the datasets they used. Finally, we investigate the directions for future research.},
	language = {en},
	urldate = {2019-05-06},
	journal = {Journal of Healthcare Engineering},
	author = {Zhang, Shugang and Wei, Zhiqiang and Nie, Jie and Huang, Lei and Wang, Shuang and Li, Zhen},
	year = {2017},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/CYRUP4JZ/CYRUP4JZ.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/IA5P4VVW/3090343.html:application/xhtml+xml}
}

@inproceedings{yang_deep_2015,
	title = {Deep convolutional neural networks on multichannel time series for human activity recognition},
	booktitle = {Twenty-{Fourth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Yang, Jianbo and Nguyen, Minh Nhut and San, Phyo Phyo and Li, Xiao Li and Krishnaswamy, Shonali},
	year = {2015},
	file = {Yang et al. - 2015 - Deep convolutional neural networks on multichannel.pdf:/Users/Matthias/Zotero/storage/9M6326D4/Yang et al. - 2015 - Deep convolutional neural networks on multichannel.pdf:application/pdf}
}

@inproceedings{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	booktitle = {{NIPS}-{W}},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017}
}

@article{luvizon_human_2017,
	title = {Human {Pose} {Regression} by {Combining} {Indirect} {Part} {Detection} and {Contextual} {Information}},
	url = {http://arxiv.org/abs/1710.02322},
	abstract = {In this paper, we propose an end-to-end trainable regression approach for human pose estimation from still images. We use the proposed Soft-argmax function to convert feature maps directly to joint coordinates, resulting in a fully differentiable framework. Our method is able to learn heat maps representations indirectly, without additional steps of artificial ground truth generation. Consequently, contextual information can be included to the pose predictions in a seamless way. We evaluated our method on two very challenging datasets, the Leeds Sports Poses (LSP) and the MPII Human Pose datasets, reaching the best performance among all the existing regression methods and comparable results to the state-of-the-art detection based approaches.},
	urldate = {2019-05-08},
	journal = {arXiv:1710.02322 [cs]},
	author = {Luvizon, Diogo C. and Tabia, Hedi and Picard, David},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.02322},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1710.02322 PDF:/Users/Matthias/Zotero/storage/FM835WI3/Luvizon et al. - 2017 - Human Pose Regression by Combining Indirect Part D.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/9UDDM47X/1710.html:text/html}
}

@inproceedings{carreira_quo_2017,
	address = {Honolulu, HI},
	title = {Quo {Vadis}, {Action} {Recognition}? {A} {New} {Model} and the {Kinetics} {Dataset}},
	shorttitle = {Quo {Vadis}, {Action} {Recognition}?},
	url = {http://ieeexplore.ieee.org/document/8099985/},
	abstract = {The paucity of videos in current action classiﬁcation datasets (UCF-101 and HMDB-51) has made it difﬁcult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classiﬁcation on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics.},
	language = {en},
	urldate = {2019-05-08},
	booktitle = {Proceedings of the 2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Carreira, Joao and Zisserman, Andrew},
	month = jul,
	year = {2017},
	pages = {4724--4733},
	file = {Carreira and Zisserman - 2017 - Quo Vadis, Action Recognition A New Model and the.pdf:/Users/Matthias/Zotero/storage/L5JV459H/Carreira and Zisserman - 2017 - Quo Vadis, Action Recognition A New Model and the.pdf:application/pdf}
}

@inproceedings{zhou_mict:_2018,
	address = {Salt Lake City},
	title = {{MiCT}: {Mixed} 3D/2D {Convolutional} {Tube} for {Human} {Action} {Recognition}},
	isbn = {978-1-5386-6420-9},
	shorttitle = {{MiCT}},
	url = {https://ieeexplore.ieee.org/document/8578152/},
	doi = {10.1109/CVPR.2018.00054},
	abstract = {Human actions in videos are three-dimensional (3D) signals. Recent attempts use 3D convolutional neural networks (CNNs) to explore spatio-temporal information for human action recognition. Though promising, 3D CNNs have not achieved high performanceon on this task with respect to their well-established two-dimensional (2D) counterparts for visual recognition in still images. We argue that the high training complexity of spatio-temporal fusion and the huge memory cost of 3D convolution hinder current 3D CNNs, which stack 3D convolutions layer by layer, by outputting deeper feature maps that are crucial for high-level tasks. We thus propose a Mixed Convolutional Tube (MiCT) that integrates 2D CNNs with the 3D convolution module to generate deeper and more informative feature maps, while reducing training complexity in each round of spatio-temporal fusion. A new end-to-end trainable deep 3D network, MiCTNet, is also proposed based on the MiCT to better explore spatio-temporal information in human actions. Evaluations on three well-known benchmark datasets (UCF101, Sport1M and HMDB-51) show that the proposed MiCT-Net signiﬁcantly outperforms the original 3D CNNs. Compared with state-of-the-art approaches for action recognition on UCF101 and HMDB51, our MiCT-Net yields the best performance.},
	language = {en},
	urldate = {2019-05-08},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Zhou, Yizhou and Sun, Xiaoyan and Zha, Zheng-Jun and Zeng, Wenjun},
	year = {2018},
	pages = {449--458},
	file = {Zhou et al. - 2018 - MiCT Mixed 3D2D Convolutional Tube for Human Act.pdf:/Users/Matthias/Zotero/storage/EB5URRX4/Zhou et al. - 2018 - MiCT Mixed 3D2D Convolutional Tube for Human Act.pdf:application/pdf}
}

@misc{noauthor_deep_nodate,
	title = {Deep {Learning} for {Videos}: {A} 2018 {Guide} to {Action} {Recognition}},
	url = {http://blog.qure.ai/notes/deep-learning-for-videos-action-recognition-review},
	urldate = {2019-05-08},
	file = {Deep Learning for Videos\: A 2018 Guide to Action Recognition:/Users/Matthias/Zotero/storage/H3ICSUY2/deep-learning-for-videos-action-recognition-review.html:text/html}
}

@article{kay_kinetics_2017,
	title = {The {Kinetics} {Human} {Action} {Video} {Dataset}},
	url = {http://arxiv.org/abs/1705.06950},
	abstract = {We describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance figures for neural network architectures trained and tested for human action classification on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classifiers.},
	urldate = {2019-05-08},
	journal = {arXiv:1705.06950 [cs]},
	author = {Kay, Will and Carreira, Joao and Simonyan, Karen and Zhang, Brian and Hillier, Chloe and Vijayanarasimhan, Sudheendra and Viola, Fabio and Green, Tim and Back, Trevor and Natsev, Paul and Suleyman, Mustafa and Zisserman, Andrew},
	month = may,
	year = {2017},
	note = {arXiv: 1705.06950},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1705.06950 PDF:/Users/Matthias/Zotero/storage/ZJPHXQSD/Kay et al. - 2017 - The Kinetics Human Action Video Dataset.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/2XESMWVZ/1705.html:text/html}
}

@inproceedings{he_deep_2016-1,
	address = {Las Vegas, NV},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2019-05-16},
	booktitle = {Proceedings of the 2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
	file = {He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:/Users/Matthias/Zotero/storage/PV7NQ66T/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf}
}

@inproceedings{laptev_learning_2008,
	address = {Anchorage, AK},
	title = {Learning {Realistic} {Human} {Actions} from {Movies}},
	url = {https://hal.inria.fr/inria-00548659},
	doi = {10.1109/CVPR.2008.4587756},
	abstract = {The aim of this paper is to address recognition of natural human actions in diverse and realistic video settings. This challenging but important subject has mostly been ignored in the past due to several problems one of which is the lack of realistic and annotated video datasets. Our first contribution is to address this limitation and to investigate the use of movie scripts for automatic annotation of human actions in videos. We evaluate alternative methods for action retrieval from scripts and show benefits of a text-based classifier. Using the retrieved action samples for visual learning, we next turn to the problem of action classification in video. We present a new method for video classification that builds upon and extends several recent ideas including local space-time features, space-time pyramids and multi-channel non-linear SVMs. The method is shown to improve state-of-the-art results on the standard KTH action dataset by achieving 91.8\% accuracy. Given the inherent problem of noisy labels in automatic annotation, we particularly investigate and show high tolerance of our method to annotation errors in the training set. We finally apply the method to the learning and classification of challenging action classes in movies and show promising results.},
	language = {en},
	urldate = {2019-05-15},
	booktitle = {Proceedings of the 2008 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Laptev, Ivan and Marszałek, Marcin and Schmid, Cordelia and Rozenfeld, Benjamin},
	year = {2008},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/9ULNT5SQ/Laptev et al. - 2008 - Learning Realistic Human Actions from Movies.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/3DJIGHB4/inria-00548659.html:text/html}
}

@inproceedings{andriluka_pictorial_2009,
	address = {Miami, FL},
	title = {Pictorial structures revisited: {People} detection and articulated pose estimation},
	shorttitle = {Pictorial structures revisited},
	doi = {10.1109/CVPR.2009.5206754},
	abstract = {Non-rigid object detection and articulated pose estimation are two related and challenging problems in computer vision. Numerous models have been proposed over the years and often address different special cases, such as pedestrian detection or upper body pose estimation in TV footage. This paper shows that such specialization may not be necessary, and proposes a generic approach based on the pictorial structures framework. We show that the right selection of components for both appearance and spatial modeling is crucial for general applicability and overall performance of the model. The appearance of body parts is modeled using densely sampled shape context descriptors and discriminatively trained AdaBoost classifiers. Furthermore, we interpret the normalized margin of each classifier as likelihood in a generative model. Non-Gaussian relationships between parts are represented as Gaussians in the coordinate system of the joint between parts. The marginal posterior of each part is inferred using belief propagation. We demonstrate that such a model is equally suitable for both detection and pose estimation tasks, outperforming the state of the art on three recently proposed datasets.},
	booktitle = {Proceedings of the 2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Andriluka, Mykhaylo and Roth, Stefan and Schiele, Bernt},
	month = jun,
	year = {2009},
	keywords = {AdaBoost classifiers, articulated pose estimation, Biological system modeling, computer vision, Detectors, Gaussian processes, Humans, Image edge detection, Indexing, Iterative methods, Layout, learning (artificial intelligence), object detection, Object detection, object recognition, people detection, pose estimation, shape context descriptors, shape recognition, spatial data structures, spatial modeling, State estimation},
	pages = {1014--1021},
	file = {IEEE Xplore Abstract Record:/Users/Matthias/Zotero/storage/FMG9PCDE/5206754.html:text/html;IEEE Xplore Full Text PDF:/Users/Matthias/Zotero/storage/TNBFNW92/Andriluka et al. - 2009 - Pictorial structures revisited People detection a.pdf:application/pdf}
}

@article{sun_deep_2019,
	title = {Deep {High}-{Resolution} {Representation} {Learning} for {Human} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1902.09212},
	abstract = {This is an official pytorch implementation of Deep High-Resolution Representation Learning for Human Pose Estimation. In this work, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. The code and models have been publicly available at {\textbackslash}url\{https://github.com/leoxiaobin/deep-high-resolution-net.pytorch\}.},
	urldate = {2019-05-13},
	journal = {arXiv:1902.09212 [cs]},
	author = {Sun, Ke and Xiao, Bin and Liu, Dong and Wang, Jingdong},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.09212},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: accepted by CVPR2019},
	file = {arXiv\:1902.09212 PDF:/Users/Matthias/Zotero/storage/7Z4K9WLB/Sun et al. - 2019 - Deep High-Resolution Representation Learning for H.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/TVDGRA8S/1902.html:text/html}
}

@article{fischler_representation_1973,
	title = {The {Representation} and {Matching} of {Pictorial} {Structures}},
	volume = {22},
	abstract = {The primary problem dealt with in this paper is the following. Given some description of a visual object, find that object in an actual photograph. Part of the solution to this problem is the specification of a descriptive scheme, and a metric on which to base the decision of "goodness" of matching or detection.},
	journal = {IEEE Transactions on Computers},
	author = {Fischler, Martin A. and Elschlager, Robert A.},
	month = jan,
	year = {1973},
	keywords = {Image},
	pages = {67--92}
}

@article{felzenszwalb_pictorial_2005,
	title = {Pictorial {Structures} for {Object} {Recognition}},
	volume = {61},
	abstract = {In this paper we present a computationally efficient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples, presenting efficient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images.},
	language = {en},
	number = {1},
	journal = {International Journal of Computer Vision},
	author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
	month = jan,
	year = {2005},
	keywords = {energy minimization, part-based object recognition, statistical models},
	pages = {55--79},
	file = {Springer Full Text PDF:/Users/Matthias/Zotero/storage/GH3TMS2Z/Felzenszwalb and Huttenlocher - 2005 - Pictorial Structures for Object Recognition.pdf:application/pdf}
}

@misc{rudolph_lecture_2018,
	title = {Lecture notes from {Introduction} to {Computational} {Intelligence}},
	url = {https://ls11-www.cs.tu-dortmund.de/people/rudolph/teaching/lectures/CI/WS2018-19/lecture.jsp},
	language = {en},
	author = {Rudolph, Günther},
	month = oct,
	year = {2018}
}

@book{goodfellow_deep_2016,
	title = {Deep {Learning}},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	annote = {http://www.deeplearningbook.org},
	file = {Goodfellow et al. - 2016 - Deep Learning.pdf:/Users/Matthias/Zotero/storage/2NL4FSA5/Goodfellow et al. - 2016 - Deep Learning.pdf:application/pdf}
}

@book{rojas_neural_1996,
	title = {Neural {Networks}: {A} {Systematic} {Introduction}},
	author = {Rojas, Raúl},
	year = {1996},
	file = {Rojas - 1996 - Neural Networks A Systematic Introduction.pdf:/Users/Matthias/Zotero/storage/T9YX8FYU/Rojas - 1996 - Neural Networks A Systematic Introduction.pdf:application/pdf}
}

@inproceedings{ramakrishna_pose_2014,
	address = {Zürich, CH},
	title = {Pose {Machines}: {Articulated} {Pose} {Estimation} via {Inference} {Machines}},
	shorttitle = {Pose {Machines}},
	doi = {10.1007/978-3-319-10605-2_3},
	abstract = {State-of-the-art approaches for articulated human pose estimation are rooted in parts-based graphical models. These models are often restricted to tree-structured representations and simple parametric potentials in order to enable tractable inference. However, these simple dependencies fail to capture all the interactions between body parts. While models with more complex interactions can be defined, learning the parameters of these models remains challenging with intractable or approximate inference. In this paper, instead of performing inference on a learned graphical model, we build upon the inference machine framework and present a method for articulated human pose estimation. Our approach incorporates rich spatial interactions among multiple parts and information across parts of different scales. Additionally, the modular framework of our approach enables both ease of implementation without specialized optimization solvers, and efficient inference. We analyze our approach on two challenging datasets with large pose variation and outperform the state-of-the-art on these benchmarks.},
	booktitle = {Proceedings of the 13th {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Ramakrishna, Varun and Munoz, Daniel and Hebert, Martial and Bagnell, J. Andrew and Sheikh, Yaser},
	year = {2014},
	keywords = {3D pose estimation, Approximation algorithm, Benchmark (computing), Cobham's thesis, Graphical model, Inference engine, Interaction, Mathematical optimization, Parametric polymorphism},
	pages = {33--47},
	file = {Submitted Version:/Users/Matthias/Zotero/storage/I9NWE8MM/Ramakrishna et al. - 2014 - Pose Machines Articulated Pose Estimation via Inf.pdf:application/pdf}
}

@article{baradel_pose-conditioned_2017,
	title = {Pose-conditioned {Spatio}-{Temporal} {Attention} for {Human} {Action} {Recognition}},
	url = {http://arxiv.org/abs/1703.10106},
	abstract = {We address human action recognition from multi-modal video data involving articulated pose and RGB frames and propose a two-stream approach. The pose stream is processed with a convolutional model taking as input a 3D tensor holding data from a sub-sequence. A specific joint ordering, which respects the topology of the human body, ensures that different convolutional layers correspond to meaningful levels of abstraction. The raw RGB stream is handled by a spatio-temporal soft-attention mechanism conditioned on features from the pose network. An LSTM network receives input from a set of image locations at each instant. A trainable glimpse sensor extracts features on a set of predefined locations specified by the pose stream, namely the 4 hands of the two people involved in the activity. Appearance features give important cues on hand motion and on objects held in each hand. We show that it is of high interest to shift the attention to different hands at different time steps depending on the activity itself. Finally a temporal attention mechanism learns how to fuse LSTM features over time. We evaluate the method on 3 datasets. State-of-the-art results are achieved on the largest dataset for human activity recognition, namely NTU-RGB+D, as well as on the SBU Kinect Interaction dataset. Performance close to state-of-the-art is achieved on the smaller MSR Daily Activity 3D dataset.},
	urldate = {2019-05-23},
	journal = {arXiv:1703.10106 [cs]},
	author = {Baradel, Fabien and Wolf, Christian and Mille, Julien},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.10106},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 10 pages, project page: https://fabienbaradel.github.io/pose\_rgb\_attention\_human\_action},
	file = {arXiv\:1703.10106 PDF:/Users/Matthias/Zotero/storage/KQ87427Q/Baradel et al. - 2017 - Pose-conditioned Spatio-Temporal Attention for Hum.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/KFR8CY8N/1703.html:text/html}
}

@inproceedings{htike_human_2014,
	address = {Beirut, LBN},
	title = {Human activity recognition for video surveillance using sequences of postures},
	booktitle = {Proceedings of the 3rd {International} {Conference} on e-{Technologies} and {Networks} for {Development} ({ICeND})},
	author = {Htike, Kyaw Kyaw and Khalifa, Othman O. and Ramli, Huda A. M. and Abushariah, Mohammad A. M.},
	month = apr,
	year = {2014},
	keywords = {Accuracy, accuracy recognition, Activity Recognition, automated surveillance, cameras, Cameras, feedforward neural networks, fuzzy C means, human activity recognition, human computer interaction, Human posture, human posture recognition system, human-computer interfaces, intelligent systems, K-means, multilayer perceptron self-organizing maps, multilayer perceptrons, Multilayer perceptrons, Neural networks, self-organising feature maps, sign language interpretation, sign language recognition, static camera, supervised learning classifiers, Surveillance, Training, unsupervised classifiers, unsupervised learning, video sequences, Video sequences, video surveillance},
	pages = {79--82}
}

@misc{dertat_applied_2017,
	title = {Applied {Deep} {Learning} - {Part} 4: {Convolutional} {Neural} {Networks}},
	url = {https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2},
	abstract = {Welcome to Part 4 of Applied Deep Learning series. Part 1 was a hands-on introduction to Artificial Neural Networks, covering both the theory and application with a lot of code examples and visualization. In Part 2 we applied deep learning to real-world datasets, covering the 3 most commonly encountered problems as case studies: binary classification, multiclass classification and regression. Part 3 explored a specific deep learning architecture: Autoencoders.},
	urldate = {2019-04-17},
	author = {Dertat, Arden},
	month = nov,
	year = {2017}
}

@misc{cornelisse_intuitive_2018,
	title = {An intuitive guide to {Convolutional} {Neural} {Networks}},
	url = {https://www.freecodecamp.org/news/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050/},
	abstract = {In this article, we will explore Convolutional Neural Networks (CNNs) and, on a high level, go through how they are inspired by the structure of the brain. If you want to read more about the brain specifically, there are more resources at the end of the article to help you further.},
	urldate = {2019-06-17},
	author = {Cornelisse, Daphne},
	month = apr,
	year = {2018}
}

@inproceedings{nair_rectified_2010,
	address = {Haifa, ISR},
	title = {Rectified {Linear} {Units} {Improve} {Restricted} {Boltzmann} {Machines}},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Machine} {Learning} ({ICML})},
	author = {Nair, Vinod and Hinton, Geoffrey E.},
	month = jun,
	year = {2010},
	pages = {807--814}
}

@article{kong_human_2018,
	title = {Human {Action} {Recognition} and {Prediction}: {A} {Survey}},
	shorttitle = {Human {Action} {Recognition} and {Prediction}},
	url = {http://arxiv.org/abs/1806.11230},
	abstract = {Derived from rapid advances in computer vision and machine learning, video analysis tasks have been moving from inferring the present state to predicting the future state. Vision-based action recognition and prediction from videos are such tasks, where action recognition is to infer human actions (present state) based upon complete action executions, and action prediction to predict human actions (future state) based upon incomplete action executions. These two tasks have become particularly prevalent topics recently because of their explosively emerging real-world applications, such as visual surveillance, autonomous driving vehicle, entertainment, and video retrieval, etc. Many attempts have been devoted in the last a few decades in order to build a robust and effective framework for action recognition and prediction. In this paper, we survey the complete state-of-the-art techniques in the action recognition and prediction. Existing models, popular algorithms, technical difficulties, popular action databases, evaluation protocols, and promising future directions are also provided with systematic discussions.},
	urldate = {2019-06-06},
	journal = {arXiv:1806.11230 [cs]},
	author = {Kong, Yu and Fu, Yun},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.11230},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1806.11230 PDF:/Users/Matthias/Zotero/storage/6U48DCI5/Kong and Fu - 2018 - Human Action Recognition and Prediction A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/3AXIYAVR/1806.html:text/html}
}

@phdthesis{pons-moll_human_2014,
	type = {{PhD} {Thesis}},
	title = {Human {Pose} {Estimation} from {Video} and {Inertial} {Sensors}},
	abstract = {The analysis and understanding of human movement is central to many applications such as sports
science, medical diagnosis and movie production. The ability to automatically monitor human activity in security sensitive areas such as airports, lobbies or borders is of great practical importance.
Furthermore, automatic pose estimation from images leverages the processing and understanding of
massive digital libraries available on the Internet. We build upon a model based approach where the
human shape is modeled with a surface mesh and the motion is parameterized by a kinematic chain.
We then seek for the pose of the model that best explains the available observations coming from
different sensors.
In a first scenario, we consider a calibrated multiview setup in an indoor studio. To obtain very
accurate results, we propose a novel tracker that combines information coming from video and a small
set of Inertial Measurement Units (IMUs). We do so by locally optimizing a joint energy consisting
of a term that measures the likelihood of the video data and a term for the IMU data. This is the
first work to successfully combine video and IMUs information for full body pose estimation. When
compared to commercial marker based systems the proposed solution is more cost efficient and less
intrusive for the user.
In a second scenario, we relax the assumption of an indoor studio and we tackle outdoor scenes with
background clutter, illumination changes, large recording volumes and difficult motions of people
interacting with objects. Again, we combine information from video and IMUs. Here we employ
a particle based optimization approach that allows us to be more robust to tracking failures. To
satisfy the orientation constraints imposed by the IMUs, we derive an analytic Inverse Kinematics
(IK) procedure to sample from the manifold of valid poses. The generated hypothesis come from a
lower dimensional manifold and therefore the computational cost can be reduced. Experiments on
challenging sequences suggest the proposed tracker can be applied to capture in outdoor scenarios.
Furthermore, the proposed IK sampling procedure can be used to integrate any kind of constraints
derived from the environment.
Finally, we consider the most challenging possible scenario: pose estimation of monocular images.
Here, we argue that estimating the pose to the degree of accuracy as in an engineered environment
is too ambitious with the current technology. Therefore, we propose to extract meaningful semantic
information about the pose directly from image features in a discriminative fashion. In particular,
we introduce posebits which are semantic pose descriptors about the geometric relationships between
parts in the body. The experiments show that the intermediate step of inferring posebits from images
can improve pose estimation from monocular imagery. Furthermore, posebits can be very useful as
input feature for many computer vision algorithms.},
	language = {en},
	school = {Gottfried Wilhelm Leibniz Universität Hannover},
	author = {Pons-Moll, Gerard},
	year = {2014},
	file = {Pons-Moll - 2014 - Human Pose Estimation from Video and Inertial Sens.pdf:/Users/Matthias/Zotero/storage/EZJR6QMR/Pons-Moll - 2014 - Human Pose Estimation from Video and Inertial Sens.pdf:application/pdf}
}

@phdthesis{zhu_articulated_2016,
	type = {{PhD} {Thesis}},
	title = {Articulated human pose estimation in images and video},
	school = {Troyes},
	author = {Zhu, Aichun},
	year = {2016},
	file = {Zhu - 2016 - Articulated human pose estimation in images and vi.pdf:/Users/Matthias/Zotero/storage/DHUVHZA3/Zhu - 2016 - Articulated human pose estimation in images and vi.pdf:application/pdf}
}

@inproceedings{felzenszwalb_efficient_2000,
	address = {Hilton Head Island, SC, USA},
	title = {Efficient matching of pictorial structures},
	volume = {2},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Felzenszwalb, Pedro F and Huttenlocher, Daniel P},
	year = {2000},
	pages = {66--73},
	file = {Felzenszwalb and Huttenlocher - 2000 - Efficient matching of pictorial structures.pdf:/Users/Matthias/Zotero/storage/P7DIQA5K/Felzenszwalb and Huttenlocher - 2000 - Efficient matching of pictorial structures.pdf:application/pdf}
}

@article{zhang_human_2019,
	title = {Human {Pose} {Estimation} with {Spatial} {Contextual} {Information}},
	url = {http://arxiv.org/abs/1901.01760},
	abstract = {We explore the importance of spatial contextual information in human pose estimation. Most state-of-the-art pose networks are trained in a multi-stage manner and produce several auxiliary predictions for deep supervision. With this principle, we present two conceptually simple and yet computational efficient modules, namely Cascade Prediction Fusion (CPF) and Pose Graph Neural Network (PGNN), to exploit underlying contextual information. Cascade prediction fusion accumulates prediction maps from previous stages to extract informative signals. The resulting maps also function as a prior to guide prediction at following stages. To promote spatial correlation among joints, our PGNN learns a structured representation of human pose as a graph. Direct message passing between different joints is enabled and spatial relation is captured. These two modules require very limited computational complexity. Experimental results demonstrate that our method consistently outperforms previous methods on MPII and LSP benchmark.},
	urldate = {2019-06-24},
	journal = {arXiv:1901.01760 [cs]},
	author = {Zhang, Hong and Ouyang, Hao and Liu, Shu and Qi, Xiaojuan and Shen, Xiaoyong and Yang, Ruigang and Jia, Jiaya},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.01760},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1901.01760 PDF:/Users/Matthias/Zotero/storage/578PBG47/Zhang et al. - 2019 - Human Pose Estimation with Spatial Contextual Info.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/US5XDTBB/1901.html:text/html}
}

@article{su_cascade_2019,
	title = {Cascade {Feature} {Aggregation} for {Human} {Pose} {Estimation}},
	url = {http://arxiv.org/abs/1902.07837},
	abstract = {Human pose estimation plays an important role in many computer vision tasks and has been studied for many decades. However, due to complex appearance variations from poses, illuminations, occlusions and low resolutions, it still remains a challenging problem. Taking the advantage of high-level semantic information from deep convolutional neural networks is an effective way to improve the accuracy of human pose estimation. In this paper, we propose a novel Cascade Feature Aggregation (CFA) method, which cascades several hourglass networks for robust human pose estimation. Features from different stages are aggregated to obtain abundant contextual information, leading to robustness to poses, partial occlusions and low resolution. Moreover, results from different stages are fused to further improve the localization accuracy. The extensive experiments on MPII datasets and LIP datasets demonstrate that our proposed CFA outperforms the state-of-the-art and achieves the best performance on the state-of-the-art benchmark MPII.},
	urldate = {2019-06-24},
	journal = {arXiv:1902.07837 [cs]},
	author = {Su, Zhihui and Ye, Ming and Zhang, Guohui and Dai, Lei and Sheng, Jianda},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.07837},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1902.07837 PDF:/Users/Matthias/Zotero/storage/LN9PXIMA/Su et al. - 2019 - Cascade Feature Aggregation for Human Pose Estimat.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/HJWDVMS4/1902.html:text/html}
}

@article{yang_articulated_2013,
	title = {Articulated {Human} {Detection} with {Flexible} {Mixtures} of {Parts}},
	volume = {35},
	number = {12},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
	author = {Yang, Yi and Ramanan, Deva},
	month = dec,
	year = {2013},
	keywords = {Algorithms, articulated human detection, articulated shapes, articulation modeling, Buffy datasets, Computational modeling, cooccurrence relations, Deformable models, deformable part model representation, deformable part models, dynamic programming, flexible mixture model, flexible part mixtures, global geometry dependency, Human factors, human pose estimation, Humans, image representation, local mixtures, Models, nonoriented parts, object detection, Object segmentation, Parse datasets, pose estimation, Pose estimation, Reproducibility of Results, Shape analysis, similar warps, solid modelling, spatial relations, standard pictorial structure models, static images, structured SVM solver, support vector machines, Theoretical, warped templates},
	pages = {2878--2890},
	file = {Yang and Ramanan - 2013 - Articulated Human Detection with Flexible Mixtures.pdf:/Users/Matthias/Zotero/storage/QBDWBIQ3/Yang and Ramanan - 2013 - Articulated Human Detection with Flexible Mixtures.pdf:application/pdf}
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	number = {4},
	journal = {The Bulletin of Mathematical Biophysics},
	author = {McCulloch, Warren S and Pitts, Walter H},
	year = {1943},
	pages = {115--133},
	file = {McCulloch and Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf:/Users/Matthias/Zotero/storage/LIGL2EV8/McCulloch and Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf:application/pdf}
}

@misc{dertat_applied_2017-1,
	title = {Applied {Deep} {Learning} - {Part} 1: {Artificial} {Neural} {Networks}},
	url = {https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6},
	urldate = {2019-07-27},
	author = {Dertat, Arden},
	month = nov,
	year = {2017}
}

@phdthesis{werbos_beyond_1974,
	type = {{PhD} {Thesis}},
	title = {Beyond {Regression}: {New} {Tools} for {Prediction} and {Analysis} in the {Behavioral} {Sciences}},
	school = {Harvard University},
	author = {Werbos, Paul J},
	year = {1974}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	number = {6088},
	journal = {Nature},
	author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	month = oct,
	year = {1986},
	pages = {533--536}
}

@phdthesis{hochreiter_untersuchungen_1991,
	type = {Diplomarbeit},
	title = {Untersuchungen zu dynamischen neuronalen {Netzen}},
	language = {German},
	school = {Technische Universität München},
	author = {Hochreiter, Josef},
	month = jun,
	year = {1991}
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2012},
	pages = {1097--1105},
	file = {Krizhevsky et al. - 2012 - Imagenet classification with deep convolutional ne.pdf:/Users/Matthias/Zotero/storage/PRS7IJXF/Krizhevsky et al. - 2012 - Imagenet classification with deep convolutional ne.pdf:application/pdf}
}

@inproceedings{sapp_modec:_2013,
	address = {Sydney, AU},
	title = {{MODEC}: {Multimodal} {Decomposable} {Models} for {Human} {Pose} {Estimation}},
	booktitle = {Proceedings of the 2013 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Sapp, Benjamin and Taskar, Ben},
	month = dec,
	year = {2013}
}

@article{eichner_articulated_2010,
	title = {Articulated human pose estimation and search in (almost) unconstrained still images},
	volume = {272},
	journal = {ETH Zurich, D-ITET, BIWI, Technical Report No},
	author = {Eichner, Martin and Marin-Jimenez, Manuel and Zisserman, Andrew and Ferrari, Vittorio},
	year = {2010},
	pages = {22}
}

@article{belongie_shape_2002,
	title = {Shape matching and object recognition using shape contexts},
	volume = {24},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
	author = {Belongie, Serge and Malik, Jitendra and Puzicha, Jan},
	month = apr,
	year = {2002},
	pages = {509--522}
}

@article{freund_short_1999,
	title = {A short introduction to boosting},
	volume = {14},
	number = {5},
	journal = {Japanese Society For Artificial Intelligence},
	author = {Freund, Yoav and Schapire, Robert E},
	year = {1999},
	pages = {771--780}
}

@article{freund_decision-theoretic_1997,
	title = {A decision-theoretic generalization of on-line learning and an application to boosting},
	volume = {55},
	number = {1},
	journal = {Journal of Computer and System Sciences},
	author = {Freund, Yoav and Schapire, Robert E},
	year = {1997},
	pages = {119--139}
}

@inproceedings{dalal_histograms_2005,
	address = {San Diego, CA},
	title = {Histograms of {Oriented} {Gradients} for {Human} {Detection}},
	abstract = {We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
	language = {en},
	urldate = {2019-08-05},
	booktitle = {Proceedings of the 2005 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Dalal, Navneet and Triggs, Bill},
	month = jun,
	year = {2005},
	pages = {886--893},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/HZRKIBPL/Dalal and Triggs - 2005 - Histograms of Oriented Gradients for Human Detecti.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/8CD677LX/inria-00548512.html:text/html}
}

@incollection{ramanan_learning_2007,
	title = {Learning to parse images of articulated bodies},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 19},
	publisher = {MIT Press},
	author = {Ramanan, Deva},
	year = {2007},
	pages = {1129--1136}
}

@inproceedings{ferrari_progressive_2008,
	address = {Anchorage, AK},
	title = {Progressive search space reduction for human pose estimation},
	doi = {10.1109/CVPR.2008.4587468},
	abstract = {The objective of this paper is to estimate 2D human pose as a spatial configuration of body parts in TV and movie video shots. Such video material is uncontrolled and extremely challenging. We propose an approach that progressively reduces the search space for body parts, to greatly improve the chances that pose estimation will succeed. This involves two contributions: (i) a generic detector using a weak model of pose to substantially reduce the full pose search space; and (ii) employing 'grabcut' initialized on detected regions proposed by the weak model, to further prune the search space. Moreover, we also propose (Hi) an integrated spatio- temporal model covering multiple frames to refine pose estimates from individual frames, with inference using belief propagation. The method is fully automatic and self-initializing, and explains the spatio-temporal volume covered by a person moving in a shot, by soft-labeling every pixel as belonging to a particular body part or to the background. We demonstrate upper-body pose estimation by an extensive evaluation over 70000 frames from four episodes of the TV series Buffy the vampire slayer, and present an application to full- body action recognition on the Weizmann dataset.},
	booktitle = {Proceedings of the 2008 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Ferrari, Vittorio and Marin-Jimenez, Manuel and Zisserman, Andrew},
	month = jun,
	year = {2008},
	keywords = {2D human pose, Arm, belief propagation, Biological system modeling, Clothing, Detectors, full-body action recognition, generic detector, Head, human pose estimation, Humans, Motion pictures, movie video shots, pose estimation, progressive search space reduction, Switches, Torso, TV, TV video shots, video material, video signal processing},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:/Users/Matthias/Zotero/storage/IVY5W4SQ/4587468.html:text/html;Submitted Version:/Users/Matthias/Zotero/storage/EWEQBWU2/Ferrari et al. - 2008 - Progressive search space reduction for human pose .pdf:application/pdf}
}

@incollection{feichtenhofer_spatiotemporal_2016,
	title = {Spatiotemporal {Residual} {Networks} for {Video} {Action} {Recognition}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	author = {Feichtenhofer, Christoph and Pinz, Axel and Wildes, Richard},
	year = {2016},
	pages = {3468--3476}
}

@incollection{yuan_exact_2016,
	title = {Exact {Recovery} of {Hard} {Thresholding} {Pursuit}},
	url = {http://papers.nips.cc/paper/6432-exact-recovery-of-hard-thresholding-pursuit.pdf},
	urldate = {2019-10-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Yuan, Xiaotong and Li, Ping and Zhang, Tong},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {3558--3566},
	file = {NIPS Snapshot:/Users/Matthias/Zotero/storage/79XZJ3A7/6432-spatiotemporal-residual-networks-for-video-action-recognition.html:text/html}
}

@inproceedings{feichtenhofer_convolutional_2016,
	address = {Las Vegas, NV},
	title = {Convolutional {Two}-{Stream} {Network} {Fusion} for {Video} {Action} {Recognition}},
	urldate = {2019-10-09},
	booktitle = {Proceedings of the 2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Feichtenhofer, Christoph and Pinz, Axel and Zisserman, Andrew},
	month = jul,
	year = {2016},
	pages = {1933--1941},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/8TWR67EB/Feichtenhofer et al. - 2016 - Convolutional Two-Stream Network Fusion for Video .pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/RYEBTIWN/Feichtenhofer_Convolutional_Two-Stream_Network_CVPR_2016_paper.html:text/html}
}

@inproceedings{tran_learning_2015,
	address = {Santiago, CL},
	title = {Learning {Spatiotemporal} {Features} {With} 3D {Convolutional} {Networks}},
	url = {https://www.cv-foundation.org/openaccess/content_iccv_2015/html/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.html},
	urldate = {2019-10-09},
	booktitle = {Proceedings of the 2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
	month = dec,
	year = {2015},
	pages = {4489--4497},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/XSYSA323/Tran et al. - 2015 - Learning Spatiotemporal Features With 3D Convoluti.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/4BRJKITN/Tran_Learning_Spatiotemporal_Features_ICCV_2015_paper.html:text/html}
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, Josef and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780}
}

@inproceedings{karpathy_large-scale_2014,
	address = {Columbus, OH},
	title = {Large-scale {Video} {Classification} with {Convolutional} {Neural} {Networks}},
	booktitle = {Proceedings of the 2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
	month = jun,
	year = {2014},
	pages = {1725--1732},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/XVTT6ZF7/Karpathy et al. - 2014 - Large-scale Video Classification with Convolutiona.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/PCUR4NMQ/Karpathy_Large-scale_Video_Classification_2014_CVPR_paper.html:text/html}
}

@inproceedings{wong_extracting_2007,
	address = {Rio de Janeiro, BR},
	title = {Extracting {Spatiotemporal} {Interest} {Points} using {Global} {Information}},
	abstract = {Local spatiotemporal features or interest points provide compact but descriptive representations for efficient video analysis and motion recognition. Current local feature extraction approaches involve either local filtering or entropy computation which ignore global information (e.g. large blobs of moving pixels) in video inputs. This paper presents a novel extraction method which utilises global information from each video input so that moving parts such as a moving hand can be identified and are used to select relevant interest points for a condensed representation. The proposed method involves obtaining a small set of subspace images, which can synthesise frames in the video input from their corresponding coefficient vectors, and then detecting interest points from the subspaces and the coefficient vectors. Experimental results indicate that the proposed method can yield a sparser set of interest points for motion recognition than existing methods.},
	booktitle = {Proceedings of the 2007 {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Wong, Shu-Fai and Cipolla, Roberto},
	month = oct,
	year = {2007},
	pages = {1--8}
}

@inproceedings{macqueen_methods_1967,
	title = {Some methods for classification and analysis of multivariate observations},
	volume = {1},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2019-10-08},
	booktitle = {Proceedings of the 5th {Berkeley} {Symposium} on {Mathematical} {Statistics} and {Probability}},
	author = {MacQueen, James},
	year = {1967},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/8QTG77Y3/MacQueen - 1967 - Some methods for classification and analysis of mu.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/GQHZWSYN/1200512992.html:text/html}
}

@article{lloyd_least_1982,
	title = {Least squares quantization in {PCM}},
	volume = {28},
	abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for2{\textasciicircum}bquanta,b=1,2, {\textbackslash}cdots, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
	number = {2},
	journal = {IEEE Transactions on Information Theory},
	author = {Lloyd, S.},
	month = mar,
	year = {1982},
	keywords = {Least-squares approximation, PCM communication, Quantization (signal), Signal quantization},
	pages = {129--137},
	file = {IEEE Xplore Abstract Record:/Users/Matthias/Zotero/storage/FRNY98FK/1056489.html:text/html;Submitted Version:/Users/Matthias/Zotero/storage/H6ZYCCV7/Lloyd - 1982 - Least squares quantization in PCM.pdf:application/pdf}
}

@article{laptev_harris3d,
	title = {On {Space}-{Time} {Interest} {Points}},
	volume = {64},
	abstract = {Local image features or interest points provide compact and abstract representations of patterns in an image. In this paper, we extend the notion of spatial interest points into the spatio-temporal domain and show how the resulting features often reflect interesting events that can be used for a compact representation of video data as well as for interpretation of spatio-temporal events.To detect spatio-temporal events, we build on the idea of the Harris and Förstner interest point operators and detect local structures in space-time where the image values have significant local variations in both space and time. We estimate the spatio-temporal extents of the detected events by maximizing a normalized spatio-temporal Laplacian operator over spatial and temporal scales. To represent the detected events, we then compute local, spatio-temporal, scale-invariant N-jets and classify each event with respect to its jet descriptor. For the problem of human motion analysis, we illustrate how a video representation in terms of local space-time features allows for detection of walking people in scenes with occlusions and dynamic cluttered backgrounds.},
	language = {en},
	number = {2},
	urldate = {2019-10-08},
	journal = {International Journal of Computer Vision},
	author = {Laptev, Ivan},
	month = sep,
	year = {2005},
	keywords = {interest points, matching, scale selection, scale-space, video interpretation},
	pages = {107--123},
	file = {Springer Full Text PDF:/Users/Matthias/Zotero/storage/RR7CTLWT/Laptev - 2005 - On Space-Time Interest Points.pdf:application/pdf}
}

@inproceedings{harris_combined_1988,
	address = {Manchester, UK},
	title = {A combined corner and edge detector.},
	booktitle = {Proceedings of the 4th {Alvey} {Vision} {Conference} ({AVC})},
	author = {Harris, Christopher G. and Stephens, Mike},
	month = sep,
	year = {1988},
	pages = {1--6}
}

@inproceedings{liu_recognizing_2009,
	address = {Miami, FL},
	title = {Recognizing realistic actions from videos “in the wild”},
	abstract = {In this paper, we present a systematic framework for recognizing realistic actions from videos “in the wild”. Such unconstrained videos are abundant in personal collections as well as on the Web. Recognizing action from such videos has not been addressed extensively, primarily due to the tremendous variations that result from camera motion, background clutter, changes in object appearance, and scale, etc. The main challenge is how to extract reliable and informative features from the unconstrained videos. We extract both motion and static features from the videos. Since the raw features of both types are dense yet noisy, we propose strategies to prune these features. We use motion statistics to acquire stable motion features and clean static features. Furthermore, PageRank is used to mine the most informative static features. In order to further construct compact yet discriminative visual vocabularies, a divisive information-theoretic algorithm is employed to group semantically related features. Finally, AdaBoost is chosen to integrate all the heterogeneous yet complementary features for recognition. We have tested the framework on the KTH dataset and our own dataset consisting of 11 categories of actions collected from YouTube and personal videos, and have obtained impressive results for action recognition and action localization.},
	booktitle = {Proceedings of the 2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Liu, Jingen and Luo, Jiebo and Shah, Mubarak},
	month = jun,
	year = {2009},
	keywords = {action localization, AdaBoost, Cameras, Computer vision, feature extraction, Feature extraction, Humans, image motion analysis, information-theoretic algorithm, informative static features, KTH dataset, motion features, Motion pictures, motion statistics, PageRank, personal videos, realistic action recognition, Shape, Spatiotemporal phenomena, unconstrained videos, video signal processing, Videos, visual vocabularies, Vocabulary, YouTube},
	pages = {1996--2003},
	file = {IEEE Xplore Abstract Record:/Users/Matthias/Zotero/storage/84DZK2UW/5206744.html:text/html}
}

@misc{noauthor_recognizing_nodate,
	title = {Recognizing realistic actions from videos “in the wild” - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore.ieee.org/document/5206744},
	urldate = {2019-10-08},
	file = {Recognizing realistic actions from videos “in the wild” - IEEE Conference Publication:/Users/Matthias/Zotero/storage/7P96GH92/5206744.html:text/html}
}

@inproceedings{schuldt_recognizing_2004,
	address = {Cambridge, UK},
	title = {Recognizing human actions: a local {SVM} approach},
	volume = {3},
	shorttitle = {Recognizing human actions},
	abstract = {Local space-time features capture local events in video and can be adapted to the size, the frequency and the velocity of moving patterns. In this paper, we demonstrate how such features can be used for recognizing complex motion patterns. We construct video representations in terms of local space-time features and integrate such representations with SVM classification schemes for recognition. For the purpose of evaluation we introduce a new video database containing 2391 sequences of six human actions performed by 25 people in four different scenarios. The presented results of action recognition justify the proposed method and demonstrate its advantage compared to other relative approaches for action recognition.},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
	author = {Schuldt, Christian and Laptev, Ivan and Caputo, Barbara},
	month = aug,
	year = {2004},
	keywords = {Cameras, Computer vision, feature extraction, Frequency, human action recognition, Humans, Image recognition, local space time features, motion pattern recognition, pattern classification, Pattern recognition, Performance evaluation, Spatial databases, Support vector machine classification, support vector machines, Support vector machines, SVM classification, video database, video databases, video representations, video signal processing},
	pages = {32--36},
	file = {IEEE Xplore Abstract Record:/Users/Matthias/Zotero/storage/EKD88EH7/1334462.html:text/html}
}

@inproceedings{marszalek_actions_2009,
	address = {Miami, FL},
	title = {Actions in {Context}},
	abstract = {This paper exploits the context of natural dynamic scenes for human action recognition in video. Human actions are frequently constrained by the purpose and the physical properties of scenes and demonstrate high correlation with particular scene classes. For example, eating often happens in a kitchen while running is more common outdoors. The contribution of this paper is three-fold: (a) we automatically discover relevant scene classes and their correlation with human actions, (b) we show how to learn selected scene classes from video without manual supervision and (c) we develop a joint framework for action and scene recognition and demonstrate improved recognition of both in natural video. We use movie scripts as a means of automatic supervision for training. For selected action classes we identify correlated scene classes in text and then retrieve video samples of actions and scenes for training using script-to-video alignment. Our visual models for scenes and actions are formulated within the bag-of-features framework and are combined in a joint scene-action SVM-based classifier. We report experimental results and validate the method on a new large dataset with twelve action classes and ten scene classes acquired from 69 movies.},
	language = {en},
	urldate = {2019-10-08},
	booktitle = {Proceedings of the 2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Marszałek, Marcin and Laptev, Ivan and Schmid, Cordelia},
	month = jun,
	year = {2009},
	pages = {2929--2936},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/7LWWAMKL/Marszałek et al. - 2009 - Actions in Context.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/8ABW8AUH/inria-00548645.html:text/html}
}

@article{fischler_random_1981,
	title = {Random {Sample} {Consensus}: {A} {Paradigm} for {Model} {Fitting} with {Applications} to {Image} {Analysis} and {Automated} {Cartography}},
	volume = {24},
	shorttitle = {Random {Sample} {Consensus}},
	abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing},
	number = {6},
	urldate = {2019-10-07},
	journal = {Communications of the ACM},
	author = {Fischler, Martin A. and Bolles, Robert C.},
	month = jun,
	year = {1981},
	keywords = {automated cartography, camera calibration, image matching, location determination, model fitting, scene analysis},
	pages = {381--395}
}

@inproceedings{bay_surf:_2006,
	address = {Graz, AT},
	title = {{SURF}: {Speeded} {Up} {Robust} {Features}},
	shorttitle = {{SURF}},
	abstract = {In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Robust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster.This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (in casu, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper presents experimental results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object recognition application. Both show SURF’s strong performance.},
	language = {en},
	booktitle = {Proceedings of the 9th {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Bay, Herbert and Tuytelaars, Tinne and Van Gool, Luc},
	year = {2006},
	keywords = {Hessian Matrix, Integral Image, Interest Point, Robust Feature, Viewpoint Change},
	pages = {404--417},
	file = {Springer Full Text PDF:/Users/Matthias/Zotero/storage/DM9D842B/Bay et al. - 2006 - SURF Speeded Up Robust Features.pdf:application/pdf}
}

@article{prest_weakly_2012,
	title = {Weakly {Supervised} {Learning} of {Interactions} between {Humans} and {Objects}},
	volume = {34},
	abstract = {We introduce a weakly supervised approach for learning human actions modeled as interactions between humans and objects. Our approach is human-centric: We first localize a human in the image and then determine the object relevant for the action and its spatial relation with the human. The model is learned automatically from a set of still images annotated only with the action label. Our approach relies on a human detector to initialize the model learning. For robustness to various degrees of visibility, we build a detector that learns to combine a set of existing part detectors. Starting from humans detected in a set of images depicting the action, our approach determines the action object and its spatial relation to the human. Its final output is a probabilistic model of the human-object interaction, i.e., the spatial relation between the human and the object. We present an extensive experimental evaluation on the sports action data set from [1], the PASCAL Action 2010 data set [2], and a new human-object interaction data set.},
	number = {3},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Prest, Alessandro and Schmid, Cordelia and Ferrari, Vittorio},
	month = mar,
	year = {2012},
	keywords = {action recognition, Action recognition, Algorithms, Artificial Intelligence, Computational modeling, Context modeling, Detectors, Face, gesture recognition, human-object interaction, Humans, Image Interpretation, Computer-Assisted, learning (artificial intelligence), model learning, Models, Statistical, object detection, object detection., Pattern Recognition, Automated, probabilistic model, probability, still images, Support vector machines, Training, weakly supervised learning},
	pages = {601--614},
	file = {IEEE Xplore Abstract Record:/Users/Matthias/Zotero/storage/8XVRB25D/5975168.html:text/html;Submitted Version:/Users/Matthias/Zotero/storage/6MNPP62C/Prest et al. - 2012 - Weakly Supervised Learning of Interactions between.pdf:application/pdf}
}

@inproceedings{vincent_detecting_2001,
	address = {Pula, HRV},
	title = {Detecting planar homographies in an image pair},
	doi = {10.1109/ISPA.2001.938625},
	abstract = {Because of their abundance and simplicity, planes are used in several computer vision tasks. Their simplicity results in that, under perspective projection, the transformation between a world plane and its corresponding image plane is projective linear, or a homography. These relations also hold between perspective views of a plane in different images. This paper proposes an algorithm that detects planar homographies in uncalibrated image pairs. It then demonstrates how this plane identification method can be used as a first step in an image analysis process, when point matching between images is unreliable. The detection is performed using a RANSAC scheme based on the linear computation of the homography matrix elements using four points. Results are shown on real image pairs.},
	booktitle = {Proceedings of the 2nd {International} {Symposium} on {Image} and {Signal} {Processing} and {Analysis} ({ISPA})},
	author = {Vincent, E. and Laganiere, Robert},
	month = jun,
	year = {2001},
	keywords = {computer vision, Computer vision, computer vision tasks, corresponding image plane, Ear, feature extraction, Geometry, homography matrix elements, Image analysis, image analysis process, image matching, Image reconstruction, Image segmentation, Information technology, Layout, linear computation, matrix algebra, perspective projection, planar homography detection, plane identification method, RANSAC scheme, Stereo vision, Transmission line matrix methods, uncalibrated image pairs, world plane},
	pages = {182--187},
	file = {IEEE Xplore Abstract Record:/Users/Matthias/Zotero/storage/CDA79JN9/938625.html:text/html;Submitted Version:/Users/Matthias/Zotero/storage/SZN2KEIS/Vincent and Laganiere - 2001 - Detecting planar homographies in an image pair.pdf:application/pdf}
}

@article{bradski_opencv_2000,
	title = {The {OpenCV} {Library}},
	journal = {Dr. Dobb's Journal of Software Tools},
	author = {Bradski, Gary},
	year = {2000},
	keywords = {bibtex-import}
}

@inproceedings{scovanner_3d_sift,
	address = {Augsburg, DE},
	title = {A 3-dimensional {Sift} {Descriptor} and {Its} {Application} to {Action} {Recognition}},
	abstract = {In this paper we introduce a 3-dimensional (3D) SIFT descriptor for video or 3D imagery such as MRI data. We also show how this new descriptor is able to better represent the 3D nature of video data in the application of action recognition. This paper will show how 3D SIFT is able to outperform previously used description methods in an elegant and efficient manner. We use a bag of words approach to represent videos, and present a method to discover relationships between spatio-temporal words in order to better describe the video data.},
	urldate = {2019-10-07},
	booktitle = {Proceedings of the 15th {ACM} {International} {Conference} on {Multimedia}},
	author = {Scovanner, Paul and Ali, Saad and Shah, Mubarak},
	year = {2007},
	pages = {357--360}
}

@inproceedings{klaser_hog3d,
	address = {Leeds, UK},
	title = {A {Spatio}-{Temporal} {Descriptor} {Based} on 3D-{Gradients}},
	abstract = {In this work, we present a novel local descriptor for video sequences. The proposed descriptor is based on histograms of oriented 3D spatio-temporal gradients. Our contribution is four-fold. (i) To compute 3D gradients for arbitrary scales, we develop a memory-efficient algorithm based on integral videos. (ii) We propose a generic 3D orientation quantization which is based on regular polyhedrons. (iii) We perform an in-depth evaluation of all descriptor parameters and optimize them for action recognition. (iv) We apply our descriptor to various action datasets (KTH, Weizmann, Hollywood) and show that we outperform the state-of-the-art.},
	language = {en},
	booktitle = {Proceedings of the 2008 {British} {Machine} {Vision} {Conference} ({BMVC})},
	author = {Kläser, Alexander and Marszałek, Marcin and Schmid, Cordelia},
	month = sep,
	year = {2008},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/IX3UTAVH/Klaser et al. - 2008 - A Spatio-Temporal Descriptor Based on 3D-Gradients.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/UMNNAUMG/inria-00514853.html:text/html}
}

@inproceedings{dalal_human_2006,
	address = {Graz, AT},
	title = {Human {Detection} {Using} {Oriented} {Histograms} of {Flow} and {Appearance}},
	abstract = {Detecting humans in films and videos is a challenging problem owing to the motion of the subjects, the camera and the background and to variations in pose, appearance, clothing, illumination and background clutter. We develop a detector for standing and moving people in videos with possibly moving cameras and backgrounds, testing several different motion coding schemes and showing empirically that orientated histograms of differential optical flow give the best overall performance. These motion-based descriptors are combined with our Histogram of Oriented Gradient appearance descriptors. The resulting detector is tested on several databases including a challenging test set taken from feature films and containing wide ranges of pose, motion and background variations, including moving cameras and backgrounds. We validate our results on two challenging test sets containing more than 4400 human examples. The combined detector reduces the false alarm rate by a factor of 10 relative to the best appearance-based detector, for example giving false alarm rates of 1 per 20,000 windows tested at 8\% miss rate on our Test Set 1.},
	language = {en},
	booktitle = {Proceedings of the 9th {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Dalal, Navneet and Triggs, Bill and Schmid, Cordelia},
	month = may,
	year = {2006},
	pages = {428--441},
	file = {Springer Full Text PDF:/Users/Matthias/Zotero/storage/V64JG33T/Dalal et al. - 2006 - Human Detection Using Oriented Histograms of Flow .pdf:application/pdf}
}

@inproceedings{cheron_pcnn_2015,
	address = {Santiago, CL},
	title = {P-{CNN}: {Pose}-{Based} {CNN} {Features} for {Action} {Recognition}},
	shorttitle = {{PCNN}},
	url = {http://openaccess.thecvf.com/content_iccv_2015/html/Cheron_P-CNN_Pose-Based_CNN_ICCV_2015_paper.html},
	urldate = {2019-09-06},
	booktitle = {Proceedings of the 2015 {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Cheron, Guilhem and Laptev, Ivan and Schmid, Cordelia},
	month = dec,
	year = {2015},
	pages = {3218--3226},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/V9S5LRYV/Cheron et al. - 2015 - P-CNN Pose-Based CNN Features for Action Recognit.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/M6JMDK7Y/Cheron_P-CNN_Pose-Based_CNN_ICCV_2015_paper.html:text/html}
}

@inproceedings{cherian_mixing_2014,
	address = {Columbus, OH},
	title = {Mixing {Body}-{Part} {Sequences} for {Human} {Pose} {Estimation}},
	url = {http://openaccess.thecvf.com/content_cvpr_2014/html/Cherian_Mixing_Body-Part_Sequences_2014_CVPR_paper.html},
	urldate = {2019-10-01},
	booktitle = {Proceedings of the 2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Cherian, Anoop and Mairal, Julien and Alahari, Karteek and Schmid, Cordelia},
	month = jun,
	year = {2014},
	pages = {2353--2360},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/F4PEKPC6/Cherian et al. - 2014 - Mixing Body-Part Sequences for Human Pose Estimati.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/4F8VR97H/Cherian_Mixing_Body-Part_Sequences_2014_CVPR_paper.html:text/html}
}

@inproceedings{brox_high_2004,
	address = {Prague, CZ},
	title = {High {Accuracy} {Optical} {Flow} {Estimation} {Based} on a {Theory} for {Warping}},
	abstract = {We study an energy functional for computing optical flow that combines three assumptions: a brightness constancy assumption, a gradient constancy assumption, and a discontinuity-preserving spatio-temporal smoothness constraint. In order to allow for large displacements, linearisations in the two data terms are strictly avoided. We present a consistent numerical scheme based on two nested fixed point iterations. By proving that this scheme implements a coarse-to-fine warping strategy, we give a theoretical foundation for warping which has been used on a mainly experimental basis so far. Our evaluation demonstrates that the novel method gives significantly smaller angular errors than previous techniques for optical flow estimation. We show that it is fairly insensitive to parameter variations, and we demonstrate its excellent robustness under noise.},
	language = {en},
	booktitle = {Proceedings of the 8th {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Brox, Thomas and Bruhn, Andrés and Papenberg, Nils and Weickert, Joachim},
	month = may,
	year = {2004},
	keywords = {Angular Error, IEEE Computer Society, Motion Estimation, Point Iteration, Smoothness Constraint},
	pages = {25--36},
	file = {Springer Full Text PDF:/Users/Matthias/Zotero/storage/8V6QH2Y9/Brox et al. - 2004 - High Accuracy Optical Flow Estimation Based on a T.pdf:application/pdf}
}

@article{soomro_ucf101:_2012,
	title = {{UCF}101: {A} {Dataset} of 101 {Human} {Actions} {Classes} {From} {Videos} in {The} {Wild}},
	shorttitle = {{UCF}101},
	url = {http://arxiv.org/abs/1212.0402},
	abstract = {We introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5\%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips.},
	urldate = {2019-10-01},
	journal = {arXiv:1212.0402 [cs]},
	author = {Soomro, Khurram and Zamir, Amir Roshan and Shah, Mubarak},
	month = dec,
	year = {2012},
	note = {arXiv: 1212.0402},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1212.0402 PDF:/Users/Matthias/Zotero/storage/9FQPLJYS/Soomro et al. - 2012 - UCF101 A Dataset of 101 Human Actions Classes Fro.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/FLJGZMH7/1212.html:text/html}
}

@inproceedings{deng_imagenet:_2009,
	address = {Miami, FL},
	title = {{ImageNet}: a {Large}-{Scale} {Hierarchical} {Image} {Database}},
	shorttitle = {{ImageNet}},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ldquoImageNetrdquo, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {Proceedings of the 2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Li, Fei Fei},
	month = jun,
	year = {2009},
	pages = {248--255},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/TH2ZJ9VZ/Deng et al. - 2009 - ImageNet a Large-Scale Hierarchical Image Databas.pdf:application/pdf}
}

@inproceedings{bourdev_detecting_2010,
	address = {Heraklion, GR},
	title = {Detecting {People} {Using} {Mutually} {Consistent} {Poselet} {Activations}},
	abstract = {Bourdev and Malik (ICCV 09) introduced a new notion of parts, poselets, constructed to be tightly clustered both in the configuration space of keypoints, as well as in the appearance space of image patches. In this paper we develop a new algorithm for detecting people using poselets. Unlike that work which used 3D annotations of keypoints, we use only 2D annotations which are much easier for naive human annotators. The main algorithmic contribution is in how we use the pattern of poselet activations. Individual poselet activations are noisy, but considering the spatial context of each can provide vital disambiguating information, just as object detection can be improved by considering the detection scores of nearby objects in the scene. This can be done by training a two-layer feed-forward network with weights set using a max margin technique. The refined poselet activations are then clustered into mutually consistent hypotheses where consistency is based on empirically determined spatial keypoint distributions. Finally, bounding boxes are predicted for each person hypothesis and shape masks are aligned to edges in the image to provide a segmentation. To the best of our knowledge, the resulting system is the current best performer on the task of people detection and segmentation with an average precision of 47.8\% and 40.5\% respectively on PASCAL VOC 2009.},
	language = {en},
	booktitle = {Proceedings of the 11th {European} {Conference} on {Computer} {Vision} ({ECCV})},
	author = {Bourdev, Lubomir and Maji, Subhransu and Brox, Thomas and Malik, Jitendra},
	month = sep,
	year = {2010},
	keywords = {Human Torso, Image Patch, Object Detection, People Detection, Visible Bound},
	pages = {168--181},
	file = {Springer Full Text PDF:/Users/Matthias/Zotero/storage/9JS5R43R/Bourdev et al. - 2010 - Detecting People Using Mutually Consistent Poselet.pdf:application/pdf}
}

@article{wang_pose-based_2018,
	title = {Pose-{Based} {Two}-{Stream} {Relational} {Networks} for {Action} {Recognition} in {Videos}},
	url = {http://arxiv.org/abs/1805.08484},
	abstract = {Recently, pose-based action recognition has gained more and more attention due to the better performance compared with traditional appearance-based methods. However, there still exist two problems to be further solved. First, existing pose-based methods generally recognize human actions with captured 3D human poses which are very difficult to obtain in real scenarios. Second, few pose-based methods model the action-related objects in recognizing human-object interaction actions in which objects play an important role. To solve the problems above, we propose a pose-based two-stream relational network (PSRN) for action recognition. In PSRN, one stream models the temporal dynamics of the targeted 2D human pose sequences which are directly extracted from raw videos, and the other stream models the action-related objects from a randomly sampled video frame. Most importantly, instead of fusing two-streams in the class score layer as before, we propose a pose-object relational network to model the relationship between human poses and action-related objects. We evaluate the proposed PSRN on two challenging benchmarks, i.e., Sub-JHMDB and PennAction. Experimental results show that our PSRN obtains the state-the-of-art performance on Sub-JHMDB (80.2\%) and PennAction (98.1\%). Our work opens a new door to action recognition by combining 2D human pose extracted from raw video and image appearance.},
	urldate = {2019-09-23},
	journal = {arXiv:1805.08484 [cs]},
	author = {Wang, Wei and Zhang, Jinjin and Si, Chenyang and Wang, Liang},
	month = may,
	year = {2018},
	note = {arXiv: 1805.08484},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1805.08484 PDF:/Users/Matthias/Zotero/storage/9Z3P44EV/Wang et al. - 2018 - Pose-Based Two-Stream Relational Networks for Acti.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/2VEKXK6Q/1805.html:text/html}
}

@inproceedings{xiaohan_nie_joint_2015,
	title = {Joint {Action} {Recognition} and {Pose} {Estimation} {From} {Video}},
	url = {http://openaccess.thecvf.com/content_cvpr_2015/html/Nie_Joint_Action_Recognition_2015_CVPR_paper.html},
	urldate = {2019-09-23},
	author = {Xiaohan Nie, Bruce and Xiong, Caiming and Zhu, Song-Chun},
	year = {2015},
	pages = {1293--1301},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/QK7UL2MM/Xiaohan Nie et al. - 2015 - Joint Action Recognition and Pose Estimation From .pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/GWMNMEPI/Nie_Joint_Action_Recognition_2015_CVPR_paper.html:text/html}
}

@article{tu_multi-stream_2018,
	title = {Multi-stream {CNN}: {Learning} representations based on human-related regions for action recognition},
	volume = {79},
	issn = {0031-3203},
	shorttitle = {Multi-stream {CNN}},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320318300359},
	doi = {10.1016/j.patcog.2018.01.020},
	abstract = {The most successful video-based human action recognition methods rely on feature representations extracted using Convolutional Neural Networks (CNNs). Inspired by the two-stream network (TS-Net), we propose a multi-stream Convolutional Neural Network (CNN) architecture to recognize human actions. We additionally consider human-related regions that contain the most informative features. First, by improving foreground detection, the region of interest corresponding to the appearance and the motion of an actor can be detected robustly under realistic circumstances. Based on the entire detected human body, we construct one appearance and one motion stream. In addition, we select a secondary region that contains the major moving part of an actor based on motion saliency. By combining the traditional streams with the novel human-related streams, we introduce a human-related multi-stream CNN (HR-MSCNN) architecture that encodes appearance, motion, and the captured tubes of the human-related regions. Comparative evaluation on the JHMDB, HMDB51, UCF Sports and UCF101 datasets demonstrates that the streams contain features that complement each other. The proposed multi-stream architecture achieves state-of-the-art results on these four datasets.},
	urldate = {2019-09-23},
	journal = {Pattern Recognition},
	author = {Tu, Zhigang and Xie, Wei and Qin, Qianqing and Poppe, Ronald and Veltkamp, Remco C. and Li, Baoxin and Yuan, Junsong},
	month = jul,
	year = {2018},
	keywords = {Convolutional Neural Network, Action recognition, Motion salient region, Multi-Stream},
	pages = {32--43},
	file = {ScienceDirect Snapshot:/Users/Matthias/Zotero/storage/P6VWIRAQ/S0031320318300359.html:text/html;Tu et al. - 2018 - Multi-stream CNN Learning representations based o.pdf:/Users/Matthias/Zotero/storage/JTYXLC8S/Tu et al. - 2018 - Multi-stream CNN Learning representations based o.pdf:application/pdf}
}

@article{wang_dense_2013,
	title = {Dense {Trajectories} and {Motion} {Boundary} {Descriptors} for {Action} {Recognition}},
	volume = {103},
	abstract = {This paper introduces a video representation based on dense trajectories and motion boundary descriptors. Trajectories capture the local motion information of the video. A dense representation guarantees a good coverage of foreground motion as well as of the surrounding context. A state-of-the-art optical flow algorithm enables a robust and efficient extraction of dense trajectories. As descriptors we extract features aligned with the trajectories to characterize shape (point coordinates), appearance (histograms of oriented gradients) and motion (histograms of optical flow). Additionally, we introduce a descriptor based on motion boundary histograms (MBH) which rely on differential optical flow. The MBH descriptor shows to consistently outperform other state-of-the-art descriptors, in particular on real-world videos that contain a significant amount of camera motion. We evaluate our video representation in the context of action classification on nine datasets, namely KTH, YouTube, Hollywood2, UCF sports, IXMAS, UIUC, Olympic Sports, UCF50 and HMDB51. On all datasets our approach outperforms current state-of-the-art results.},
	language = {en},
	number = {1},
	urldate = {2019-09-23},
	journal = {International Journal of Computer Vision},
	author = {Wang, Heng and Kläser, Alexander and Schmid, Cordelia and Liu, Cheng-Lin},
	month = may,
	year = {2013},
	keywords = {Action recognition, Dense trajectories, Motion boundary histograms},
	pages = {60--79},
	file = {Springer Full Text PDF:/Users/Matthias/Zotero/storage/33HI6VMH/Wang et al. - 2013 - Dense Trajectories and Motion Boundary Descriptors.pdf:application/pdf}
}

@inproceedings{wang_action_2013,
	address = {Sydney, AU},
	title = {Action {Recognition} with {Improved} {Trajectories}},
	urldate = {2019-09-23},
	booktitle = {Proceedings of the 2013 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	author = {Wang, Heng and Schmid, Cordelia},
	month = dec,
	year = {2013},
	pages = {3551--3558},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/XLGPWF4J/Wang and Schmid - 2013 - Action Recognition with Improved Trajectories.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/CH7XWDZ2/Wang_Action_Recognition_with_2013_ICCV_paper.html:text/html}
}

@incollection{simonyan_two-stream_2014-1,
	title = {Two-{Stream} {Convolutional} {Networks} for {Action} {Recognition} in {Videos}},
	url = {http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf},
	urldate = {2019-09-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2014},
	pages = {568--576},
	file = {NIPS Full Text PDF:/Users/Matthias/Zotero/storage/85Z482HL/Simonyan and Zisserman - 2014 - Two-Stream Convolutional Networks for Action Recog.pdf:application/pdf;NIPS Snapshot:/Users/Matthias/Zotero/storage/2XMRV9RL/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.html:text/html}
}

@article{carreira_quo_2017-1,
	title = {Quo {Vadis}, {Action} {Recognition}? {A} {New} {Model} and the {Kinetics} {Dataset}},
	shorttitle = {Quo {Vadis}, {Action} {Recognition}?},
	url = {http://arxiv.org/abs/1705.07750},
	abstract = {The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics. We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9\% on HMDB-51 and 98.0\% on UCF-101.},
	urldate = {2019-09-10},
	journal = {arXiv:1705.07750 [cs]},
	author = {Carreira, Joao and Zisserman, Andrew},
	month = may,
	year = {2017},
	note = {arXiv: 1705.07750},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Removed references to mini-kinetics dataset that was never made publicly available and repeated all experiments on the full Kinetics dataset},
	file = {arXiv\:1705.07750 PDF:/Users/Matthias/Zotero/storage/5MDZ5M8G/Carreira and Zisserman - 2017 - Quo Vadis, Action Recognition A New Model and the.pdf:application/pdf;arXiv.org Snapshot:/Users/Matthias/Zotero/storage/2YNM3X29/1705.html:text/html}
}

@inproceedings{chin_ann_human_2015,
	title = {Human activity recognition: {A} review},
	shorttitle = {Human activity recognition},
	doi = {10.1109/ICCSCE.2014.7072750},
	abstract = {Human Activity Recognition is one of the active research areas in computer vision for various contexts like security surveillance, healthcare and human computer interaction. In this paper, a total of thirty-two recent research papers on sensing technologies used in HAR are reviewed. The review covers three area of sensing technologies namely RGB cameras, depth sensors and wearable devices. It also discusses on the pros and cons of the mentioned sensing technologies. The findings showed that RGB cameras have lower popularity when compared to depth sensors and wearable devices in HAR research.},
	author = {Chin Ann, Ong and Lau, Bee},
	month = mar,
	year = {2015},
	pages = {389--393},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/RXHHTDFJ/Chin Ann and Lau - 2015 - Human activity recognition A review.pdf:application/pdf}
}

@inproceedings{tran_closer_2018,
	title = {A {Closer} {Look} at {Spatiotemporal} {Convolutions} for {Action} {Recognition}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Tran_A_Closer_Look_CVPR_2018_paper.html},
	urldate = {2019-09-10},
	author = {Tran, Du and Wang, Heng and Torresani, Lorenzo and Ray, Jamie and LeCun, Yann and Paluri, Manohar},
	year = {2018},
	pages = {6450--6459},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/KQDIFTQ4/Tran et al. - 2018 - A Closer Look at Spatiotemporal Convolutions for A.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/AQG8AXCS/Tran_A_Closer_Look_CVPR_2018_paper.html:text/html}
}

@inproceedings{choutas_potion:_2018,
	address = {Salt Lake City, UT},
	title = {{PoTion}: {Pose} {MoTion} {Representation} for {Action} {Recognition}},
	shorttitle = {{PoTion}},
	url = {http://openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html},
	urldate = {2019-09-10},
	booktitle = {Proceedings of the 2018 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Choutas, Vasileios and Weinzaepfel, Philippe and Revaud, Jérôme and Schmid, Cordelia},
	month = jun,
	year = {2018},
	pages = {7024--7033},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/G6LBK4U6/Choutas et al. - 2018 - PoTion Pose MoTion Representation for Action Reco.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/R6SG3DLN/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html:text/html}
}

@inproceedings{wang_temporal_2016,
	address = {Amsterdam, NL},
	title = {Temporal {Segment} {Networks}: {Towards} {Good} {Practices} for {Deep} {Action} {Recognition}},
	abstract = {Deep convolutional networks have achieved great success for visual recognition in still images. However, for action recognition in videos, the advantage over traditional methods is not so evident. This paper aims to discover the principles to design effective ConvNet architectures for action recognition in videos and learn these models given limited training samples. Our first contribution is temporal segment network (TSN), a novel framework for video-based action recognition. which is based on the idea of long-range temporal structure modeling. It combines a sparse temporal sampling strategy and video-level supervision to enable efficient and effective learning using the whole action video. The other contribution is our study on a series of good practices in learning ConvNets on video data with the help of temporal segment network. Our approach obtains the state-the-of-art performance on the datasets of HMDB51 ( 69.4{\textbackslash},{\textbackslash}\% 69.4\% 69.4{\textbackslash},{\textbackslash}\% ) and UCF101 ( 94.2{\textbackslash},{\textbackslash}\% 94.2\% 94.2{\textbackslash},{\textbackslash}\% ). We also visualize the learned ConvNet models, which qualitatively demonstrates the effectiveness of temporal segment network and the proposed good practices (Models and code at https://github.com/yjxiong/temporal-segment-networks).},
	language = {en},
	booktitle = {Proceedings of the 14th {European} {Conference} on {Computer} {Vision} ({ECCV})},
	publisher = {Springer International Publishing},
	author = {Wang, Limin and Xiong, Yuanjun and Wang, Zhe and Qiao, Yu and Lin, Dahua and Tang, Xiaoou and Van Gool, Luc},
	month = oct,
	year = {2016},
	keywords = {Action recognition, ConvNets, Good practices, Temporal segment networks},
	pages = {20--36},
	file = {Springer Full Text PDF:/Users/Matthias/Zotero/storage/8CV74NNQ/Wang et al. - 2016 - Temporal Segment Networks Towards Good Practices .pdf:application/pdf}
}

@inproceedings{chollet_xception:_2017,
	address = {Honolulu, HI},
	title = {Xception: {Deep} {Learning} {With} {Depthwise} {Separable} {Convolutions}},
	shorttitle = {Xception},
	url = {http://openaccess.thecvf.com/content_cvpr_2017/html/Chollet_Xception_Deep_Learning_CVPR_2017_paper.html},
	urldate = {2019-10-22},
	booktitle = {Proceedings of the 2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Chollet, Francois},
	month = jul,
	year = {2017},
	pages = {1251--1258},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/SP4ZRVCL/Chollet - 2017 - Xception Deep Learning With Depthwise Separable C.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/MBF3BVTV/Chollet_Xception_Deep_Learning_CVPR_2017_paper.html:text/html}
}

@phdthesis{sifre_rigid-motion_2014,
	type = {{PhD} {Thesis}},
	title = {Rigid-{Motion} {Scattering} {For} {Image} {Classification}},
	abstract = {Image classification is the problem of assigning a label that best describes the content of unknown images, given a set of training images with known labels. This thesis introduces image classification algorithms based on the scattering transform, studies their properties and describes extensive classification experiments on challenging texture and object image datasets. Images are high dimensional signals for which generic machine learning algorithms fail when applied directly on the raw pixel space. Therefore, most successful approaches involve building a specific low dimensional representation on which the classification is performed. Traditionally, the representation was engineered to reduce the dimensionality of images by building invariance to geometric transformations while retaining discriminative features. More recently, deep convolutional networks have achieved state-of-the-art results on most image classification tasks. Such networks progressively build more invariant representations through a hierarchy of convolutional layers where all the weights are learned. This thesis proposes several scattering representations. Those scattering representa-tions have a structure similar to convolutional networks, but the weights of scattering are},
	school = {Echole Polytechnique},
	author = {Sifre, Laurent},
	year = {2014},
	file = {Citeseer - Full Text PDF:/Users/Matthias/Zotero/storage/GX3HX6FL/Sifre and Mallat - 2014 - Ecole Polytechnique, CMAP PhD thesis Rigid-Motion .pdf:application/pdf;Citeseer - Snapshot:/Users/Matthias/Zotero/storage/YD8V9XRU/summary.html:text/html}
}

@inproceedings{szegedy_inception-v4_2017,
	address = {San Francisco, CA},
	title = {Inception-v4, {Inception}-{ResNet} and the {Impact} of {Residual} {Connections} on {Learning}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08\% top-5 error on the test set of the ImageNet classification (CLS) challenge.},
	language = {en},
	urldate = {2019-10-22},
	booktitle = {Proceedings of the 31st {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
	month = feb,
	year = {2017},
	pages = {4278--4284},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/M9K2FE6A/Szegedy et al. - 2017 - Inception-v4, Inception-ResNet and the Impact of R.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/TRIIT4DW/14806.html:text/html}
}

@inproceedings{sun_compositional_2017,
	address = {Venice, IT},
	title = {Compositional {Human} {Pose} {Regression}},
	url = {http://openaccess.thecvf.com/content_iccv_2017/html/Sun_Compositional_Human_Pose_ICCV_2017_paper.html},
	urldate = {2019-10-22},
	author = {Sun, Xiao and Shang, Jiaxiang and Liang, Shuang and Wei, Yichen},
	month = oct,
	year = {2017},
	pages = {2602--2611},
	file = {Full Text PDF:/Users/Matthias/Zotero/storage/UQM6GEVJ/Sun et al. - 2017 - Compositional Human Pose Regression.pdf:application/pdf;Snapshot:/Users/Matthias/Zotero/storage/P5KBQ2PL/Sun_Compositional_Human_Pose_ICCV_2017_paper.html:text/html}
}

@article{hochreiter_vanishing_1998,
	title = {The {Vanishing} {Gradient} {Problem} {During} {Learning} {Recurrent} {Neural} {Nets} and {Problem} {Solutions}},
	volume = {06},
	abstract = {Recurrent nets are in principle capable to store past inputs to produce  the currently desired output. Because of this property recurrent nets  are used in time series prediction and process control. Practical  applications involve temporal dependencies spanning many time steps,  e.g. between relevant inputs and desired outputs. In this case, however,  gradient based learning methods take too much time. The extremely  increased learning time arises because the error vanishes as it gets  propagated back. In this article the de-caying error flow is theoretically  analyzed. Then methods trying to overcome vanishing gradients are briefly  discussed. Finally, experiments comparing conventional algorithms and  alternative methods are presented. With advanced methods long time lag  problems can be solved in reasonable time.},
	number = {02},
	urldate = {2019-10-21},
	journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	author = {Hochreiter, Josef},
	month = apr,
	year = {1998},
	pages = {107--116},
	file = {Snapshot:/Users/Matthias/Zotero/storage/A9TEB63K/S0218488598000094.html:text/html}
}

@inproceedings{zuffi_pictorial_2012,
	address = {Providence, RI},
	title = {From {Pictorial} {Structures} to deformable structures},
	abstract = {Pictorial Structures (PS) define a probabilistic model of 2D articulated objects in images. Typical PS models assume an object can be represented by a set of rigid parts connected with pairwise constraints that define the prior probability of part configurations. These models are widely used to represent non-rigid articulated objects such as humans and animals despite the fact that such objects have parts that deform non-rigidly. Here we define a new Deformable Structures (DS) model that is a natural extension of previous PS models and that captures the non-rigid shape deformation of the parts. Each part in a DS model is represented by a low-dimensional shape deformation space and pairwise potentials between parts capture how the shape varies with pose and the shape of neighboring parts. A key advantage of such a model is that it more accurately models object boundaries. This enables image likelihood models that are more discriminative than previous PS likelihoods. This likelihood is learned using training imagery annotated using a DS “puppet.” We focus on a human DS model learned from 2D projections of a realistic 3D human body model and use it to infer human poses in images using a form of non-parametric belief propagation.},
	booktitle = {Proceedings of the 2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Zuffi, Silvia and Freifeld, Oren and Black, Michael J.},
	month = jun,
	year = {2012},
	keywords = {2D articulated objects, 2D projections, Deformable models, deformable structures, human poses, image likelihood models, image representation, Joints, nonparametric belief propagation, nonrigid articulated objects, pairwise constraints, pictorial structures, probabilistic model, probability, PS models, Shape, Solid modeling, Torso, Training, Vectors},
	pages = {3546--3553},
	file = {IEEE Xplore Abstract Record:/Users/Matthias/Zotero/storage/VPVRKF4I/6248098.html:text/html}
}

@misc{saha_comprehensive_2018,
	title = {A {Comprehensive} {Guide} to {Convolutional} {Neural} {Networks} — the {ELI}5 way},
	url = {https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53},
	abstract = {Artificial Intelligence has been witnessing a monumental growth in bridging the gap between the capabilities of humans and machines…},
	language = {en},
	urldate = {2019-11-04},
	journal = {Medium},
	author = {Saha, Sumit},
	month = dec,
	year = {2018},
	file = {Snapshot:/Users/Matthias/Zotero/storage/RKL7RQBA/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53.html:text/html}
}

@misc{sobel_3x3_1968,
	address = {Stanford, CA},
	title = {A 3x3 isotropic gradient operator for image processing},
	author = {Sobel, Irwin and Feldman, Gary},
	year = {1968}
}

@misc{max_planck_institute_for_intelligent_systems_jhmdb_nodate,
	title = {{JHMDB} {Dataset} {Puppet} tool},
	url = {http://jhmdb.is.tue.mpg.de/puppet_tool},
	urldate = {2019-11-06},
	author = {{Max Planck Institute for Intelligent Systems}},
	file = {MPI JHMDB Dataset:/Users/Matthias/Zotero/storage/4IDISCG3/puppet_tool.html:text/html}
}